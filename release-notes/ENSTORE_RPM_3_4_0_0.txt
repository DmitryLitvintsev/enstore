<html><body><pre>
ENSTORE_RPM_4_3_0_0 is the same as ENSTORE_RPM_3_3_0_0.
The only difference is that ENSTORE_RPM_4_3_0_0 rpm requires and uses postgresql92.  
                      Enstore release notes, from ENSTORE_RPM_3_1_2_2 to ENSTORE_RPM_3_3_0_0
                      Detailed cvs commit logs
./src
========== dbaccess.py ====================================================================================

fix issue with quota being disabled after uprgade. Get rid of pg module in quota.py 

========== edb.py ====================================================================================

distinguish between None and 0 values http://uqbar/reviews/r/687/ 

========== encp.py ====================================================================================

do not skip copy requests when "--enable-rediretion" flag is used, but no redirection occured. http://uqbar/reviews/r/686/ 

========== enstore_constants.py ====================================================================================

Added plot of drive usage hours versus date, separately for each drive type and storage group. (bz 1360) 

Added plots of file read and writes per mount. (bz 1340) 

Added Drive Hours plot. (bz 1321) 

========== enstore_functions3.py ====================================================================================

KIAE has IBM 3592 Advanced data tape cartridge, JC (4TB compressed?). In their library the cartridges are named as A00188JC. Add this pattern. (rb 680). 

========== enstore_html.py ====================================================================================

BUG FIX: Synchronize migrator status generating script. 
Synchronize migrator status generating scripts with new migrator status message format. 
The new format contains sub-dictionaries with information about active works (can be more than one). 
The old format: {'current_id': '8d2b225d-e234-476b-a5ae-d3290a21fabe', 'current_migration_file': '/pnfs/fs/usr/data2/file_aggregation/packages/ANM.FF1.cpio_odc/tmp/package-M1-2013-07-25T11:07:54.787Z.tar', 'internal_state': 'WRITING_TO_TAPE', 'state': 'ARCHIVING', 'time_in_internal_state': 4.416400194168091, 'time_in_state': 10.067698001861572 'status': ('ok', None), 'work': 'get_status' } 
The new format: {'8d2b225d-e234-476b-a5ae-d3290a21fabe': {'current_id': '8d2b225d-e234-476b-a5ae-d3290a21fabe', 'current_migration_file': '/pnfs/fs/usr/data2/file_aggregation/packages/ANM.FF1.cpio_odc/tmp/package-M1-2013-07-25T11:07:54.787Z.tar', 'internal_state': 'WRITING_TO_TAPE', 'state': 'ARCHIVING', 'time_in_internal_state': 4.416400194168091, 'time_in_state': 10.067698001861572}, 'status': ('ok', None), 'work': 'get_status' } (bz 1353, rb 717) 

========== enstore_make_plot_page.py ====================================================================================

Added plot of drive usage hours versus date, separately for each drive type and storage group. (bz 1360) 

Added plots of file read and writes per mount. (bz 1340) 

Added Drive Hours plot. (bz 1321) 

========== enstore.py ====================================================================================

rb681: Ignore exception raised by importing scan.py. 

========== enstore_sfa_files_in_transition_cgi.py ====================================================================================

fix anchor tag syntax http://uqbar/reviews/r/695/ 

========== enstore_status.py ====================================================================================

BUG FIX: Synchronize migrator status generating script. 
synchronize migrator status generating scripts with new migrator status message format. 
The new format contains sub-dictionaries with information about active works (can be more than one). 
The old format: {'current_id': '8d2b225d-e234-476b-a5ae-d3290a21fabe', 'current_migration_file': '/pnfs/fs/usr/data2/file_aggregation/packages/ANM.FF1.cpio_odc/tmp/package-M1-2013-07-25T11:07:54.787Z.tar', 'internal_state': 'WRITING_TO_TAPE', 'state': 'ARCHIVING', 'time_in_internal_state': 4.416400194168091, 'time_in_state': 10.067698001861572 'status': ('ok', None), 'work': 'get_status' } 
The new format: {'8d2b225d-e234-476b-a5ae-d3290a21fabe': {'current_id': '8d2b225d-e234-476b-a5ae-d3290a21fabe', 'current_migration_file': '/pnfs/fs/usr/data2/file_aggregation/packages/ANM.FF1.cpio_odc/tmp/package-M1-2013-07-25T11:07:54.787Z.tar', 'internal_state': 'WRITING_TO_TAPE', 'state': 'ARCHIVING', 'time_in_internal_state': 4.416400194168091, 'time_in_state': 10.067698001861572}, 'status': ('ok', None), 'work': 'get_status' } (bz 1353, rb 717) 

========== file_clerk_client.py ====================================================================================

Added an option to replay app cache written events unconditionally. Before only events older than 24 hours were replayed. 
This option is needed when we do not want to wait 24 hours. This is useful for pre-downtime work. 
Here is what changed on the client side: --replay [REPLAY] : if RELAY is not specified the behaviour is as it was before. If REPLAY > 1 (integer) all events are replayed. The file clerk was also modified to have MAX_CONNECTIONS=MAX_THREADS+1 as in volume clerk (change was done by Dmitry, I am just recording it). (bz 1346, rb 709). 

add timeout and retry parameters to file clerk function calls. http://uqbar/reviews/r/704/ 

========== file_clerk.py ====================================================================================

:Feature addition: Add disk library to evt_cache_miss_fc. 
Add disk library to evt_cache_miss_fc to provide directions to dispatcher configured for clustered cache 
what qpid queue to send read request. This says: stage file to a cluster cache associated with certain disk library (bz 1350, rb 714). 

Added an option to replay app cache written events unconditionally. Before only events older than 24 hours were replayed. This option is needed when we do not want to wait 24 hours. This is useful for pre-downtime work. Here is what changed on the client side: --replay [REPLAY] : if RELAY is not specified the behaviour is as it was before. If REPLAY > 1 (integer) all events are replayed. The file clerk was also modified to have MAX_CONNECTIONS=MAX_THREADS+1 as in volume clerk (change was done by Dmitry, I am just recording it). (bz 1346, rb 709). 

========== histogram.py ====================================================================================

plot active bytes on tape per month per storage group http://uqbar/reviews/r/698/ 

add __gt__ operator to have ability to sort histograms based on integral http://uqbar/reviews/r/696/ 

add list of command and ability to add gnuplot commands http://uqbar/reviews/r/688/ 

========== inquisitor.py ====================================================================================

BUG FIX: synchronize migrator status generating script. synchronize migrator status generating scripts with new migrator status message format. The new format contains sub-dictionaries with information about active works (can be more than one). The old format: {'current_id': '8d2b225d-e234-476b-a5ae-d3290a21fabe', 'current_migration_file': '/pnfs/fs/usr/data2/file_aggregation/packages/ANM.FF1.cpio_odc/tmp/package-M1-2013-07-25T11:07:54.787Z.tar', 'internal_state': 'WRITING_TO_TAPE', 'state': 'ARCHIVING', 'time_in_internal_state': 4.416400194168091, 'time_in_state': 10.067698001861572 'status': ('ok', None), 'work': 'get_status' } The new format: {'8d2b225d-e234-476b-a5ae-d3290a21fabe': {'current_id': '8d2b225d-e234-476b-a5ae-d3290a21fabe', 'current_migration_file': '/pnfs/fs/usr/data2/file_aggregation/packages/ANM.FF1.cpio_odc/tmp/package-M1-2013-07-25T11:07:54.787Z.tar', 'internal_state': 'WRITING_TO_TAPE', 'state': 'ARCHIVING', 'time_in_internal_state': 4.416400194168091, 'time_in_state': 10.067698001861572}, 'status': ('ok', None), 'work': 'get_status' } (bz 1353, rb 717) 

========== inventory.py ====================================================================================

skip inventoring disk and null media type volume 

pass correct argument to Quota constructor http://uqbar/reviews/r/694/ 

========== lmd_policy_selector.py ====================================================================================

BUG FIX: Make exact match with the pattern. Old code was matching "ABC" and "ABCD" for the same pattern "ABC". This is incorrect. Also add disk library to thye returned value. This is required for the SFA scalability subproject. (bz 1349, rb 711). 

========== migrator_client.py ====================================================================================

Allow additional cache served by new cache server to be added to SFA. This implies using clusters of disk movers and migrators grouped around the same disk library. Details are described in https://cdcvs.fnal.gov/redmine/attachments/download/11201/Scaling%20SFA%20through%20clusters.docx (bz 1352, rb 715) 

Add trace options to control debugging messages. bz. 1312, rb 683 

========== mover.py ====================================================================================

Mover reports transfer failure 2 times per 1 failed set_new_bitfile call. This increases self.consecutive_failures by 2 instead of by 1. The default value of self.max_consecutive_failures = 2, so the mover with default self.max_consecutive_failures value immediately goes into error state. Do not call transfer_failed in set_new_bitfile as it will be called by the calling method anyway. (bz 1355, rb 719) 

Enhancement: Make sure that new_bitfile request returns an expected reply. 1. Increase new_bitfile request timeout and do not retry. 2. Make sure that reply is returned for a sent request. (bz 1348, rb 710) 

Bug fix: volume assert causes tracebacks or coredumps. If there is an error in volume assert, such as CRC MISMATCH, Mover may have a traceback or coredump due to the absence of synchronization between net thread and assert thread. 1. Traceback: Net tread calls transfer_failed() and assert thread continues with processing files because self.assert_ok.set() was set too early in transfer_failed() 2. Coredump: While transfer_failed() is dismounting the tape assert_thread calls transfer_completed, which also tries to dismount a tape. (bz 1342, rb 705). 

========== namespace.py ====================================================================================

By default use ChimeraFS class. StorageFS.__init__ has few holes. One of them: there is no selection of any superclass if no arguments are given. This change makes ChimeraFS class selected by default. (bz 1344, rb 707) 

========== plotter_main.py ====================================================================================

Added plot of drive usage hours versus date, separately for each drive type and storage group. (bz 1360) 

Added plots of file read and writes per mount. (bz 1340) 

Added Drive Hours plot. (bz 1321) 

========== purge_files.py ====================================================================================

Add disk library to file list, which will be used by dispatcher to direct requests to migrators. (bz 1351, rb 712) 

========== quota.py ====================================================================================

fix issue with quota being disabled after uprgade. Get rid of pg module in quota.py 

========== scan.py ====================================================================================

Prevented getenv call during module import. (bz 1328) 

========== set_cache_status.py ====================================================================================

Increase pad size to not allow total udp message size to exceed maximum udp message size. 

./src/cache/messaging
========== file_list.py ====================================================================================

Add disk library to file list, which will be used by dispatcher to direct requests to migrators. (bz 1351, rb 712) 

========== pe_client.py ====================================================================================

:Feature addition: Add disk library to evt_cache_miss_fc. Add disk library to evt_cache_miss_fc to provide directions to dispatcher configured for clustered cache what qpid queue to send read request. This says: stage file to a cluster cache associated with certain disk library (bz 1350, rb 714). 

./src/cache/servers
========== dispatcher.py ====================================================================================

Allow additional cache served by new cache server to be added to SFA. This implies using clusters of disk movers and migrators grouped around the same disk library. Details are described in https://cdcvs.fnal.gov/redmine/attachments/download/11201/Scaling%20SFA%20through%20clusters.docx (bz 1352, rb 715) 

========== migration_dispatcher.py ====================================================================================

Allow additional cache served by new cache server to be added to SFA. This implies using clusters of disk movers and migrators grouped around the same disk library. Details are described in https://cdcvs.fnal.gov/redmine/attachments/download/11201/Scaling%20SFA%20through%20clusters.docx (bz 1352, rb 715) 

========== migrator.py ====================================================================================

Allow additional cache served by new cache server to be added to SFA. This implies using clusters of disk movers and migrators grouped around the same disk library. Details are described in https://cdcvs.fnal.gov/redmine/attachments/download/11201/Scaling%20SFA%20through%20clusters.docx (bz 1352, rb 715) 

Do not check every file in a list by its pnfsid. Check first a tag in directory where original pnfs name points. If this fails, check by pnfs id. Additionally use cached directory names list and do not check the tag for directory in this list. Also added more internal states to have a more detailed log of what was done and in what period of time. (bz 1345, rb 708) 

The file purging condition was wrong. It did not allow to purge files if the size of occupied disk space was bigger than lower watermark. bz 1330, rb 693 

Do not include files with archive_status ARCHIVED into the list of files to aggregate. bz 1325, rb 689 

./spec
========== enstore_python_2.7.spec ====================================================================================

Version="3.3.0" Release="0" 

========== enstore.spec ====================================================================================

enstore.spec is a copy of enstore_python_2.7.spec 

./etc

========== enstore_configuration_template ====================================================================================

Modified after intallation review 

========== extra_python.pth ====================================================================================

Use python 2.7 

========== volume_audit.html ====================================================================================

fixed typo 

added link fo D0 KCF1 plot 

./release-notes
========== ENSTORE_RPM_3_1_2_2.txt ====================================================================================

./modules
========== EXfer.c ====================================================================================

change cond_wait_ts.tv_sec from 6 hours to 24 hours which determines how long to wait in condition. Transfers of large files were failing after 6 hours. http://www-ccf.fnal.gov/Bugzilla/show_bug.cgi?id=1362 http://uqbar/reviews/r/721/ 

./databases/schemas/xml
========== enstoredb.xml ====================================================================================

historic data on tape: add monthly cronjob to record available pizzabytes http://uqbar/reviews/r/690/ 

./databases/schemas/ddl/enstoredb
========== enstoredb_functions.sql ====================================================================================

fix issue with quota being disabled after uprgade. Get rid of pg module in quota.py 

add trigger to delete files fom active_file_copying if file is marked deleted 

./sbin
========== make_enstore_system.py ====================================================================================

pass correct argument to enstore_system_html.EnstoreSystemHtml constructor 

========== xfer_stats_monthly.py ====================================================================================

plot active bytes on tape per month per storage group http://uqbar/reviews/r/698/ 

./crontabs
========== delfile ====================================================================================

Use delfile_chimera.py 

========== enstore_db ====================================================================================

historic data on tape: add monthly cronjob to record available pizzabytes http://uqbar/reviews/r/690/ 

========== enstore_plots ====================================================================================

Added plot of drive usage hours versus date, separately for each drive type and storage group. (bz 1360) 

Added plots of file read and writes per mount. (bz 1340) 

Added Drive Hours plot. (bz 1321) 

</pre></body></html>
