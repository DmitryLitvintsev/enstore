<body>
<font size=7><b><i>
Enstore Data Storage System
</i></b></font>
<p>
<font size=+3><b>Table of Contents</b></font>
<ul>
<li><a href="#overview">1 Overview of Enstore Architecture</a>
<li><a href="#pnfs">1.1 The DESY <em>Pnfs</em> Namespace</a>
<li><a href="#pserver">1.1.1 <em>Pnfs</em> Tests</a>
<li><a href="#pserver">1.1.2 <em>Pnfs</em> Server Installation</a>
<li><a href="#pclient">1.1.3 <em>Pnfs</em> Client Installation</a>
<li><a href="#pcmd">1.1.4 pcmd: Enstore related <em>pnfs</em> commands</a>
<li><a href="#volmaps">1.1.5 <em>Pnfs</em> Volume Maps</a>
<li><a href="#encp">1.2 ENCP</a>
<li><a href="#encp_throttle">1.2.1 Throttling in <em>encp</em></a>
<li><a href="#encp_list_from hsm">1.2.2 Specifying lists of files when reading from or writing to the HSM</a>
<li><a href="#servers">1.3 Enstore servers</a>
<li><a href="#volume_clerk">1.3.1 Volume Clerk</a>
<li><a href="#file_clerk">1.3.2 File Clerk</a>
<li><a href="#library_manager">1.3.3 Library Manager</a>
<li><a href="#user_request">1.3.3.1 Users' Requests</a>
<li><a href="#mover_request">1.3.3.2 Movers' Requests</a>
<li><a href="#pub_request">1.3.3.3 Publisher Requests</a>
<li><a href="#mover">1.3.4 Mover</a>
<li><a href="#config_server">1.3.5 Configuration Server</a>
<li><a href="#log_server">1.3.6 Log Server</a>
<li><a href="#media_changer">1.3.7 Media Changer</a>
<li><a href="#inquisitor">1.3.8 Inquisitor</a>
<li><a href="#db">2 Databases in Enstore</a>
<li><a href="#backup_recovery">2.1 Backup and Recovery Procedures</a>
<li><a href="#backup">2.1.1 Backup </a>
<li><a href="#recovery">2.2.2 Recovery</a>
<li><a href="#current_db">2.2 Current Underlying Database Implemented in Enstore</a>
<li><a href="#admin_tools">2.3 Administrative Tools</a>
<li><a href="#protocol">3 Communication Protocols</a>
<li><a href="#read_protocol">3.1 Read Protocol</a>
<li><a href="#write_protocol">3.2 Write Protocol</a>
<li><a href="#error">4 Error Control</a>
</ul>

<p>
<h2><a name="overview">
1 Overview of Enstore Architecture
</h2>

Enstore uses a client-server architecture to provide a generic interface for users
to efficiently use mass storage systems.
Enstore supports multiple distributed media robots,
each of which may handle multiple media types or individual directly attached
drives, and multiple distributed mover nodes.
The system architecture does not dictate an exact hardware architecture.
Rather, it specifies a set of general and generic networked hardware
and software components.
These components are loosely coupled in the sense that each one can be
replaced easily without affecting the rest of the system and
each class of components can be easily expanded to accommodate
the increased demand in performance or capacity.
<p>
The system is written in <em>python</em>, a scripting
language that has advanced object-oriented features.
Python provides a sound environment for quick turn-around
and a seamless integration/migration path to
fully compiled languages, such as C and C++, if there is a demand
for even better performance.
<p>
Enstore has four major kinds of software components:
<ul>
<li><em>namespace</em>, implemented by the <em>pnfs</em> package from
        <em>DESY</em>
<li><em>encp</em>, a program used to copy files to and from media libraries
<li><em>servers</em>
        <ul>
        <li>Configuration Server
        <li>Volume Clerk
        <li>File Clerk
        <li>Multiple, distributed Library Managers
        <li>Multiple, distributed Movers
        <li>Media Changer
        <li>Log Server
        <li>Inquisitor
        <li>Alarm Server
        </ul>
<li><em>administration tools</em>
</ul>
These software components, as well as hardware components,
are shown schematically in the following system context diagram.
Hardware components are connected via IP.
Great care has been taken to ensure that the system will function well
under extreme load conditions.
By design, there is no preset limit on the number of concurrent user
computers nor on the number of physical media libraries or drives.
The system is only limited by the availability of physical resources.
We control all of the source code for the system except for that of
<em>pnfs</em> (which is a well supported product from DESY).
<p>
<img src="enstoreSimple.gif">
<p>
<a href="enstoreSimple.ps">(also available in Postscript)</a>
<p>
Enstore does not yet support a disk cache or buffer in front of the media.
However, one is conceivable and planned for a future release.
<p>
Like <em>tcp</em>, the system is architected with distributed and
peer-to-peer reliability.
Each request originating from the <em>encp</em> program is branded
with a unique ID.
<em>Encp</em> retries under well-defined circumstances, issuing
an equivalent request with a new unique ID.
The system can instruct <em>encp</em> to retry if it needs to back out
of an operation.

<h2><a name="pnfs">
1.1 The DESY <em>Pnfs</em> Namespace
</h2>

<em>Pnfs</em> is a DESY written and supported package.  Detailed
information about <em>pnfs</em> can be found on the DESY
<a href=http://mufasa.desy.de/pnfs/Welcome.html>http://mufasa.desy.de/pnfs/Welcome.html</a>
and
<a href=http://watphrakeo.desy.de/pnfs/>http://watphrakeo.desy.de/pnfs/</a>
web pages. The <em>pnfs</em> servers can not be installed without the permission of DESY.
<p>
The DESY <em>pnfs</em> package  implements an <em>nfs-v2</em> daemon and mount daemon.
These daemons do not actually serve a file system, but, instead make a
collection of database entries looks like a file system, and provide
control information for the system. Each file that is created in <em>pnfs</em> has 8
layers that Enstore uses to store metadata information about the file transfers.
Normal UNIX permissions and administered export points
are used to prevent unauthorized access to the name space.
<p>
To inspect files, users mount their portion of the <em>pnfs</em> file
system on their own computers,
and interact with it using the native operating system utilities.
For example, users can <em>ls</em>, <em>stat</em>,
<em>mv</em>, <em>rm</em> or <em>touch</em> existing "files",
but are given errors on attempts to read
or write the content of the files.
Users can also <em>mkdir</em> and <em>rmdir</em>, and <em>ln</em> files. Hard links should be used to ensure all
the metadata information is linked; symbolic links will not give the user what he naively expects.
<p>
There are some special <em>pnfs</em> files can also contain data that administrators can write to and users
can read from, so in that sense <em>pnfs</em> also provides normal files as well,
but these files are the exception rather than the rule.
<p>
Enstore uses <em>pnfs</em> for three different kinds of access and information:
<ol>
<li>Administration Interactions
    <br>
    An administrator can create special files, called wormholes, in the <em>pnfs</em> name space.
    For example, one special file signifies that the system needs to be
    drained (maybe due to an impending shutdown). Existence of this file
    causes <em>encp</em> to stall,
    preventing users from submitting additional jobs and thus draining the Enstore system of transfers.
    The name of the Enstore-draining wormhole file is "local-pnfs-mountpoint/.(config)(flags)/disabled".
    Additional wormholes can be created as needed.
    <p>
<li>Configuration Information
    <br>
    Some creation details
    need to be provided before the user can write files to media. Enstore uses <em>pnfs</em> tag files
    (usually just called tags)for these purposes.
    Tags are are
    associated with a directory and not any specific file.
    Examples of configuration information that is specified with tags include the file family name, file
    family width, Library Manager.
     <p>
<li>User File information
    <br>
    The rest of the system identifies a file by a 64-bit numeric
    identifier, dubbed a "bit file ID".
    After a file is written, the File Clerk generates a bfid and
    <em>encp</em> stores this information
    in one of the <em>pnfs</em> file layers.
    <em>Encp</em> then reads this bit file ID and gives it to the Enstore servers
    when fetching data.
    Other <em>encp</em> file transfer details, such as time of last access or location of
    where the file was copied to or transfer rates, are stored in a different
    metadata layers of the same <em>pnfs</em> file.
</ol>

<h3><a name="ptests">
1.1.2 <em>Pnfs</em> Tests
</h3>
<em>Pnfs</em> was tested in the prototype with very good results. The code was run
on a 200 MHz Pentium Pro Linux machine with SCSI disks.  The <em>pnfs</em> code had
not been run extensively on Linux machines before and there a few minor
glitches were found during initial running. Support from DESY was outstanding
and all problems were solved quickly.

As a test of <em>pnfs'</em> capabilities, the February 1998 set of 250K HPSS filenames
were put into <em>pnfs</em>. Approximately 20 <em>pnfs</em> databases were used for this
test, with each database corresponding to existing HPSS "experimental"
separation. [That is, D0 had its own database, SDSS had its own, and so
forth.] Name lookup was done on each database simultaneously from 3
machines (IRIX, Linux, and AIT) for several days.  <em>Pnfs</em> performed
flawlessly and was able to provide names at a rate of 3-15 names/S. The
database can be further optimized, but this performance is already adequate
for Enstore.
<p>
*** These tests will have to be repeated *** under the final hardware configuration,
but there is not indication of any problems.

<h3><a name="pserver">
1.1.2 <em>Pnfs</em> Server Installation
</h3>

<em>Pnfs</em> Server installation is something an experienced administrator
should do. It is not something that a user should ever have to do.
Further, permission to install <em>pnfs</em> must be granted by the MSS
group at DESY. These requests should be made to
<a href="mailto:patrick@watphrakeo.desy.de">Patrick Furhmann</a> or
<a href="mailto:gasthuber@desy.de">Martin Gasthuber</a> at DESY.

DESY has granted Enstore permission to fully use, but not change, its
<em>pnfs</em> package at Fermilab.  DESY has also granted Enstore the right
to distribute a binary + necessary scripts version of <em>pnfs</em>
(<it>i.e.</it>, no source code) via the normal Fermilab UPD product
distribution mechanism. For example, the current (Jan 99) version in UPD is
<pre>$ upd list pnfs

DATABASE=/ftp/upsdb
        Product=pnfs    Version=v3_1_3a-f4      Flavor=Linux+2
                Qualifiers=""   Chain=current

</pre>

The UPD version can be decoded as follows: "v3_1_3a" is the DESY version of
<em>pnfs</em>, and the "-f4" signifies the 4th Fermi "release". None of the
DESY code is modified - Fermilab only adds its UPS packaging framework and
some local installation instructions. All fixes, changes, or updates to
<em>pnfs</em>, will always come from DESY.

It is expected that there will be only a few <em>pnfs</em> servers at
Fermilab. To date, <em>pnfs</em> servers have been installed on 3 Linux
nodes without any difficulty. Each time, a set of installation instructions
has been improved; however the <em>pnfs</em> server installation is still
not completely automatic.  The Fermilab installation instructions are
distributed along with the UPD product.

On the node that is serving <em>pnfs</em>, <em>pnfs</em> takes over the normal
function of exporting nfs.  Otherwise the machine is general
purpose.  To be explicit, the only 2 processes <em>pnfs</em> server machine can not
run are rpc.mountd and rpc.nfsd. It runs the <em>pnfs</em> versions of these
instead. These processes are only concerned with exporting <em>pnfs</em>.
For example, Rip6 is the current (Jan 99) Enstore <em>pnfs</em>server. Here is its /etc/fstab
<pre>
rip6$ cat /etc/fstab
/dev/sda6               /                       ext2    defaults        1 1
/dev/sda5               swap                    swap    defaults        0 0
/dev/sdc1               /rip6a                  ext2    defaults,grpid  2 1
/dev/fd0                /mnt/floppy             ext2    noauto          0 0
none                    /proc                   proc    defaults        0 0
rip8:/fnal              /fnal                   nfs     soft,rsize=8192,wsize=8192      0 0
rip8:/home              /home                   nfs     soft,rsize=8192,wsize=8192      0 0
rip8:/usr/local         /usr/local              nfs     soft,rsize=8192,wsize=8192      0 0
localhost:/fs           /pnfs/fs                nfs     noauto,intr,bg,hard,rw,noac       0 0
rip6:/grau-ait          /pnfs/grau/ait          nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/grau-dlt          /pnfs/grau/dlt          nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/grau-mammoth      /pnfs/grau/mammoth      nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/stk-red20         /pnfs/stk/red20         nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/stk-red50         /pnfs/stk/red50         nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/rip6disk1         /pnfs/rip6              nfs     noauto,user,intr,bg,hard,rw,noac 0 0
</pre>
<p>
As you can see, rip6 is nfs mounting 3 disks from rip8 and mounting the
pnfs disks it is exporting as well as the local disks.
There are also numerous Enstore processes running on rip6, for example:
<pre>
USER       PID %CPU %MEM  SIZE   RSS TTY STAT START   TIME COMMAND
bakken    3280  0.0  7.3 20240  9448  ?  S    00:17   0:14 python /home/bakken/enstore/src/configuration_server.py
bakken    3334  0.0  4.9 20112  6356  ?  S    00:17   0:01 python /home/bakken/enstore/src/log_server.py
bakken    3366  0.0  5.7 37980  7352  ?  S    00:17   0:02 python /home/bakken/enstore/src/volume_clerk.py
bakken    3398  0.0  5.3 37760  6832  ?  S    00:17   0:01 python /home/bakken/enstore/src/file_clerk.py
bakken    3433  0.0  5.1 36472  6560  ?  S    00:17   0:00 python /home/bakken/enstore/src/media_changer.py
bakken    3465  0.0  8.8 33456 11300  ?  S    00:17   0:01 python /home/bakken/enstore/src/mover.new.py
bakken    3515  0.0  0.3  1140   492  ?  S    00:18   0:00 db_checkpoint -h
bakken    3520  0.0  0.3  1612   420  ?  S    00:18   0:00 db_deadlock -h
bakken    3523  0.0  5.3 30508  6808  ?  S    00:18   0:01 python /home/bakken/enstore/src/admin_clerk.py
bakken    3673  0.0  8.1 36760 10432  ?  S    00:20   0:34 python /home/bakken/enstore/src/inquisitor.py
bakken   12178  0.0  0.8  1552  1048  p2 S    19:38   0:00 /bin/login -h willow fnal.gov -p bakken
</pre>
<p>
The main point, often confused, is that the <em>pnfs</em> server node
remains a general purpose and usable machine.
<p>
Permission to mount the <em>pnfs</em> namespace is granted using a
mechanism similar to the normal Unix nfs export permission scheme.  There
are DESY commands (the pmount command) that make this entire process very
simple.
<p>
<em>Pnfs</em> can be started automatically on a boot-up. This allows other nodes to
easily mount the namespaces after reboots.
<p>
Finally, it should be noted that a Run II <em>pnfs</em> server will need is
a SCSI raid level5 disk system for its databases.
<p>
*** Live Backups of database and recovery procedures *** - to be discussed during March trip to
DESY. This has not been a priority yet.

<h3><a name="pclient">
1.1.3 <em>Pnfs</em> Client Installation
</h3>

No <em>pnfs</em> client software is needed -- the <em>pnfs</em> file system
(really namespace) just has to be mounted!  Enstore has been written such
that it recognizes all <em>pnfs</em> namespace if it has a local mounting
point beginning with /pnfs/... . Enstore uses /pnfs as convenient key.

Typical steps for mounting a new <em>pnfs</em> namespace (in this example
the <em>pnfs</em> namespace is called "grau-ait" and it is served from the
rip6 node) are:

<ol>
  <li> As root, mkdir -p /pnfs/grau/ait
  <li> As root, append to /etc/fstab:
       <pre>rip6:/grau-ait /pnfs/grau/ait nfs user,intr,bg,hard,rw,noac 0 0</pre>
       The "intr,bg,hard,rw,noac" mount options should not be changed as they are needed for proper operation.
  <li> mount /pnfs/grau/ait This can be done as a normal user if the "user" mount option
       is specified in the /etc/fstab file.
</ol>
Of course, the actual steps depend on the <em>pnfs</em> installation.
<p>
*** Automounting will be investigated in early Feb 99 *** It is expected to work without
problems. This is deemed especially important since it will streamline the production
farm nodes disk mount administration.
<p>

Finally, it should be noted that mounting <em>the </em>pnfs namespace does
not restrict the node in any other way - it can import and mount any other
file systems and run any tasks as it normally would.

<h3><a name="pcmd">
1.1.4 pcmd: Enstore related <em>pnfs</em> commands
</h3>

All non-I/O unix commands that operate on normal filesystems can also
(typically) be used on the <em>pnfs</em> namespace. DESY has provided
several examples on their web pages and tools in their <em>pnfs</em>
package that allow users to view and control the special features of
<em>pnfs</em>.  Enstore has tailored these tools into a single script,
called pcmd, that allows users to control, manipulate and query pnfs files
that enstore creates. The pcmd tool is distributed along with the encp
client. It is almost entirely written in shell, and therefore, it is
standalone and doesn't require any other products, including python.
<p>
Commonly used commands are:
<ul>
   <li>pcmd help        <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the online help

   <li>pcmd info file   <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists "important" info about the file *** Needs work to be fully functional ***
                        <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;note: "eval `pcmd info file`" will set shell variables for the user
	<pre>
	$ pcmd info M1
	bfid="91184924000000L";
	volume="flop309";
	location_cookie="68608";
	size="1252";
	file_family="jon4";
	filename="/pnfs/enstore/airedale/jon4/M1";
	orig_name="/pnfs/enstore/airedale/jon4/M1";
	map_file="/pnfs/enstore/volmap/jon4/flop309/000000068608";
	pnfsid_file="00020000000000000050AE88";
	pnfsid_map="00020000000000000050AEA0";
	</pre>

   <li>pcmd tags directory      <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the tags in the directory
        <pre>
        $ pcmd tags .
        .(tag)(library)  =  ait
        .(tag)(file_family)  =  jon-ait-3
        .(tag)(file_family_width)  =  2
        </pre>
   <li>pcmd library [value]           <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sets/lists library tag to value (must have correct cwd)
	<pre>
	$ pcmd library
	ait
	$ pcmd library xxx
	$ pcmd library
	xxx
	</pre>
   <li>pcmd file_family [value]       <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sets/lists library tag to value (must have correct cwd)
	<pre>
	$ pcmd file_family
	jon-ait-3
	$ pcmd file_family xxx
	$ pcmd file_family
	xxx
	</pre>
   <li>pcmd file_family_width [value] <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sets/lists library tag to value (must have correct cwd)
	<pre>
	$ pcmd file_family_width    
	2
	$ pcmd file_family_width 10
	$ pcmd file_family_width
	10
	</pre>
   <li>pcmd files volmap-tape   <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists all the files on specified tape in volmap *** Needs work to be fully functional ***
   <li>pcmd volume volumename   <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the volmap-tape for the specifed volumename *** Needs work to be fully functional ***

   <li>pcmd bfid   file         <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the bit file id of the file
	<pre>
	$ pcmd bfid testfile
	91551931700000L
	</pre>
   <li>pcmd parked file         <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the last parked location of the file
	                        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parked feature is not implemented
   <li>pcmd debug  file         <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the debug info about the file transfer
   <li>pcmd xref   file         <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the cross-reference info about the file
	<pre>
	$ pcmd xref testfile
	CA2902 (tape label)
	'0000_000000000_0000132' (positioning info)
	104857600 (file size)
	jon-ait-1 (file family)
	/pnfs/grau/ait/jon1/100MB.trand__Jan05005204rip8.fnal.gov16199 (original name)
	/pnfs/grau/ait/volmap/jon-ait-1/CA2902/0000_000000000_0000132 (volume map name)
	0001000000000000000928D0 (pnfs id of file)
	0001000000000000000928E0 (pnfs id of volume map file)
	</pre>
   <li>pcmd ls   file [layer]   <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;does an ls on the named layer in the file
	<pre>
	$ pcmd ls testfile 3
	4 -rw-rw-r--   1 bakken   g023         3692 Jan  5 00:55 ./.(use)(3)(testfile)
	</pre>

   <li>pcmd {cat|more|less} file layer     <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the layer of the file
    	                        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it is easier to use pcmd bfid|parked|debug|xref commands

   <li>pcmd {tagcat|tagmore|tagless} tag  directory <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists the tag in the directory
    	                        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it is easier to use pcmd library|file_family|file_family_width  commands

   <li>pcmd enstore_state          <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists whether enstore is still accepting transfers
	<pre>
	$ pcmd enstore_state
	Enstore enabled
	</pre>
	
   <li>pcmd pnfs_state mount-point <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lists whether pnfs mountpoint is up *** Not fully functional yet ***
	<pre>
	$ pcmd pnfs_state /pnfs/grau/ait
	Pnfs up
	</pre>
</ul>

Don't use these unless you know what you are doing:
<ul>
   <li>pcmd echo text file layer   <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;echos text to named layer of the file

   <li>pcmd rm file layer          <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;deletes (clears) named layer of the file

   <li>pcmd cp unixfile file layer <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copies unix file to named layer of file

   <li>pcmd size file size         <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sets the size of the file

   <li> pcmd tagecho text tagname   <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;echos text to the named tag

   <li>pcmd tagrm tag              <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;removes the tag (tricky, see DESY documents)

   <li>pcmd io file                <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sets io mode (can't clear it easily)
</ul>

Don't use these unless you can interpret the results:
<ul>
   <li>pcmd id        file        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the pnfs id
   <li>pcmd showid    id          <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the showid information
   <li>pcmd const     file        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the onst information
   <li>pcmd nameof    id          <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the filename
   <li>pcmd path      id          <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the complete file path
   <li>pcmd parent    id          <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the parent
   <li>pcmd counters  file        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the counters
   <li>pcmd counterN  dbnum (must have cwd in pnfs)  <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows of the counters
   <li>pcmd cursor    file        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the cursor
   <li>pcmd position  file        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the directory position
   <li>pcmd database  file        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the database information
   <li>pcmd databaseN dbnum (must have cwd in pnfs)  <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shows the database information
</ul>

<h3><a name="volmaps">
1.1.5 <em>Pnfs</em> Volume Maps
</h3>

In order to do request files from a tape in an optimal transversal, it is
necessary for the user to know the order of the files on a tape.  It is expected that an
experiment's catalog will keep this information.  As an additional aid for the user,
<em>encp</em> creates a duplicate file entry in <em>pnfs</em> (in addition to storing
the filename and its metadata).  This duplicate entry is organized by file
families and volumes instead of by the user's directory structure.  The naming
convention for the duplicate entries is such that the file order on the tape
could be readily known.  For example, for sequential files on tape, the name
is just the file number; for files one a disk, it is just the byte
offset. When a user wanted to know what was on a particular tape, he would
just look in the duplicate tree to get the files - then he would read the
necessary information about the linked file from one of the <em>pnfs</em> layers (such
as the bfid, original file name...) to make his transfer request to the mass
storage.

Let's say a user transferred a file to mass storage and called it
/pnfs/enstore/airedale/one.  The file was stored on disk floppy named
"flop301"  with a byte offset=0 and in a file family named "jon".

<pre>
$ ls -alsFt /pnfs/enstore/airedale/one
   0 -rw-rw-r--   1 bakken   root           26 Jul 21 12:09 /pnfs/enstore/airedale/one
</pre>

The duplicate entry is /pnfs/enstore/volmap/jon/flop301/000000000000
In the general case, "jon" would be replaced by the file family name,
"flop301" would become the volume name and "000000000000" would be the file
order on tape.

<pre>
$ ls -alsFt pnfs/enstore/volmap/jon/flop301/000000000000
   0 -rw-r--r--   1 root     root           26 Jul 21 12:09 /pnfs/enstore/volmap/jon/flop301/000000000000
</pre>

The original file and the duplicate file contain cross reference information
to allow the user to get to the other one and enough information to setup a
transfer. An example of this was shown in the previous section in "pcmd info file"

Consider a few cases.
<ul>
<li> If a user wants dozens of files, she can lookup up the volume name where
each file is stored and group her requests so files on one tape are grouped
together.
<li>If a user wants a dozen files from a certain file family but doesn't care
which ones, she can look into the volmap area and pick out files on a specific
volume and set up the request that way.
</ul>

Finally, since the user will not have delete authority to the duplicate
volmap file, this mechanism gives us an automatic trash can facility.  If the
user deletes his entry in <em>pnfs</em>, we still have the duplicate entry and can
"easily" restore the original.


<h2><a name="encp">
1.2 ENCP
</h2>
Reading and writing files means interacting with media.
This is done with an Enstore provided utility, <em>encp</em>.
<em>encp</em> is very similar to the <em>cp</em> command in UNIX.
The syntax is:
<pre>
% encp [options] src_file dst_file
</pre>
Currently there is no wildcarding allowed, but this is a straight forward
extension to <em>encp</em>.

<h3><a name="encp_throttle">
1.2.1 Throttling in <em>encp</em>
</h3>

It is important not to swamp any system.
In Enstore, a first level of throttling is implemented in
<em>encp</em>.
Control communications in Enstore uses a simple reliable
request-response protocol implemented in UDP, whereas data transfers
are implemented using two TCP ports.
A fixed number, currently 30, of pre-allocated TCP ports are
arbitrated among all instances of <em>encp</em> on a given machine.
Consequently, the system will survive the worst sort of abuse,
for example, someone forking off 200 copy requests,
since at most 15 will be active in the system at any time.

<h3><a name="encp_list_from hsm">
1.2.2 Specifying lists of files when reading from or writing to the HSM
</h3>

<em>Encp</em> duplicates unix cp's behaviour when one tries to read or write
multiple input files from the HSM: <br> ecmd encp [options]
source... directory<br> That is, the final item has to be a directory when
specifying an input list.  There is 1 caveat when specifying multiple files
using a single <em>encp</em> command: there is at most 1 mover processing your
request list at a time. If you want more than mover active, you should use
multiple <em>encp</em> commands when you are reading lists of files from more than
one volume.  The reason <em>encp</em> uses just one mover is to keep the system
simple (if you want to use multiple movers, then use many <em>encp</em>
commands).
<p>

On reads from the HSM, <em>encp</em> scans all your files and groups them according
to which volume they are on and then submits all the file requests that are
on one specific volume, reads all the files for that volume from the hsm
and then proceeds to the next volume.  <em>Encp</em> processes the volumes in any
order it chooses.
<p>

On writes to the HSM, <em>encp</em> processes each input file sequentially. Since
the user must specify a single output directory, all input files belong to
the same file family, and hence could all go to the same tape (if
possible). <em>Encp</em> sets a flag,"don't dismount the volume too quickly -
there's more files coming for the same family", that the mover uses to
postpone the dismount and thereby bypass the extra times involved in the
volume manipulations.  Please note that there is no guarantee that all the
files will go to one tape (there might not be room) or that they will be
grouped together on the tape (there may be other writes to the same family
that get intermixed).
<p>

Consider the following example (P=/pnfs/enstore/airedale) for reading from
the HSM:
<p>

There are the following files on flop301: ran-1, ran-2 ran-3, ran-4<br>
There are the following files on flop302: ran-5, ran-6 ran-7, ran-8<br>
There are the following files on flop302: ran-9, ran-10 ran-11, ran-12<br>
<p>

<em>Encp</em> submits the requests for all files on flop301 and reads back those
files and then does the same for flop302 and flop303.
<p>

Here is the output of an actual test:
<pre>
$ ecmd encp $P/ran-1       $P/ran-2        $P/ran-3        $P/ran-4 \
            $P/test2/ran-5 $P/test2/ran-6  $P/test2/ran-7  $P/test2/ran-8 \
            $P/test3/ran-9 $P/test3/ran-10 $P/test3/ran-11 $P/test3/ran-12 .
/pnfs/enstore/airedale/test2/ran-5 -> /home/bakken/enstore/src/ran-5 : 102400 bytes copied from flop302 at 0.190780317582 MB/S  requestor:bakken     cum= 3.534426
/pnfs/enstore/airedale/test2/ran-6 -> /home/bakken/enstore/src/ran-6 : 102400 bytes copied from flop302 at 0.426923710317 MB/S  requestor:bakken     cum= 3.787567
/pnfs/enstore/airedale/test2/ran-7 -> /home/bakken/enstore/src/ran-7 : 102400 bytes copied from flop302 at 0.462479436263 MB/S  requestor:bakken     cum= 3.999860
/pnfs/enstore/airedale/test2/ran-8 -> /home/bakken/enstore/src/ran-8 : 102400 bytes copied from flop302 at 0.462523304015 MB/S  requestor:bakken     cum= 4.212137
/pnfs/enstore/airedale/test3/ran-9 -> /home/bakken/enstore/src/ran-9 : 102400 bytes copied from flop303 at 0.129460905635 MB/S  requestor:bakken     cum= 5.479408
/pnfs/enstore/airedale/test3/ran-10 -> /home/bakken/enstore/src/ran-10 : 102400 bytes copied from flop303 at 0.491317380712 MB/S  requestor:bakken     cum= 5.679356
/pnfs/enstore/airedale/test3/ran-11 -> /home/bakken/enstore/src/ran-11 : 102400 bytes copied from flop303 at 0.454401235403 MB/S  requestor:bakken     cum= 5.895427
/pnfs/enstore/airedale/test3/ran-12 -> /home/bakken/enstore/src/ran-12 : 102400 bytes copied from flop303 at 0.455049173862 MB/S  requestor:bakken     cum= 6.111198
/pnfs/enstore/airedale/ran-1 -> /home/bakken/enstore/src/ran-1 : 102400 bytes copied from flop301 at 0.124436148701 MB/S  requestor:bakken     cum= 7.414610
/pnfs/enstore/airedale/ran-2 -> /home/bakken/enstore/src/ran-2 : 102400 bytes copied from flop301 at 0.45439695057 MB/S  requestor:bakken     cum= 7.630682
/pnfs/enstore/airedale/ran-3 -> /home/bakken/enstore/src/ran-3 : 102400 bytes copied from flop301 at 0.422761844505 MB/S  requestor:bakken     cum= 7.862842
/pnfs/enstore/airedale/ran-4 -> /home/bakken/enstore/src/ran-4 : 102400 bytes copied from flop301 at 0.454517460805 MB/S  requestor:bakken     cum= 8.078855
</pre>

Here is the same transfer, but this time with the "--verbose=5" option enabled,
so more details of the transfers are printed out:
<pre>
$ ecmd encp --list $P/ran-1       $P/ran-2        $P/ran-3        $P/ran-4 \
                   $P/test2/ran-5 $P/test2/ran-6  $P/test2/ran-7  $P/test2/ran-8 \
                   $P/test3/ran-9 $P/test3/ran-10 $P/test3/ran-11 $P/test3/ran-12 .
Storing/checking  local info   cum= 0.00021493434906
  dt: 0.00107896327972    cum= 0.00155103206635
Checking input pnfs files: ['/pnfs/enstore/airedale/ran-1', '/pnfs/enstore/airedale/ran-2', '/pnfs/enstore/airedale/ran-3', '/pnfs/enstore/airedale/ran-4', '/pnfs/enstore/airedale/test2/ran-5', '/pnfs/enstore/airedale/test2/ran-6', '/pnfs/enstore/airedale/test2/ran-7', '/pnfs/enstore/airedale/test2/ran-8', '/pnfs/enstore/airedale/test3/ran-9', '/pnfs/enstore/airedale/test3/ran-10', '/pnfs/enstore/airedale/test3/ran-11', '/pnfs/enstore/airedale/test3/ran-12']    cum= 0.00179898738861
  dt: 0.834562063217    cum= 0.836575031281
Checking output unix files: ['/home/bakken/enstore/src/.']    cum= 0.836683034897
  ['/home/bakken/enstore/src/ran-1', '/home/bakken/enstore/src/ran-2', '/home/bakken/enstore/src/ran-3', '/home/bakken/enstore/src/ran-4', '/home/bakken/enstore/src/ran-5', '/home/bakken/enstore/src/ran-6', '/home/bakken/enstore/src/ran-7', '/home/bakken/enstore/src/ran-8', '/home/bakken/enstore/src/ran-9', '/home/bakken/enstore/src/ran-10', '/home/bakken/enstore/src/ran-11', '/home/bakken/enstore/src/ran-12']
  dt: 0.80575799942    cum= 1.64285504818
Requesting callback ports    cum= 1.64292299747
  airedale.fnal.gov 7613
  dt: 0.0070469379425    cum= 1.65014898777
Calling Config Server to find file clerk    cum= 1.6502250433
  pcfarm9 7511
  dt: 0.00744593143463    cum= 1.65786099434
Calling file clerk for file info    cum= 1.65792798996
  dt: 0.747581005096    cum= 2.40566301346
Sending ticket to file clerk    cum= 2.4057289362
  Q'd: /pnfs/enstore/airedale/test2/ran-5 90121648900000L bytes: 102400 on flop302 (516608, 619520) dt: 0.125765   cum=2.531657
  Q'd: /pnfs/enstore/airedale/test2/ran-6 90121649400000L bytes: 102400 on flop302 (619520, 722432) dt: 0.251316   cum=2.657218
  Q'd: /pnfs/enstore/airedale/test2/ran-7 90121651400000L bytes: 102400 on flop302 (722432, 825344) dt: 0.452826   cum=2.858725
  Q'd: /pnfs/enstore/airedale/test2/ran-8 90121652100000L bytes: 102400 on flop302 (825344, 928256) dt: 11.345965   cum=13.751878
  dt: 11.3462849855    cum= 13.752177
Waiting for mover to call back    cum= 13.7522759438
  airedale.fnal.gov 7614 cum: 13.7816929817
  dt: 0.0294079780579    cum= 13.7819700241
Receiving data for file  /home/bakken/enstore/src/ran-5    cum= 13.7820539474
  bytes: 102400  Socket read Rate =  1.99837046939  MB/s
  dt: 0.0488679409027    cum= 13.8313089609
Waiting for final mover dialog    cum= 13.8313920498
  dt: 0.119197010994    cum= 13.9508379698
/pnfs/enstore/airedale/test2/ran-5 -> /home/bakken/enstore/src/ran-5 : 102400 bytes copied from flop302 at 0.491777789917 MB/S  requestor:bakken     cum= 13.951014
Waiting for mover to call back    cum= 13.9696760178
  airedale.fnal.gov 7615 cum: 13.9908440113
  dt: 0.0211460590363    cum= 13.9911179543
Receiving data for file  /home/bakken/enstore/src/ran-6    cum= 13.9912029505
  bytes: 102400  Socket read Rate =  2.06947075913  MB/s
  dt: 0.0471889972687    cum= 14.0387710333
Waiting for final mover dialog    cum= 14.0388560295
  dt: 0.119550943375    cum= 14.1586530209
/pnfs/enstore/airedale/test2/ran-6 -> /home/bakken/enstore/src/ran-6 : 102400 bytes copied from flop302 at 0.516757071819 MB/S  requestor:bakken     cum= 14.158835
Waiting for mover to call back    cum= 14.1599019766
  airedale.fnal.gov 7616 cum: 14.2008810043
  dt: 0.0409680604935    cum= 14.2011679411
Receiving data for file  /home/bakken/enstore/src/ran-7    cum= 14.2012529373
  bytes: 102400  Socket read Rate =  2.36295877514  MB/s
  dt: 0.0413279533386    cum= 14.2429490089
Waiting for final mover dialog    cum= 14.2430330515
  dt: 0.114647984505    cum= 14.3579289913
/pnfs/enstore/airedale/test2/ran-7 -> /home/bakken/enstore/src/ran-7 : 102400 bytes copied from flop302 at 0.493126210693 MB/S  requestor:bakken     cum= 14.358107
Waiting for mover to call back    cum= 14.359153986
  airedale.fnal.gov 7617 cum: 14.3949129581
  dt: 0.0357400178909    cum= 14.3951740265
Receiving data for file  /home/bakken/enstore/src/ran-8    cum= 14.3953809738
  bytes: 102400  Socket read Rate =  2.15405489264  MB/s
  dt: 0.0453360080719    cum= 14.4412109852
Waiting for final mover dialog    cum= 14.4412950277
  dt: 0.122217059135    cum= 14.5637539625
/pnfs/enstore/airedale/test2/ran-8 -> /home/bakken/enstore/src/ran-8 : 102400 bytes copied from flop302 at 0.477291649552 MB/S  requestor:bakken     cum= 14.563928
udp_client.send: read old info: (16, {'unique_id': 901216851.972, 'bfid': '90121652100000L', 'file_clerk': {'bof_space_cookie': '(825344, 928256)', 'sanity_cookie': '(5000, 33638)', 'external_label': 'flop302', 'bfid': '90121652100000L', 'complete_crc': 30442}, 'sanity_cookie': '(5000, 33638)', 'complete_crc': 30442, 'pnfs_info': {'minor': 5, 'mode': 33268, 'gid': 0, 'gname': 'root', 'rminor': 0, 'uid': 5406, 'pnfsFilename': '/pnfs/enstore/airedale/test2/ran-8', 'pstat': (33204, 38375816, 5, 1, 5406, 0, 102400, 901216512, 901216512, 901216521), 'rmajor': 0, 'uname': 'bakken', 'major': 0}, 'user_info': {'gname': 'g023', 'uid': 5406, 'gid': 1530, 'fullname': '/home/bakken/enstore/src/ran-8', 'uname': 'bakken', 'machine': ('Linux', 'airedale', '2.0.35', '#1 Mon Jul 20 08:54:09 CDT 1998', 'i686')}, 'bof_space_cookie': '(825344, 928256)', 'sanity_size': 5000, 'external_label': 'flop302', 'user_callback_port': 7613, 'work': 'read_from_hsm', 'user_callback_host': 'airedale.fnal.gov', 'status': 'ok'}, 901216872.506) ('131.225.97.10', 7511)
  Q'd: /pnfs/enstore/airedale/test3/ran-9 90121653300000L bytes: 102400 on flop303 (928256, 1031168) dt: 0.136023   cum=14.701104
  Q'd: /pnfs/enstore/airedale/test3/ran-10 90121653900000L bytes: 102400 on flop303 (1031168, 1134080) dt: 0.287928   cum=14.853003
  Q'd: /pnfs/enstore/airedale/test3/ran-11 90121655100000L bytes: 102400 on flop303 (1134080, 1236992) dt: 0.413479   cum=14.978561
  Q'd: /pnfs/enstore/airedale/test3/ran-12 90121655600000L bytes: 102400 on flop303 (1236992, 1339904) dt: 0.541382   cum=15.106496
  dt: 12.7008770704    cum= 15.1067709923
Waiting for mover to call back    cum= 15.1068719625
  airedale.fnal.gov 7618 cum: 15.6871869564
  dt: 0.580304026604    cum= 15.6874320507
Receiving data for file  /home/bakken/enstore/src/ran-9    cum= 15.6875170469
  bytes: 102400  Socket read Rate =  1.34824877428  MB/s
  dt: 0.0724319219589    cum= 15.7603620291
Waiting for final mover dialog    cum= 15.7604500055
  dt: 0.0972409248352    cum= 15.8579269648
/pnfs/enstore/airedale/test3/ran-9 -> /home/bakken/enstore/src/ran-9 : 102400 bytes copied from flop303 at 0.130022665084 MB/S  requestor:bakken     cum= 15.858102
Waiting for mover to call back    cum= 15.859210968
  airedale.fnal.gov 7619 cum: 15.9228279591
  dt: 0.0635979175568    cum= 15.9230870008
Receiving data for file  /home/bakken/enstore/src/ran-10    cum= 15.9231729507
  bytes: 102400  Socket read Rate =  1.52604351981  MB/s
  dt: 0.0639930963516    cum= 15.9875530005
Waiting for final mover dialog    cum= 15.9876400232
  dt: 0.0923020839691    cum= 16.0801730156
/pnfs/enstore/airedale/test3/ran-10 -> /home/bakken/enstore/src/ran-10 : 102400 bytes copied from flop303 at 0.441947460475 MB/S  requestor:bakken     cum= 16.080348
Waiting for mover to call back    cum= 16.0814180374
  airedale.fnal.gov 7620 cum: 16.1459280252
  dt: 0.0644949674606    cum= 16.1461839676
Receiving data for file  /home/bakken/enstore/src/ran-11    cum= 16.1462689638
  bytes: 102400  Socket read Rate =  1.42897512202  MB/s
  dt: 0.0683400630951    cum= 16.2150000334
Waiting for final mover dialog    cum= 16.2150889635
  dt: 0.0967879295349    cum= 16.3121169806
/pnfs/enstore/airedale/test3/ran-11 -> /home/bakken/enstore/src/ran-11 : 102400 bytes copied from flop303 at 0.423289390535 MB/S  requestor:bakken     cum= 16.312295
Waiting for mover to call back    cum= 16.3133749962
  airedale.fnal.gov 7621 cum: 16.3780419827
  dt: 0.0646479129791    cum= 16.3782949448
Receiving data for file  /home/bakken/enstore/src/ran-12    cum= 16.3783789873
  bytes: 102400  Socket read Rate =  1.48648697691  MB/s
  dt: 0.0656960010529    cum= 16.4444799423
Waiting for final mover dialog    cum= 16.4445669651
  dt: 0.0912410020828    cum= 16.5361510515
/pnfs/enstore/airedale/test3/ran-12 -> /home/bakken/enstore/src/ran-12 : 102400 bytes copied from flop303 at 0.438329425538 MB/S  requestor:bakken     cum= 16.536341
  Q'd: /pnfs/enstore/airedale/ran-1 90121542000000L bytes: 102400 on flop301 (2048, 104960) dt: 0.129802   cum=16.667315
  Q'd: /pnfs/enstore/airedale/ran-2 90121542700000L bytes: 102400 on flop301 (104960, 207872) dt: 0.260114   cum=16.797613
  Q'd: /pnfs/enstore/airedale/ran-3 90121543200000L bytes: 102400 on flop301 (207872, 310784) dt: 0.390148   cum=16.927652
  Q'd: /pnfs/enstore/airedale/ran-4 90121543600000L bytes: 102400 on flop301 (310784, 413696) dt: 0.517002   cum=17.054500
  dt: 14.6489070654    cum= 17.0548000336
Waiting for mover to call back    cum= 17.0549010038
  airedale.fnal.gov 7622 cum: 17.6872760057
  dt: 0.632364988327    cum= 17.687525034
Receiving data for file  /home/bakken/enstore/src/ran-1    cum= 17.6876089573
  bytes: 102400  Socket read Rate =  1.47265556183  MB/s
  dt: 0.0663130283356    cum= 17.7543139458
Waiting for final mover dialog    cum= 17.7544020414
  dt: 0.0931440591812    cum= 17.8477829695
/pnfs/enstore/airedale/ran-1 -> /home/bakken/enstore/src/ran-1 : 102400 bytes copied from flop301 at 0.123163392809 MB/S  requestor:bakken     cum= 17.847959
Waiting for mover to call back    cum= 17.8490309715
  airedale.fnal.gov 7623 cum: 17.9164079428
  dt: 0.0673609972    cum= 17.9166640043
Receiving data for file  /home/bakken/enstore/src/ran-2    cum= 17.9167480469
  bytes: 102400  Socket read Rate =  0.66466315728  MB/s
  dt: 0.146925926208    cum= 18.0640599728
Waiting for final mover dialog    cum= 18.0641460419
  dt: 0.0902429819107    cum= 18.1546230316
/pnfs/enstore/airedale/ran-2 -> /home/bakken/enstore/src/ran-2 : 102400 bytes copied from flop301 at 0.319555867275 MB/S  requestor:bakken     cum= 18.154798
Waiting for mover to call back    cum= 18.1558859348
  airedale.fnal.gov 7624 cum: 18.2239129543
  dt: 0.0680110454559    cum= 18.224167943
Receiving data for file  /home/bakken/enstore/src/ran-3    cum= 18.2242510319
  bytes: 102400  Socket read Rate =  1.40887460294  MB/s
  dt: 0.0693150758743    cum= 18.293956995
Waiting for final mover dialog    cum= 18.2940440178
  dt: 0.0940099954605    cum= 18.3883080482
/pnfs/enstore/airedale/ran-3 -> /home/bakken/enstore/src/ran-3 : 102400 bytes copied from flop301 at 0.42014781079 MB/S  requestor:bakken     cum= 18.388487
Waiting for mover to call back    cum= 18.3895900249
  airedale.fnal.gov 7625 cum: 18.4557709694
  dt: 0.0661630630493    cum= 18.4560240507
Receiving data for file  /home/bakken/enstore/src/ran-4    cum= 18.4561079741
  bytes: 102400  Socket read Rate =  1.43308480601  MB/s
  dt: 0.0681440830231    cum= 18.524641037
Waiting for final mover dialog    cum= 18.5247290134
  dt: 0.0877720117569    cum= 18.6127489805
/pnfs/enstore/airedale/ran-4 -> /home/bakken/enstore/src/ran-4 : 102400 bytes copied from flop301 at 0.437588604748 MB/S  requestor:bakken     cum= 18.612926
Complete:  1228800  bytes in  12  files  in 18.6140960455 S.  Overall rate =  0.0629563206903  MB/s
</pre>

Here's an example of writing 4 unix files to the HSM:

<pre>
Linux-airedale[353] 09:17:37 jon1$ ecmd encp ran-1 ran-2 ran-3 ran-4 $P/test
  /home/bakken/enstore/test/jon1/ran-1 -> /pnfs/enstore/airedale/test/ran-1 : 10485760 bytes copied to flop306 at 1.11655493497 MB/S  requestor:bakken     cumt= 10.983013
  /home/bakken/enstore/test/jon1/ran-2 -> /pnfs/enstore/airedale/test/ran-2 : 10485760 bytes copied to flop306 at 1.18191504602 MB/S  requestor:bakken     cumt= 19.482414
  /home/bakken/enstore/test/jon1/ran-3 -> /pnfs/enstore/airedale/test/ran-3 : 10485760 bytes copied to flop306 at 1.25504795528 MB/S  requestor:bakken     cumt= 27.481132
  /home/bakken/enstore/test/jon1/ran-4 -> /pnfs/enstore/airedale/test/ran-4 : 10485760 bytes copied to flop306 at 1.02843066309 MB/S  requestor:bakken     cumt= 37.235880
Complete:  41943040  bytes in  4  files  in 37.239297986 S.  Overall rate =  1.07413410465  MB/s
</pre>

And here's the same test but with the verbose=5 flag set:

<pre>
Linux-airedale[368] 09:21:40 jon1$ ecmd encp --verbose=5 ran-1 ran-2 ran-3 ran-4 $P/test/testa
Getting clients, storing/checking local info   cumt= 0.125026941299
Connecting to configuration server at  pcfarm9.fnal.gov 7510
  dt: 0.0242840051651    cumt= 0.151304960251
csc= <configuration_client.configuration_client instance at 8119fc0>
u= <udp_client.UDPClient instance at 811a3e8>
logc= <log_client.LoggerClient instance at 811a580>
uinfo= {'gname': 'g023', 'uid': 5406, 'gid': 1530, 'fullname': '', 'uname': 'bakken', 'machine': ('Linux', 'airedale', '2.0.35', '#1 Mon Jul 20 08:54:09 CDT 1998', 'i686')}
Checking input unix files: ['/home/bakken/enstore/test/jon1/ran-1', '/home/bakken/enstore/test/jon1/ran-2', '/home/bakken/enstore/test/jon1/ran-3', '/home/bakken/enstore/test/jon1/ran-4']    cumt= 0.152971982956
  dt: 0.185861945152    cumt= 0.339438915253
ninput= 4
inputlist= ['/home/bakken/enstore/test/jon1/ran-1', '/home/bakken/enstore/test/jon1/ran-2', '/home/bakken/enstore/test/jon1/ran-3', '/home/bakken/enstore/test/jon1/ran-4']
file_size= [10485760, 10485760, 10485760, 10485760]
delayed_dismount= 1
Checking output pnfs files: ['/pnfs/enstore/airedale/test/testa']    cumt= 0.345446944237
  dt: 0.51175403595    cumt= 0.857663989067
outputlist= ['/pnfs/enstore/airedale/test/testa/ran-1', '/pnfs/enstore/airedale/test/testa/ran-2', '/pnfs/enstore/airedale/test/testa/ran-3', '/pnfs/enstore/airedale/test/testa/ran-4']
library= ['airedaledisk', 'airedaledisk', 'airedaledisk', 'airedaledisk']
file_family= ['jon', 'jon', 'jon', 'jon']
width= [1, 1, 1, 1]
pinfo= [{'pstat': (16893, 38392440, 5, 1, 5406, 1530, 512, 901808412, 901808412, 901808412), 'minor': 0, 'rmajor': 0, 'mode': 32798, 'gname': 'g023', 'rminor': 0, 'uid': 5406, 'pnfsFilename': '/pnfs/enstore/airedale/test/testa/ran-1', 'gid': 1530, 'major': 0, 'uname': 'bakken'}, {'pstat': (16893, 38392440, 5, 1, 5406, 1530, 512, 901808412, 901808412, 901808412), 'minor': 0, 'rmajor': 0, 'mode': 32798, 'gname': 'g023', 'rminor': 0, 'uid': 5406, 'pnfsFilename': '/pnfs/enstore/airedale/test/testa/ran-2', 'gid': 1530, 'major': 0, 'uname': 'bakken'}, {'pstat': (16893, 38392440, 5, 1, 5406, 1530, 512, 901808412, 901808412, 901808412), 'minor': 0, 'rmajor': 0, 'mode': 32798, 'gname': 'g023', 'rminor': 0, 'uid': 5406, 'pnfsFilename': '/pnfs/enstore/airedale/test/testa/ran-3', 'gid': 1530, 'major': 0, 'uname': 'bakken'}, {'pstat': (16893, 38392440, 5, 1, 5406, 1530, 512, 901808412, 901808412, 901808412), 'minor': 0, 'rmajor': 0, 'mode': 32798, 'gname': 'g023', 'rminor': 0, 'uid': 5406, 'pnfsFilename': '/pnfs/enstore/airedale/test/testa/ran-4', 'gid': 1530, 'major': 0, 'uname': 'bakken'}]
p= <pnfs.pnfs instance at 8115a30>
Requesting callback ports    cumt= 0.862877011299
  airedale.fnal.gov 7600
  dt: 0.238645911217    cumt= 1.10273790359
Calling Config Server to find airedaledisk.library_manager    cumt= 1.10303294659
   airedale 7517
  dt: 0.0058319568634    cumt= 1.10947692394
Sending ticket to airedaledisk.library_manager    cumt= 1.10977995396
  Q'd: /home/bakken/enstore/test/jon1/ran-1 airedaledisk family: jon bytes: 10485760 dt: 0.0713790655136    cumt= 1.18164396286
Waiting for mover to call back    cumt= 1.18626201153
  airedale.fnal.gov 7601 cum: 2.64205896854
  dt: 1.45555996895    cumt= 2.64386999607
Sending data for file  /pnfs/enstore/airedale/test/testa/ran-1    cumt= 2.64423692226
  bytes: 10485760  Socket Write Rate =  2.01092899528  MB/s
  dt: 4.97282600403    cumt= 7.61788499355
Waiting for final mover dialog    cumt= 7.61800897121
  dt: 0.35071003437    cumt= 7.96896100044
Adding file to pnfs    cumt= 7.96911489964
  dt: 1.93133103848    cumt= 9.90079092979
  /home/bakken/enstore/test/jon1/ran-1 -> /pnfs/enstore/airedale/test/testa/ran-1 : 10485760 bytes copied to flop306 at 1.14729553162 MB/S  requestor:bakken     cumt= 9.903162
Sending ticket to airedaledisk.library_manager    cumt= 9.91201090813
  Q'd: /home/bakken/enstore/test/jon1/ran-2 airedaledisk family: jon bytes: 10485760 dt: 0.0295230150223    cumt= 9.94205701351
Waiting for mover to call back    cumt= 9.94686996937
  airedale.fnal.gov 7601 cum: 10.7699859142
  dt: 0.822878956795    cumt= 10.7718319893
Sending data for file  /pnfs/enstore/airedale/test/testa/ran-2    cumt= 10.7721740007
  bytes: 10485760  Socket Write Rate =  2.1859992052  MB/s
  dt: 4.57456707954    cumt= 15.3475339413
Waiting for final mover dialog    cumt= 15.3476879597
  dt: 0.278494954109    cumt= 15.6264359951
Adding file to pnfs    cumt= 15.6266009808
  dt: 2.92102301121    cumt= 18.5479849577
  /home/bakken/enstore/test/jon1/ran-2 -> /pnfs/enstore/airedale/test/testa/ran-2 : 10485760 bytes copied to flop306 at 1.16245629222 MB/S  requestor:bakken     cumt= 18.549760
Sending ticket to airedaledisk.library_manager    cumt= 18.5514349937
  Q'd: /home/bakken/enstore/test/jon1/ran-3 airedaledisk family: jon bytes: 10485760 dt: 0.0314730405807    cumt= 18.5834269524
Waiting for mover to call back    cumt= 18.5839298964
  airedale.fnal.gov 7601 cum: 19.4579730034
  dt: 0.873831033707    cumt= 19.459815979
Sending data for file  /pnfs/enstore/airedale/test/testa/ran-3    cumt= 19.4601159096
  bytes: 10485760  Socket Write Rate =  2.0228595011  MB/s
  dt: 4.94349706173    cumt= 24.4044009447
Waiting for final mover dialog    cumt= 24.4045529366
  dt: 0.289401054382    cumt= 24.6942389011
Adding file to pnfs    cumt= 24.6957499981
  dt: 1.75857806206    cumt= 26.4547179937
  /home/bakken/enstore/test/jon1/ran-3 -> /pnfs/enstore/airedale/test/testa/ran-3 : 10485760 bytes copied to flop306 at 1.27029727496 MB/S  requestor:bakken     cumt= 26.456495
Sending ticket to airedaledisk.library_manager    cumt= 26.4581599236
  Q'd: /home/bakken/enstore/test/jon1/ran-4 airedaledisk family: jon bytes: 10485760 dt: 0.0315480232239    cumt= 26.4902279377
Waiting for mover to call back    cumt= 26.4907259941
  airedale.fnal.gov 7601 cum: 27.4995449781
  dt: 1.00860905647    cumt= 27.5014169216
Sending data for file  /pnfs/enstore/airedale/test/testa/ran-4    cumt= 27.5017179251
  bytes: 10485760  Socket Write Rate =  2.04191980491  MB/s
  dt: 4.89735198021    cumt= 32.3998779058
Waiting for final mover dialog    cumt= 32.4000319242
  dt: 0.259968996048    cumt= 32.6602540016
Adding file to pnfs    cumt= 32.6604199409
  dt: 2.11265301704    cumt= 34.7734309435
  /home/bakken/enstore/test/jon1/ran-4 -> /pnfs/enstore/airedale/test/testa/ran-4 : 10485760 bytes copied to flop306 at 1.20713143087 MB/S  requestor:bakken     cumt= 34.775209
Complete:  41943040  bytes in  4  files  in 34.7769209146 S.  Overall rate =  1.15018808302  MB/s

</pre>
<h2><a name="servers">
1.3 Enstore servers
</h2>

Enstore servers are software entities which handle media, and in a
future release, disk caches.
The high level  concepts are as follows:
<dl>
<dt><em>Physical library</em>
<dd>Physical Library represents a real, tangible collection of media
along with software drivers/utilities to manipulate, read and write
and organize them.

A physical library can be thought of as consisting of
<ul>
        <li>one or more virtual libraries
        <li>a media changer
        <li>one of more media export/import slots
        <li>one of more drives (tape, cdrom, disk, etc.)
        <li>volumes (tape cartridges, cdroms, etc.)
</ul>
<p>
<em>Virtual Library</em> -- A virtual library contains one and only one
kind of media.
For example, Enstore divides an STK powderhorn
library holding 50, 20 and 10 GB redwood media into at least three
virtual libraries.  In common usage, the term "library" in Enstore
refers to a virtual library. Writes are directed to a specific (virtual)
library, thus selecting the media.
<p>
<em>Drives</em> -- Drives are bound to special processes called Mover
clients.
The drives can be dynamically assigned allowing the number of drives
to be less than the number of virtual libraries.
<p>
<em>Volumes</em> -- Are uniquely identified by an external label,
which is known to the Media Changer.
<p>
<dt><em>Quota Family</em>
<dd>A quota family is a set of pairs of media names and maximum number of volumes.
All files are created with respect to a quota family.
Creation of a file is not allowed if the maximum number of volumes
in that family would be exceeded.
<p>
<dt><em>File family</em>:
<dd>A file family is specified by a name and an integer "width".
A file family is associated with every file creation.
Within a given library, Enstore keeps no more
than <em>"width"</em> volumes open for writing, and loads volumes on
no more than <em>"width"</em>
number of drives at any given moment.
This is not striping, but rather, the number of different volumes,
and hence different files, which can be active at one time.
Once a volume is associated with a file family, only files in that
family will be placed on the volume.
By design, there is no pre-set limit on the number of file families.
Clever use of file families will allow volumes to be faulted out to
"shelf", and also to decrease access times for subsequent reads.  When a file
family has filled all of its "width" media, new media, limited by a quota,
are drawn out of a pool of blanks.
</dl>
Media ejected to shelf are put into a shelf virtual library and
are controlled by a shelf Library Manager.
Users are informed that this data is currently unavailable, and if
they really want the data, arrangements should be made to have the
media placed in a library which is accessible,
or get it manually later.
<p>
<h3><a name="volume_clerk">
1.3.1 Volume Clerk
</h3>
The Volume Clerk has a single table database.
There is one record for each volume known to the system.
The record is looked up by a key.
The key is the volume's external label.
The information tracked for each volume is described in the table below:
The default values are shown in parentheses ().
<table>
<td valign=top><b>Column Name</b>
<td valign=top><b>Type</b>
<td valign=top><b>Comments</b>
<tr>
<td valign=top>external_label
<td valign=top>string [primary_key]
<td valign=top>Volume name specified by user on volume creation; is used to
display volume meta-data.
<tr>
<td valign=top>file_family
<td valign=top>string ("none")
<td valign=top>File family name, specified by user on volume creation; only
files that belong to this family will be stored on this volume.
<tr>
<td valign=top>media_type
<td valign=top>string
<td valign=top>Specified at volume creation; implies the block-size; used for
writing.
<tr>
<td valign=top>library
<td valign=top>string
<td valign=top>Specified by user on volume declaration; defines which (virtual)
library currently holds the volume
<tr>
<td valign=top>first_access
<td valign=top>int (-1)
<td valign=top>Unix time when user issues the first write command to copy data to the volume.
Set by the Volume Clerk.
<tr>
<td valign=top>last_access
<td valign=top>int (-1)
<td valign=top>Unix time when  user last accessed the volume. Set by the Volume Clerk.
<tr>
<td valign=top>declared
<td valign=top>int
<td valign=top>Unix time when the volume is declared available to the system. Set by the Volume Clerk.
<tr>
<td valign=top>capacity_bytes
<td valign=top>64-bit int
<td valign=top>Specified by user on volume creation; estimate of the number of
bytes that would fit on the volume.
<tr>
<td valign=top>blocksize
<td valign=top>int
<td valign=top>Set by the Volume Clerk; derived from the the media type.
<tr>
<td valign=top>remaining_bytes
<td valign=top>64-bit int
<td valign=top>Specified by the user on volume creation; estimate of the number
of bytes that would fit on the volume; updated by the Volume Clerk every time
data are written to the media.
<tr>
<td valign=top>eod_cookie
<td valign=top>string ("none")
<td valign=top>Tells the driver how to space to the end of the volume; it is
driver specific; updated by the Volume Clerk when data are written on the
media.
<tr>
<td valign=top>wrapper
<td valign=top>string ("cpio")
<td valign=top>Wrapper method; currently specifies the format of the files on
the volume.
<tr>
<td valign=top>sum_rd_err
<td valign=top>int (0)
<td valign=top>Read error count; Volume Clerk increments this field when the Mover
receives an error while reading from the volume.
<tr>
<td valign=top>sum_rd_access
<td valign=top>int (0)
<td valign=top>Read access count; Volume Clerk increments this field
every time a file is read.
<tr>
<td valign=top>sum_wr_err
<td valign=top>int (0)
<td valign=top>Write error count; Volume Clerk increments this field when the
Mover receives an error while writing to the volume.
<tr>
<td valign=top>sum_wr_access
<td valign=top>int (0)
<td valign=top>Write access count; Volume Clerk increments this field every
time a file is written.
<tr>
<td valign=top>user_inhibit
<td valign=top>string (d:"none" or "readonly", "noaccess")
<td valign=top>Specified by user at volume creation; access level for this
volume, updated by Volume Clerk.
<tr>
<td valign=top>system_inhibit
<td valign=top>string (d:"none" or "writing", "readonly", "full", "noaccess")
<td valign=top>Administrator generated limitation on the kind of access permitted to
this volume; updated by Volume Clerk when data are written on the volume, an
error occurred while data were being written or the file size exceeded the
remaining number of bytes on the volume.
</table>
<p>
The Volume Clerk does the following operations:
<ul>
<li>show the name of all the volumes
<li>show volume information
<li>add a volume
<li>delete a volume
<li>find an appropriate volume on which to write the file
<li>change the number of remaining bytes on the volume
<li>set the number of read/write errors
<li>set the current status of the volume
<li>set the volume as readonly
<li>start/stop backup of volume journals
</ul>

<h3><a name="file_clerk">
1.3.2 File Clerk
</h3>
The File Clerk tracks files in the system. There is one record for each file
in the system.
The records are keyed.
The key is the string version of the bit file ID.
The default values are shown in parentheses ().
The fields tracked are as follows:
<table>
<td valign=top><b>Column Name</b>
<td valign=top><b>Type</b>
<td valign=top><b>Comments</b>
<tr>
<td valign=top>bfid
<td valign=top>string [primary_key]
<td valign=top>bit file ID; uniquely identifies every file in the system.
<tr>
<td valign=top>external_label
<td valign=top>string
<td valign=top>Volume name on which the file has been written; same as the
external_label in the volume table.
<tr>
<td valign=top>bof_space_cookie
<td valign=top>string
<td valign=top>Driver specific string telling how to space to the file on the
media. A lexical sort of all bof_space_cookies for a given volume will yield a optimized
traversal of the volume.
<tr>
<td valign=top>complete_crc
<td valign=top>int
<td valign=top>crc of all the bits sent by the user.
<tr>
<td valign=top>sanity_cookie
<td valign=top>string ("(0,0)")
<td valign=top>Number of bytes used for a sanity crc and the sanity crc
itself.  The sanity crc is just the normal crc but only for the 1st N bytes in
the file. This allows the Mover to check early in the transfer process that it
probably has the right user file selected; it at least will know if it has the
wrong file.
</table>
<p>
The File Clerk supports the following requests:
<ul>
<li>show bfid of all the files
<li>show file information
<li>start/stop backup of file journals
<li>assist in processing file read requests
</ul>

<h3><a name="library_manager">
1.3.3 Library Manager
</h3>
The Library Manager is a server which queues up and dispatches work for
a virtual library. There is one Library Manager for each virtual library.
It has three types of clients
<ol>
<li><em>Users</em> -- seeking to have their files read or written files.
<li><em>Movers</em> -- seeking to actually read or write files.
<li><em>Publishers</em> -- seeking HTML describing the library's current work.
</ol>
<h3><a name="user_request">
1.3.3.1 Users' Requests
</h3>
<dl>
<dt>Writes into the system
<dd>Based on the user's mss destination filename, a <em>pnfs</em> tag
associated with the destination directory, identifies
    the library for a write request allowing the <em>encp</em> program to compose
    a write request and contact the appropriate Library Manager
    directly.
    The Library Manager queues the work, and acknowledges the request.

<dt>Read from the system
<dd>Given the fact that users may mv the <em>pnfs</em> files, on reads from the system,
     <em>pnfs</em> can only provide the bit
    file ID associated with the file. In this case, <em>encp</em> contacts the
     bit File Clerk, and that
    software ultimately contacts the appropriate Library Manager to
    queue up the work.
</dl>
Work can be prioritized.
Larger priority numbers means higher priority.
Currently, write and read are both priority 1 for our test purposes.
Any priority mechanism should be able to be developed.
However, the system will exhaust all work for a volume,
given that it has been mounted, regardless of priority.
<p>
<h3><a name="mover_request">
1.3.3.2 Movers' Requests
</h3>
Movers seek to transport data between media and users over a TCP socket.
Movers contact Library Managers seeking work. If the Library Manager
has work, the Mover is requested to mount a volume, and report
back. When reporting back, the Mover may be told to contact a waiting
<em>encp</em> program and read or write a file.
The Mover may be told to unmount a volume as there is no more work for
the volume.
A Mover may have dismounted a volume unilaterally because it ran into
trouble.  This is summarized in the tables below
<table>
<td><b>Mover sends</b>
<td><b>Library Manager may respond</b>
<tr>
<td valign=top>idle_mover
<td>if work needs to be done - bind a volume; <br>or if no work - just acknowledge
<tr>
<td valign=top>have_bound_volume
<td>if reads/writes pending for the volume - read/write;  <br>or if no work - unbind volume
<tr>
<td>unilateral_unbind
<td>just acknowledge
</table>

<table>
<td valign=top><b>Library has just responded</b>
<td valign=top><b>Mover sends</b>
<td valign=top><b>Library Manager presumes</b>
<tr>
<td valign=top>bind or...<br>read or...<br>write
<td valign=top>idle_mover
<td valign=top>Mover crashed and was re-started
<tr>
<td valign=top>
<td valign=top>have_bound_volume
<td valign=top>look for work on that volume<br>
    if work, give it<br>
    if none, give unbind volume
<tr>
<td valign=top>
<td valign=top>unilateral_unbind
<td valign=top>take any work reserved for that Mover and put it back
   in the unassigned work queue
<tr>
<td valign=top>acknowledged a...<br>unilateral unbind or..<br>idle Mover
<td valign=top>idle_mover
<td valign=top>Mover is available for work, If more work available, bind a
    volume
<tr>
<td valign=top>
<td valign=top>have_bound_volume
<td valign=top>it has restarted, the Mover had a volume from a previous
    instance of me, tell it to unbind
<tr>
<td valign=top>
<td valign=top>Unilateral_unbind
<td valign=top>Just acknowledge
</table>

Note that if a Mover should crash holding a volume, the worst that can
happen is that the Library Manager will be unable to schedule work for
that volume. If the physical library has more than one drive, the system
should be able to continue servicing requests.
<p>
<h3><a name="pub_request">
1.3.3.3 Publisher Requests
</h3>
It is possible to query the Library Manager and get information about its
internal queues thereby knowing where work is.  The prototype has a
rudimentary implementation of this feature. More work needs to be done before
this can be considered functional.

<h3><a name="mover">
1.3.4 Mover
</h3>
A Mover task is bound to a single drive, and seeks to use that drive to service
read and write requests.  It communicates with the Library Manager in a
defined protocol, as just described.
<p>
The Mover is responsible for efficient data movement and as such is an
integral part of the system.  The architecture allows for performance critical
code to be written in "C" thus allowing efficient access to fundamental OS
features such as forking with minimal to no language overhead.
<p>
Although a Mover is bound to a drive, and a drive may serve more than one
virtual library, i.e., the Mover has a dynamic list of of Library Managers
that it is supposed to service.  This has two benefits.
First, since a Library Manager handles only one type of media, a drive which
handles multiple types of media (i.e. different capacity media) can be shared
without a static partitioning of the system.
Second, if we are partitioning resources in a library, we can assign a Library
Manager to each type of use.  For example, suppose Group A and Group B want to share
the capacity of a library.
Suppose half the tapes belong to Group A  and the half to Group B. We want to guarantee that
Group A have one third of the tape drives, Group B have one third, and the last third
be shared.  The Movers can be configured to do this easily.  And with some slight
changes, this is how we can guarantee resources to data acquisition.
<p>
The Mover hunts Library Managers that have work when it is idle by
consulting a Configuration Server.
A Mover's configuration gives a list of Library Managers to hunt among
for work.
If there is no work at any Library Manager,
the Mover sleeps for a while and begins the hunt again.
While sleeping the Mover is sensitive to datagrams at a
specific UDP address. A Library Manager uses this mechanism to try to
hasten a Mover when the Library Manager has work queued up.
<p>
When a Mover has found a Library Manager that has work, it attempts
to mount the salient volume, by contacting the physical library's Media
Changer.  If there is some error, it issues a unilateral unbind to the
Library Manager. If all is well, it issues a have_bound_volume to the
Library Manager.
<p>
Reads -- Once a volume is bound the Mover may read a volume and send data
to a waiting <em>encp</em> program. Two tcp ports are involved.
        The steps are:
<ol>
<li>Arbitrate for a TCP port for the data transfer.
<li>Open the tcp port (control port) specified in the request,
    send the <em>encp</em> program the data transfer port address,
    hold the control port open. If it is dropped, abort.
<li>Read the data from a volume, (stripping any wrapper like tar headers)
    verify the sanity crc. send the data to the user.
<li>Close the data port.
<li>Tell the user done and all is well.
</ol>

If any errors occur while reading the volume, an attempt is made to
characterize them as either media or drive.  This is discussed more completely
in the section on Error control.
<p>
Writes -- Once a volume is bound the Mover may receive data and write it to
the volume. Two tcp ports are involved.
        The steps are:
<ol>
<li>Arbitrate for a TCP port for the data transfer.
<li>Open the tcp port (control port) specified in the request,
    send the <em>encp</em> program the data transfer port address,
    hold the control port open. If it is dropped, abort.
<li>Mark the volume as "writing". That will cause the volume to
    not be selected for subsequent writes, should we crash.
<li>Using the eod_space_cookie, space to end of volume. Try
    to verify that we are actually at the end of volume.
<li>Receive data from <em>encp</em>, wrapper it, and write it to the volume
<li>Close the data port.
<li>Compute new eod_cookie and tell Volume Clerk that the
    volume is writable. Update remaining bytes as well.
<li>Compute the file location cookie, and tell the bit
    File Clerk about the new file. Get a bit file ID in return.
<li>Give the bit file ID to <em>encp</em>. We are done.
</ol>

If any errors occur while writing the volume, an attempt is made to
characterize
them as either media or drive.  This is discussed more completely in the
section on Error control.  If the user drops the control tcp
channel unilaterally, assume he has aborted.
<p>
<h3><a name="config_server">
1.3.5 Configuration Server
</h3>
The Configuration Server maintains the crucial information about
system configuration, such as the location and parameters of each server.
The first thing that each server does when it starts up is to
ask the Configuration Server for information (e.g. the location
of any other server with which to communicate).  New configurations can be
loaded into the Configuration Server without disturbing the current running
system.
<h3><a name="log_server">
1.3.6 Log Server
</h3>
The Log Server receives messages from other processes and logs them into
formatted log files.
Basically, these messages are transactional records.
Log files are labeled by dates.
At midnight each day, the currently opened log file gets closed and another
one is opened.
<p>
Additionally it is expected that a trace module will be developed.
<h3><a name="media_changer">
1.3.7 Media Changer
</h3>
The Media Changer mounts and dismounts the media into and from the drive
according to a request from the Mover.  One Media Changer can serve multiple
drives and libraries.  When the drives are in the robot, the Media Changer
is the interface to the robotic software.
<h3><a name="inquisitor">
1.3.8 Inquisitor
</h3>
The Inquisitor 
obtains information from the Enstore system and creates reports
using this information.
<h3>1.3.8.1 Command Line Control of the Inquisitor 
</h3>
Inquisitor functionality may be controlled through a command line interface using
<B>ecmd</B>.  A summary of the supported commands is given below.  In addition
to the following commands, the Inquisitor command line interface supports the
general commands supported by all other servers; <i>alive</i>,
<i>config_host</i>, <i>config_port</i>, <i>help</i>, <i>server_verbose</i>,
<i>verbose</i>.
<P>
<TABLE BORDER COLS= WIDTH="100%" NOSAVE >
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the time to wait for an response to an alive request</TD>
<TD NOSAVE>ecmd inq --alive_rcv_timeout=6</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the number of retries when doing an alive request</TD>
<TD NOSAVE>ecmd inq --alive_retries=4</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>dump some of the inquisitor internal information to its controlling terminal (useful for debugging)</TD>
<TD NOSAVE>ecmd inq --dump</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the maximum size of the ascii status file</TD>
<TD NOSAVE>ecmd inq --get_max_ascii_size</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the maximum number of encp status lines displayed</TD>
<TD NOSAVE>ecmd inq --get_max_encp_lines</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the html status file auto refresh rate</TD>
<TD NOSAVE>ecmd inq --get_refresh</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the frequency for monitoring the volume clerk</TD>
<TD NOSAVE>ecmd inq --get_timeout volume_clerk</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the frequency for looking for work</TD>
<TD NOSAVE>ecmd inq --get_timeout</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the maximum size of the ascii status file</TD>
<TD NOSAVE>ecmd inq --max_ascii_size=40000</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the maximum number of encp status lines displayed</TD>
<TD NOSAVE>ecmd inq --max_encp_lines=13</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots</TD>
<TD NOSAVE>ecmd inq --plot</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and use the log files located in the specified directory</TD>
<TD NOSAVE>ecmd inq --plot --logfile_dir=/tmp/logs</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and only plot information after the specified start_time</TD>
<TD NOSAVE>ecmd inq --plot --start_time=1998-12-25</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and only plot information before the specified stop_time</TD>
<TD NOSAVE>ecmd inq --plot --stop_time=1998-12-31</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and only plot information between the specified times</TD>
<TD NOSAVE>ecmd inq --plot --start_time=1998-12-01 --stop_time=1998-12-31</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the html status file auto refresh rate</TD>
<TD NOSAVE>ecmd inq --refresh=60</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for monitoring the admin_clerk to the value in the config file</TD>
<TD NOSAVE>ecmd inq --reset_timeout admin_clerk</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for looking for work to the value in the config file</TD>
<TD NOSAVE>ecmd inq --reset_timeout</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for monitoring the file_clerk</TD>
<TD NOSAVE>ecmd inq --timeout=55 file_clerk</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for looking for work</TD>
<TD NOSAVE>ecmd inq --timeout=10</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>close the current ascii status file and open a new one</TD>
<TD NOSAVE>ecmd inq --timestamp</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>monitor the log server now</TD>
<TD NOSAVE>ecmd inq --update log_server</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>monitor all the servers now</TD>
<TD NOSAVE>ecmd inq --update</TD>
</TR>
</TABLE>

<h3>1.3.8.2 Inquisitor Config File Values
</h3>
The Inquisitor looks for the following values in the Inquisitor section of the
Enstore config file. The default value is used if the dictionary element is not
found. Dictionary elements with no default must be specified in the Enstore 
config file.  All frequencies are specified in seconds.<BR>
<P>
<TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>DICTIONARY ELEMENT</B></TD>
<TD NOSAVE><B>DEFINITION</B></TD>
<TD NOSAVE><B>DEFAULT</B></TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>alive_rcv_timeout</TD>
<TD NOSAVE>seconds to wait for response to alive request</TD>
<TD NOSAVE>5</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>alive_retries</TD>
<TD NOSAVE>times to retry alive request</TD>
<TD NOSAVE>2</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>ascii_file</TD>
<TD NOSAVE>directory for ascii status file(s)</TD>
<TD NOSAVE>./</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>default_server_timeout</TD>
<TD NOSAVE>frequency to monitor servers not listed in <B>timeouts</B></TD>
<TD NOSAVE>60</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>host</TD>
<TD NOSAVE>node where inquisitor runs</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>html_file</TD>
<TD NOSAVE>directory for html status files</TD>
<TD NOSAVE>./</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>logname</TD>
<TD NOSAVE>ascii value used for id in messages to log server</TD>
<TD NOSAVE>INQS</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>max_ascii_size</TD>
<TD NOSAVE>maximum allowed size (bytes) of ascii status file</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>max_encp_lines</TD>
<TD NOSAVE>maximum number of encp lines to display</TD>
<TD NOSAVE>25</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>port</TD>
<TD NOSAVE>udp port for inquisitor communication</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>refresh</TD>
<TD NOSAVE>frequency for auto-refresh of html status page</TD>
<TD NOSAVE>120</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>timeout</TD>
<TD NOSAVE>frequency that inquisitor looks for work</TD>
<TD NOSAVE>5</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>timeouts</TD>
<TD NOSAVE>dictionary of frequencies for monitoring each server</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
</TABLE>
<P>
In addition to the information listed above, the inquisitor will look for the
<B>inq_timeout</B> dictionary element in each of the individual server sections.
If present, the value of this dictionary element will be used to specify the 
timeout value for monitoring this server.  This is the same as if the 
<B>timeouts</B>  dictionary element mentioned
above contained a dictionary element for the particular server.  For example, 
in order to monitor the file_clerk every 65 seconds, the Enstore config file 
must have one of the following in it:<BR>

<UL>
<LI>in the inquisitor section - a dictionary element, within the <B>timeouts</B>
dictionary element, for the file clerk set to 65
<LI>in the file_clerk section - the dictionary element <B>inq_timeout</B> set to
65
</UL>
The value in the individual server dictionary element will take precedence over
the value in the Inquisitor dictionary element.
<P>
In order to block monitoring of a particular server, set it's timeout value to 
-1.
<P>
An example Inquisitor dictionary element is given below:<BR>
<P>
<PRE>
configdict['inquisitor']   = { 'host':'rip6',          \
                               'port':7505,            \
                               'logname':'INQSRV',     \
                               'timeout':10,           \
                               'alive_rcv_timeout': 5, \
                               'alive_retries':1,      \
                               'ascii_file':'/rip6a/enstore/inquisitor/', \
                               'html_file':'/fnal/ups/prd/www_pages/enstore/', \
                               'default_server_timeout': 15, \
                               'timeouts' : { 'ait.library_manager': 15} }
</PRE>


<h2><a name="db">
2 Databases in Enstore
</h2>
The database used in Enstore must provide the following:
<ul>
<li>
Support journaling of the database to record all changes and support full
     database recovery.
<li>
Support database check-pointing in order to enable full database recovery.
<li>
Support performing daily backups of the database, log, and journal files.
<li>
Support recovery of corrupted databases using the journal or log files.
<li>
Support "python dictionary" interface.
</ul>

<h2><a name="backup_recovery">
2.1 Backup and Recovery Procedures
</h2>
<h3><a name="backup">
2.1.1 Backup
</h3>
The backup procedure is a cron job that is performed routinely. It copies
the database files, log files and journal files to a designated
directory on a remote host. Two environment variables should be set
up for this purpose: ENSTORE_BCKP_HST (default="localhost") and
ENSTORE_DB_BACKUP (default="tmp/backup"). The backup procedure will
perform the following actions:
<p>
<table>
<tr>
<td valign=top>Libtp database
<td valign=top><ul>
<li>defines what log files are involved in active transactions<br>
<li>creates the tar file of database files and all log files <br>
<li>deletes all log files that are not involved in active transactions
</ul>
<tr>
<td valign=top>Volume journal files
<td valign=top><ul>
<li>does journal file checkpointing (hold database access,
move current file to volume.jou.time_stamp, open empty
journal file, release database access)
<li>creates tar file of volume database file and journal files
<li>deletes old journal files
</ul>
<tr>
<td valign=top>File journal file
<td valign=top><ul>
<li>does journal file checkpointing
<li>creates tar file of file database file and journal files
<li>deletes old journal files
</ul>
<tr>
<td valign=top>Archives creation
<td valign=top><ul>
<li>creates new directory on remote host under designated
"root archival" director (name dbase.time_stamp)
<li>moves all the tar files to this area
</ul>
<tr>
<td valign=top>Archives cleanup
<td valign=top><ul><li>deletes all the archival directories created more then
N days ago (default is 10 days)</ul>
</table>
<h3><a name="recovery">
2.2.2 Recovery
</h3>
Recovery should be a job initiated manually in case of database corruption.
There are two ways to recover from a crash:
<p>
<table>
<tr>
<td valign=top>libtp utility
<td valign=top><ul>
<li>copy database tar file in designated directory, untar the file
<li>copy the latest log file from $ENSTORE_DB,
<li>run db_recover utility
<li>delete all related files from $ENSTORE_DB
<li>move the resulting database files and the latest log file to $ENSTORE_DB
</ul>
<tr>
<td valign=top>recovery from the journal files (not implemented yet)
<td valign=top><ul>
<li>copy database (database files and journal files) tar file
<li>in designated directory, untar the file
<li>copy the latest jouurnal file from the $ENSTORE_DB,
<li>run jou_recover utility
<li>delete all related files from $ENSTORE_DB
<li>move the resulting database files to $ENSTORE_DB
</ul>
</table>

<h3><a name="current_db">
2.2 Current Underlying Database Implemented in Enstore
</h3>

The current Enstore implementation uses LIBTP (BSD DB v2.3) as the underlying
database product.
LIBTP is free for non-profit organizations like Fermilab, and has the
following features:

<ul>
<li>
one key dictionary-like database. It is designed to
store/retrieve binary large objects (BLOBs) of arbitrary length, by text key.
<li>
ability to store data items of unlimited size
<li>
support for various data storage structures: hash table, binary tree,
numbered records
<li>
allows duplicate keys (Enstore doesn't use them)
<li>
data scanning with cursors, multiple cursors may be opened at the same time
<li>
different levels of cursor stability
<li>
transactions
<li>
transaction logging
<li>
check-pointing
<li>
backup and recovery tools
<li>
custom locks
<li>
deadlock detection
</ul>


A LIBTP-Python shelve-like interface was developed. It provides access to:
<ul>
<li>All three data structures: hash table, binary tree, numbered records
<li>Cursors
<li>Transactions
<li>Locks
</ul>

LIBTP was chosen based on the following considerations:

<ul>
<li>
Nimbleness to allow us to set up test stands while developing and not being encumbered by
     database licensing issues.
<li>
It is similar to dbm-like databases used for the initial Enstore design.
This made it easy to develop a Python interface for it and any necessary
changes to the Enstore code were localized and relatively easy to make.
<li>
Database maintenance is relatively inexpensive. It requires only two
processes to run. One for check-pointing and the other for deadlock detection.
<li>
It is simple and fast enough.
<li>
It provides tools for database transaction logging, database backup and
recovery.
<li>
It is readily obtainable and free.
</ul>

We don't consider the current selection of the LIBTP database to be
necessarily the final one.  For example, although it may not be necessary
for Enstore's internal needs, implementation of maintenance functionality
is not realistically possible without a way to select data using attributes
other than the primary database key. There are two possible ways to solve
this problem. One way is to create additional database index tables. This
is a straightforward extension on top of LIBTP, although probably not the
most elegant solution. Another possible method we are considering is to use
relational databases in Enstore.  For the Enstore project, databases are a
modular component and switching databases means modifying a module only. If
a new database is chosen, we are expecting that the database servers and
clients will have to be redesigned to take advantage of the relational
features.  Python interfaces will also have to be developed to allow the
current framework to keep working.

<h2><a name="admin_tools">
2.3 Administrative Tools
</h2>

Administrative tools will provide the following operations:
<ul>
<li>Display all volumes for a specified media
<li>List all the files and their location on a single or set of media
<li>List file/files on the media by creation date
<li>List all the media that belongs to a specified file family
<li>List files that belongs to a specified file family
<li>Display the date of the last mount for a specified volume
<li>List all media belonging to a file family sorted by the most recent media
mount date
<li>List all media belonging to a file family where the last access date is
before a specified date
<li>Export meta-data of ejected media into a flat file
<li>Import meta-data from a flat file when importing the media from outside the
Enstore system
<li>List all files/volumes that belong to user/group
<li>Mark the volume as readonly if all of the files on the media are older
than a specified date
<li> Delete specified files in the <em>pnfs</em> trashbin.
<li>Find and recycle volumes where all files have been deleted.
<li>Check for files known to the File Clerk but unknown to <em>pnfs</em>
</ul>

<h2><a name="protocol">
3 Communication Protocols
</h2>
The base protocol for Enstore is UDP for "brief" messages and TCP for data
transfers.

UDP message sizes are all less than size of maximum UDP packet size so the
protocol is very simple.

<p>
The base server protocol is the same for all servers.  State-fullness is
minimized, not eliminated.
<p>
Each transmission has a unique ID, timeout and maximum number of retries
associated with it.  The timeout allows for debugging.
For each reception, the "message" is checked against messages received to see
if the reception is a repeat. If the reception is a *repeat request*, send a
saved copy of the response; if the reception is *repeat response*, just
ignored it. This will take care of the case when a timeout/retry happens just
before a response is received.

<h2><a name="read_protocol">
3.1 Read Protocol
</h2>
The communications performed during a read operation are illustrated in the
diagram below and described more fully in the following text.
<p>
NOTE: The communications between the Mover and the Configuration Server happens
approximately every two minutes.  It has been added to the following drawing
to show that this communication is important, but it can occur anywhere in the
communications flow before the Mover contacts the Library Manager.
<p>
<img src=read.gif>
<p>
<a href="read.ps">(also available in Postscript)</a>
<p>
<ul>
<li>
The user (through <em>encp</em>) contacts <em>pnfs</em> asking for a bit file ID (bfid)
for the named file.
<li>
<em>pnfs</em> returns the bfid to <em>encp</em>.
<li>
<em>Encp</em> asks the Configuration Server with which File Clerk should it be
communicating.
<li>
The Configuration Server returns the location of the appropriate File Clerk.
<li>
<em>Encp</em> sends the read request to the File Clerk.
<li>
The file Clerk asks the Volume Clerk with which Library manger should it be
communicating.
<li>
The Volume Clerk returns the location of the appropriate Library Manager.
<li>
The File Clerk asks the Library Manager to read the file.
<li>
The Library Manager tells the File Clerk that the request has been placed in
it's queue.
<li>
The File Clerk informs <em>encp</em> that the read request has been queued.
<li>
The Mover asks the Configuration Server with which Library Manager should it
be communicating.
<li>
The Configuration Server returns the location of the appropriate Library
Manager.
<li>
The Mover asks the Library Manager if there is any work for it to do.
<li>
The Library Manager tells the Mover which volume to mount.
<li>
The Mover asks the Media Changer to mount a particular volume.
<li>
The Media Changer responds once the volume is mounted.
<li>
The Mover tells the Library Manager that the volume is mounted.
<li>
The Library Manager tells the Mover which file to read.
<li>
The Mover tells <em>encp</em> from which host and port to read the data.
<li>
The Mover send the data to <em>encp</em>.
<li>
The Mover tells <em>encp</em> when all the data has been transferred and sends
the crc information.
<li>
The read has completed.
<li>
The Mover tells the Library Manager that he still has the volume mounted.
</ul>

<h2><a name="write_protocol">
3.2 Write Protocol
</h2>
The communications performed during a write operation are illustrated in the
diagram below and described more fully in the following text.
<p>
NOTE: The communications between the Mover and the Configuration Server happens
approximately every two minutes.  It has been added to the following drawing
to show that this communication is important, but it can occur anywhere in the
communications flow before the Mover contacts the Library Manager.
<p>
<img src=write.gif>
<p>
<a href="write.ps">(also available in Postscript)</a>
<p>
<ul>
<li>
The user (through <em>encp</em>) contacts <em>pnfs</em>with a request to create a file.
<li>
<em>pnfs</em>returns the file family and volume library information to <em>encp</em>.
<li>
<em>Encp</em> asks the Configuration Server with which Library Manager should
it be communicating.
<li>
The Configuration Server returns the location of the appropriate Library
Manager.
<li>
<em>Encp</em> sends the write request to the Library Manager, including file
family and number of bytes.
<li>
The Library Manager tells the <em>encp</em> that the request has been placed in
it's queue.
<li>
The Mover asks the Configuration Server with which Library Manager should it
be communicating.
<li>
The Configuration Server returns the location of the appropriate Library
Manager.
<li>
The Mover asks the Library Manager if there is any work for it to do.
<li>
The Library Manager asks the Volume Clerk for a volume for the file with the
specified size and file family.
<li>
The Volume Clerk returns the volume to the Library Manager.
<li>
The Library Manager moves the file internally from one queue to another.
<li>
The Library Manager tells the Mover which volume to mount.
<li>
The Mover asks the Media Changer to mount a particular volume.
<li>
The Media Changer responds once the volume is mounted.
<li>
The Mover tells the Library Manager that the volume is mounted.
<li>
The Mover tells <em>encp</em> to which host and port to write the data.
<li>
The Mover tells the Volume Clerk that he is appending to this volume.
<li>
The Volume Clerk acknowledges this.
<li>
<em>Encp</em> sends the data to the Mover.
<li>
The Mover tells the Volume Clerk that the append operation is done and how much
space is left on the volume.
<li>
The Volume Clerk acknowledges this.
<li>
The Mover tells the File Clerk which file has been created.
<li>
The File Clerk responds with the bit file ID.
<li>
The Mover tells <em>encp</em> that the file has been written and sends the bit
file ID and the crc.
<li>
<em>Encp</em> tells <em>pnfs</em>that the file has been created, and the bfid should
be stored.
<li>
The write has completed.
<li>
The Mover tells the Library Manager that he still has the volume mounted.
<li>
The Library Manager tells the Mover that there is no work to be done.
<li>
The Mover tells the Media Changer to dismount the volume.
</ul>

<h2><a name="error">
4 Error Control
</h2>

This section represents our error control development plan -- error control
was only partially implemented in the prototype. <p>

Error control in Enstore is simple, because much of the system state
is stored in the <em>encp</em> client. The <em>encp</em> client can be given a
"retryable error" and will resend data for a write into the system, or the
system can restart a read from the system from scratch. This retry
is given a very high priority when it is received by the Library
Manager -- put on the head of the unassigned work queue.

The Enstore system keeps unassigned read and write requests in a queue of
unallocated work in the Library Manager. Once a read or write requests'
turn comes, the Library Manager puts the request in a "work awaiting
mount" or "work at mover" queue. The reason for this is to track the
volumes for scheduling : the Library Manager must not command a Mover
to mount a volume already in use by another Mover. It is the Mover, and not the
Library Manager which completes requests.

The three Library Manager queues are:
<ul>
<li>Unscheduled work
<li>work awaiting a volume mount
<li>work at a Mover.
</ul>
<p>
It is important to keep these queues consistent. Volume and reading
errors are handled in the Mover, and can be handled by detailed design in
the Mover.
<p>
Many interesting errors are related to when the volume cannot be written
or read, or when it is suspected that volume is jammed, etc.
<p>
We need experience with the actual hardware. In the interim we make the
following assumptions:
<ul>
<li>
If we have trouble during a load or unload operation, we assume that
the volume is physically jammed. Until a human looks at the problem, we
will permit no future operations on the drive or the volume.
<li>
If several drives have fatal errors on writing a volume, we will not write on that
volume anymore.
<li>
If a drive has errors on "consecutive" volumes, we should "stop
using the drive". The number of errors allowed should be picked
up through the Mover configuration, since we may have "more
temperamental" and "less temperamental" drives, we may want to
configure this parameter for each Mover individually.
<li>
If volume reads or spaces give error, we should track the errors. If
the recent history of the volume shows many errors, we should mark the
volume "no access" and bring it to the attention of an administrator.
Therefore, the Volume Clerk needs to track the "recent history" of
errors, and have a per-volume figure of merit which will mark the volume
for no access, given many recent errors.
<li>
Library Manager:
<br>
Two Features should be added to the tickets to support retries.
<ul>
<li>Tickets should be marked as retries or not by <em>encp</em>. Retries go
to the head of the queue.
<li>Tickets should be marked with an "avoid this mover" field for read
retries, so the same drive can be avoided.
</ul>
<em>encp</em> should generate these fields appropriately, and the queuing logic
in the Library Manager ought to implement the logic associated with these
fields.
<p>
</ul>
The Enstore system should check the read/write state of a volume read very
early when the request is queued, to give a prompt error if the access
is not allowed. However, an error may change the state of a volume while
its corresponding work is in the Library Manager queue. Therefore, a
late check is required. The system already performs a "late check"
for writes, since a volume must be selected for every file. The system
needs a "late check" for reads. Failing a late check means contacting
<em>encp</em> to give it the bad news. The contact should be done in the Mover,
not in the Library Manager, just before the Mover goes off to mount the volume
for read, since the Mover is better able to tolerate a very slow or
non-existent <em>encp</em>. It is good to contact
<em>encp</em> before a mount, too, since people will ^C <em>encp</em>'s and
it is best to discover this before incurring the expense of a mount.
<ul>
<li>
"Freeze the volume in the drive" means:
<ul>
<li>Not unloading the volume from the drive
<li>Freezing the volume
<li>Off-lining the Drive
</ul>
<li>
"Freezing the volume" means:
<ul>
<li>mark the volume as "system noaccess"
<li>log that this happened and let an administrator look at the
problem in the morning.
<li><em>Encp</em> shall not retry.
</ul>
<li>
"Off-lining the drive" means:
<ul>
<li>Preserving as much state as possible.
<li>Writing a complete description in an error log.
<li>Leaving the problem until business hours unless the capacity of
the system falls below a threshold.
</ul>
</ul>

<table>
<th><font size=+1><b>Volume Write Errors</b></font>
<tr>
<td valign=top><b>WRITE_NOTAPE</b>
<td valign=top>"no such tape" error in library mount failure on write.
    If this happens, the volume data base is inconsistent with reality.
    Mover shall freeze the volume.
    <em>Encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_TAPEBUSY</b>
<td valign=top>"The Media Changer says that the volume is in another drive"
    Enstore has a bug, or some other system has mounted the volume or
    library micro put the volume somewhere else.
    Mover shall freeze the volume.
    <em>Encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_DRIVEBUSY</b>
<td valign=top>"The Media Changer says that some other volume is in the drive"
    Need to investigate if this could ever be a clean volume otherwise a bug in
    Enstore or a misconfiguration.     Mover shall offline the drive.
    <em>Encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_BADMOUNT</b>
<td valign=top>Other mount failure on write, or load operation failed.
    Must assume jammed volume.
    Mover shall freeze the volume in the drive.
    <em>Encp</em> shall retry
<tr>
<td valign=top><b>WRITE_BADSPACE</b>
<td valign=top>"EOD cookie does not produce EOD"
    Wrong volume, software bug or drive space error.
    Mover shall freeze the volume in the drive.
    <em>Encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_ERROR</b>
<td valign=top>"Error when writing data block or file mark"
    Run of the mill write error.
    If the drive has had many recent errors, Mover shall
    "offline the drive"
    The volume shall be marked read-only.
    <em>Encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_EOT</b>
<td valign=top>"Hit EOT while writing data block or file mark"
    Hit EOT. Mover shall mark volume full.
    <em>Encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_UNLOAD</b>
<td valign=top>"error when unloading volume from drive"
    Must assume that the volume is physically jammed.
    Mover shall freeze the volume in the drive.
    <em>Encp</em> shall retry.
    No <em>encp</em> is associated with the volume at this time.
<tr>
<td valign=top><b>WRITE_UNMOUNT</b>
<td valign=top>"Media Changer gives error when unmounting volume"
    Should not happen. Maybe the volume is hanging in the drive. Does no harm.
    Mover shall freeze the volume in the drive.
    No <em>encp</em> is associated with the volume at this time.
<tr>
<td valign=top><b>WRITE_NOBLANKS</b>
<td valign=top>The Volume Clerk has no blank volumes to give.
    The requests shall return an error.
    (do not "wait forever" -- DAQ should switch to an alternate library)
    An administrator shall be paged.
<tr>
<th><font size=+1><b>Volume Read Errors</b></font>
<tr>
<td valign=top><b>READ_NOTAPE</b>
<td valign=top>"No such volume in library" mount failure on read.
    If this happens, the volume data base is inconsistent with reality.
    Mover shall freeze the volume.
<tr>
<td valign=top><b>READ_TAPE_BUSY</b>
<td valign=top>"The Media Changer says that the volume is in another drive"
    Enstore has a bug, or some other system has mounted the volume or
    the robotic software put the volume somewhere else.
    Mover shall freeze the volume.
<tr>
<td valign=top><b>READ_DRIVEBUSY</b>
<td valign=top>"The Media Changer says that some other volume is in the drive"
    Need to investigate if this could ever be a clean volume otherwise a bug in
    Enstore or a misconfiguration.     Mover shall offline the drive.
    <em>Encp</em> shall retry.
<tr>
<td valign=top><b>READ_BADMOUNT</b>
<td valign=top>Other Mount failure on read, or load operation failed.
    We have to assume that the volume is jammed in the drive.
    Mover shall freeze the volume in the drive.
<tr>
<td valign=top><b>READ_BADLOCATE</b>
<td valign=top>Failed space or initial CRC's don't match on initial read.
    We have attempted to space to a file on the volume. Either the file
    location
    cookie is somehow corrupted, we have the wrong volume in the drive or
    the volume drive cannot space properly.
    Mover shall freeze the volume in the drive.
    <em>Encp</em> shall not retry
<tr>
<td valign=top><b>READ_ERROR</b>
<td valign=top>Error when reading data block.
    Run of the mill read error.
    Mover shall consider the recent error history of the drive and the volume.
    If the drive has had many recent errors, Mover shall offline the drive.
    <em>Encp</em> shall retry.
    If the volume has had many recent errors, Mover shall freeze the volume.
    <em>Encp</em> shall not retry.
    If the volume has had few recent errors,
    Mover shall fail the transfer,
    <em>encp</em> shall retry, and annotate that this Mover shall be avoided.
<tr>
<td valign=top><b>READ_COMP_CRC</b>
<td valign=top>Failure of complete CRC.
    This needs investigating, as it should not happen. The drive and the volume
    are suspicious. Corrupt file location cookie, drive space error, wrong
    volume in the drive, etc.
    Mover shall freeze the volume in the drive.
    <em>Encp</em> shall not retry.
<tr>
<td valign=top><b>READ_EOT</b>
<td valign=top>Hit EOT when reading.
    Should not happen. Corrupt file location cookie, drive space error, or
    wrong volume in the drive. Should have hit an EOF.
    Mover shall freeze the volume in the drive.
    <em>Encp</em> shall not retry.
<tr>
<td valign=top><b>READ_EOD</b>
<td valign=top>Hit EOD when reading.
    Should not happen. Corrupt file location cookie, drive space error, or
    wrong volume in the drive. Should have hit an EOF.
    Mover shall freeze the volume in the drive.
    <em>Encp</em> shall not retry.
<tr>
<td valign=top><b>READ_UNLOAD</b>
<td valign=top>Error when unloading volume.
    Must assume that the volume is physically jammed.
    Mover shall freeze the volume in the drive.
    No <em>encp</em> is associated with the volume at this time.
<tr>
<td valign=top><b>READ_UNMOUNT</b>
<td valign=top>Error from Media Changer when unloading volume.
    Should not happen. Maybe the volume is hanging in the drive. Does no harm.
    Mover shall freeze the volume in the drive.
    No <em>encp</em> is associated with the volume at this time
<tr>
<th><font size=+1><b>Other Errors</b></font>
<tr>
<td valign=top><b>ENCP_GONE</b>
<td valign=top><em>Encp</em> has gone away while request is queued.
    The Mover contacts <em>encp</em> to ping it before a mount, and if
    <em>encp</em> is not there, the Mover does not go on to contact the Media
    Changer, but instead returns a UNILATERAL_UNBIND to the Library Manager.
<tr>
<td valign=top><b>TCP_HUNG</b>
<td valign=top>DESY reports that
<ul>
<li>sometimes machines can crash....
<li><em>encp</em> processes can be killed in a way that...
<li><em>encp</em> client's disks many fill up ...
<li>etc...
</ul>
which are all seen as the data TCP link is seen to
just hang.
<p>
DESY's treatment is "harsh" -- They compute an anticipated transfer
time for every socket operation and abort the transfer if the
actual transfer takes more than several times the expected value.
Right now, Enstore does not consider this to be an error and just
waits. However, we are vulnerable to disaster. It is possible to see
circumstances where we might tie all the Movers up if, say, one clients
system's disk fills up.
<tr>
<td valign=top><b>LM_CRASH</b>
<td valign=top>If a Library Manager crashes, and loses its queue of pending work,
    the <em>encp</em>'s will never be called back, and will wait
    forever.
    This is a problem. A basis for the solution is for a waiting
    <em>encp</em> to ping its Library Manager,
    every 30 mins to see if its request has gotten lost.
    It would also be good if a --timeout switch was available to
    <em>encp</em>,
    since the users may wish to get an error rather than suffer a long
    delay.
<tr>
<td valign=top><b>MOVER_CRASH</b>
<td valign=top>
Case 1) If a Mover is connected to an <em>encp</em>, then <em>encp</em> will
notice its sockets being torn down prematurely. <em>encp</em> shall retry. This
is not good enough.
<p>
For reads, the volume is tied up because the Library Manager has the volume as
being "at a mover". The retry will be hung in the Library Manager queue
because the Library Manager is unaware of the Mover crash.
<p>
For writes, 1 is effectively deducted from the file family width,
since the Library Manager is unaware of the Mover crash.
<p>
This is all a bug, the minimal effect of a Mover crash should be:
<ul>
<li>
Since the status of the volume is unknown, the status should be akin
to "freeze drive and volume". However, subsequent reads will not produce an
error, but will be queued until the Mover is restarted.
<li>
Allow writes to continue on a different volume of the same file
family, without affecting the "width".
</ul>
<p>
Case 2) Mover has a volume, but is not associated with an <em>encp</em> and
crashes.
Same problems as case 1) but <em>encp</em> will notice.
The total amount of time spent in this possibility is small compared to
the time corresponding to case 1 and case 3.
<p>
Case 3) Mover is idle.
This is O.K. The system degrades as specified.
</table>
</body>
