<body>
<font size=7><b><i>
Enstore Data Storage System
</i></b></font>
<p>
<font size=+3><b>Table of Contents</b></font>
<ul>
<li><a href="#overview">1 Overview of Enstore Architecture</a>
<li><a href="#pnfs">1.1 pnfs Namespace</a>
<li><a href="#volmaps">1.1.1 Volume Maps</a>
<li><a href="#encp">1.2 ENCP</a>
<li><a href="#encp_throttle">1.2.1 Throttling in <em>encp</em></a>
<li><a href="#encp_list_from hsm">1.2.2 Specifying lists of files when reading from the HSM</a>
<li><a href="#servers">1.3 Enstore servers</a>
<li><a href="#volume_clerk">1.3.1 Volume Clerk</a>
<li><a href="#file_clerk">1.3.2 File Clerk</a>
<li><a href="#library_manager">1.3.3 Library Manager</a>
<li><a href="#user_request">1.3.3.1 Users' Requests</a>
<li><a href="#mover_request">1.3.3.2 Movers' Requests</a>
<li><a href="#pub_request">1.3.3.3 Publisher Requests</a>
<li><a href="#mover">1.3.4 Mover</a>
<li><a href="#config_server">1.3.5 Configuration Server</a>
<li><a href="#log_server">1.3.6 Log Server</a>
<li><a href="#media_changer">1.3.7 Media Changer</a>
<li><a href="#db">2 Databases in Enstore</a>
<li><a href="#backup_recovery">2.1 Backup and Recovery Procedures</a>
<li><a href="#backup">2.1.1 Backup </a>
<li><a href="#recovery">2.2.2 Recovery</a>
<li><a href="#current_db">2.2 Current Underlying Database Implemented in Enstore</a>
<li><a href="#admin_tools">2.3 Administrative Tools</a>
<li><a href="#protocol">3 Communication Protocols</a>
<li><a href="#read_protocol">3.1 Read Protocol</a>
<li><a href="#write_protocol">3.2 Write Protocol</a>
<li><a href="#error">4 Error Control</a>
</ul>

<p>
<h2><a name="overview">
1 Overview of Enstore Architecture
</h2>

Enstore uses a client-server architecture to provide a generic interface for users
to efficiently use mass storage systems.
Enstore supports multiple distributed media robots,
each of which may handle multiple media types or individual directly attached
drives, and multiple distributed mover nodes.
The system architecture does not dictate an exact hardware architecture.
Rather, it specifies a set of general and generic networked hardware
and software components.
These components are loosely coupled in the sense that each one can be
replaced easily without affecting the rest of the system and
each class of components can be easily expanded to accommodate
the increased demand in performance or capacity.
<p>
The system is written in <em>python</em>, a scripting
language that has advanced object-oriented features.
Python provides a sound environment for quick turn-around
and a seamless integration/migration path to
fully compiled languages, such as C and C++, if there is a demand
for even better performance.
<p>
Enstore has four major kinds of software components:
<ul>
<li><em>namespace</em>, implemented by the <em>pnfs</em> package from
        <em>DESY</em>
<li><em>encp</em>, a program used to copy files to and from media libraries
<li><em>servers</em>
        <ul>
        <li>Configuration Server
        <li>Volume Clerk
        <li>File Clerk
        <li>Multiple, distributed Library Managers
        <li>Multiple, distributed Movers
        <li>Media Changer
        <li>Log Server
        </ul>
<li><em>administration tools</em>
</ul>
These software components, as well as hardware components,
are shown schematically in the following system context diagram.
Hardware components are connected via IP.
Great care has been taken to ensure that the system will function well
under extreme load conditions.
By design, there is no preset limit on the number of concurrent user
computers nor on the number of physical media libraries or drives.
The system is only limited by the availability of physical resources.
We control all of the source code for the system except for that of
<em>pnfs</em> (which is a well supported product from DESY).
<p>
<img src="enstoreSimple.gif">
<p>
<a href="enstoreSimple.ps">(also available in Postscript)</a>
<p>
Enstore does not yet support a disk cache or buffer in front of the media.
However, one is conceivable and planned for a future release.
<p>
Like <em>tcp</em>, the system is architected with distributed and
peer-to-peer reliability.
Each request originating from the <em>encp</em> program is branded
with a unique ID.
<em>Encp</em> retries under well-defined circumstances, issuing
an equivalent request with a new unique ID.
The system can instruct <em>encp</em> to retry if it needs to back out
of an operation.

<h2><a name="pnfs">
1.1 Pnfs Namespace
</h2>
The DESY <a href=http://mufasa.desy.de/pnfs/Welcome.html><em>pnfs</em>
(http://mufasa.desy.de/pnfs/Welcome.html) </a>package implements an <em>nfs</em> daemon and
mount daemon.
These daemons do not actually serve a file system, but, instead make a
collection of database entries looks like a file system,
and provide control information for the system.
<p>
To inspect files, users mount their portion of the <em>pnfs</em> file
system on their own computers,
and interact with it using the native operating system utilities.
For example, users can <em>ls</em>, <em>stat</em>,
<em>mv</em>, <em>rm</em> or <em>touch</em> existing "files",
but are given errors on attempts to read
or write the content of the files.
Users can also <em>mkdir</em> and <em>rmdir</em>, and <em>ln</em> files. [Some
special files can also contain data that administrators can write to and users
can read from, so in that sense pnfs also provides normal files as well.]
<p>
Normal UNIX permissions and administered export points
are used to prevent unauthorized access to the name space.
<p>
Enstore uses pnfs for three different kinds of access and information:
<ol>
<li>Administration Interactions
    <br>
    An administrator can create special files in <em>pnfs</em> name space.
    For example, one type of file signifies that the system needs to be
    drained. Existence of the file causes <em>encp</em> to stall,
    preventing users from submitting additional jobs.
    <p>
<li>Configuration Information
    <br>
    When files are created on some media some creation details
    need to be provided. Enstore uses pnfs tag files for these purposes.
    These files are created in a specific part of the name space and are
     associated with a directory and not any specific file.
    Examples of configuration information include file family name, file
     family width, Library Manager and so forth.
     <p>

<li>User File information
    <br>
    The rest of the system identifies a file by a 64-bit numeric
    identifier, dubbed a "bit file ID".
    After a file is written, the File Clerk generates a bfid and
     <em>encp</em>     stores this information
     in one of the pnfs file layers.
    <em>Encp</em> then reads this bit file ID and gives it to the servers
    when fetching data.
     Other encp file transfer details, such as time of last access or location of
     where the file was copied to or transfer rates, are stored in a different
     layer of the same pnfs file.
</ol>

Pnfs was tested in the prototype with very good results. The code was run
on a 200 MHz Pentium Pro Linux with scsi disks.  The pnfs code had not been
run extensively on linux machines before and there a few minor glitches
were found during initial running. Support from DESY was outstanding and
all problems were solved quickly.

As a test of pnfs' capabilities, the current set of 250K HPSS filenames
were put into pnfs. Approximately 20 pnfs databases were used for this
test, with each database corresponding to a HPSS "experimental"
separation. [That is, D0 had its own database, SDSS had its own, and so
forth.] Name lookup was done on each database simultaneously from 3
machines (IRIX, Linux, and AIT) for several days.  Pnfs performed
flawlessly and was able to provide names at a rate of 3-15 names/sec. The
database can be further optimized, but this performance is already adequate
for Enstore.



<h3><a name="volmaps">
1.1.1 Volume Maps</em>
</h3>

In order to do request files from a tape in an optimal transversal, it is
necessary for the user to know the order of the files on a tape.  To aid in
this process, encp creates a duplicate entry in pnfs (in addition to storing
the filename and its metadata).  This duplicate entry is organized by file
families and volumes instead of by the user's directory structure.  The naming
convention for the duplicate entries is such that the file order on the tape
could be readily known.  For example, for sequential files on tape, the name
is just the file number; for files one a disk, it is just the byte
offset. When a user wanted to know what was on a particular tape, he would
just look in the duplicate tree to get the files - then he would read the
necessary information about the linked file from one of the pnfs layers (such
as the bfid, original file name...) to make his transfer request to the mass
storage.

Here is an example:

Let's say a user transferred a file to mas storage and called it
/pnfs/enstore/airedale/one.  The file was stored on disk floppy named
"flop301"  with a byte offset=0 and in a file family named "jon".

<pre>
$ ls -alsFt /pnfs/enstore/airedale/one
   0 -rw-rw-r--   1 bakken   root           26 Jul 21 12:09 /pnfs/enstore/airedale/one
</pre>

The duplicate entry is /pnfs/enstore/volmap/jon/flop301/000000000000
In the general case, "jon" would be replaced by the file family name,
"flop301" would become the volume name and "000000000000" would be the file
order on tape.

<pre>
$ ls -alsFt pnfs/enstore/volmap/jon/flop301/000000000000
   0 -rw-r--r--   1 root     root           26 Jul 21 12:09 /pnfs/enstore/volmap/jon/flop301/000000000000
</pre>

The original file and the duplicate file contain cross reference information
to allow the user to get to the other one and enough information to setup a
transfer.

<pre>
Linux-airedale[448] 09:20:35 jon$ pcmd info /pnfs/enstore/airedale/one
bfid="90104096300000L";
volume="flop301";
cookie="(0, 512)";
file_family="jon";
orig_name="/pnfs/enstore/airedale/one";
map_file="/pnfs/enstore/volmap/jon/flop301/000000000000";

$ pcmd info  /pnfs/enstore/volmap/jon/flop301/000000000000
bfid="90104096300000L";
volume="flop301";
cookie="(0, 512)";
file_family="jon";
orig_name="/pnfs/enstore/airedale/one";
map_file="/pnfs/enstore/volmap/jon/flop301/000000000000";
</pre>

Consider a few cases.
<ul>
<li> If a user wants dozens of files, she can lookup up the volume name where
each file is stored and group her requests so files on one tape are grouped
together.
<li>If a user wants a dozen files from a certain file family but doesn't care
which ones, she can look into the volmap area and pick out files on a specific
volume and set up the request that way.
</ul>

Finally, since the user will not have delete authority to the duplicate
volmap file, this mechanism gives us an automatic trash can facility.  If the
user deletes his entry in pnfs, we still have the duplicate entry and can
"easily" restore the original.


<h2><a name="encp">
1.2 ENCP
</h2>
Reading and writing files means interacting with media.
This is done with an enstore provided utility, <em>encp</em>.
<em>encp</em> is very similar to the <em>cp</em> command in UNIX.
The syntax is:
<pre>
% encp [options] src_file dst_file
</pre>
Currently there is no wildcarding allowed, but this is a straight forward
extension to encp.

<h3><a name="encp_throttle">
1.2.1 Throttling in <em>encp</em>
</h3>

It is important not to swamp any system.
In <em>Enstore</em>, a first level of throttling is implemented in
<em>encp</em>.
Control communications in <em>Enstore</em> uses a simple reliable
request-response protocol implemented in <em>UDP</em>, whereas data transfers
are implemented using two TCP ports.
A fixed number, currently 30, of pre-allocated TCP ports are
arbitrated among all instances of <em>encp</em> on a given machine.
Consequently, the system will survive the worst sort of abuse,
for example, someone forking off 200 copy requests,
since at most 15 will be active in the system at any time.

<h3><a name="encp_list_from hsm">
1.2.2 Specifying lists of files when reading from the HSM
</h3>

Encp duplicates unix cp's behaviour when one tries to read multiple input
files from the HSM: <br> ecmd encp [options] source... directory<br> That
is, the final item has to be a directory when specifying an input list.
There is 1 caveat when reading multiple files using a single encp command:
there is at most 1 mover processing your request list at a time.  Encp
scans all your files and groups them according to which volume they are on
and then submits all the file requests that are on one specific volume,
reads all the files for that volume from the hsm and then proceeds to the
next volume.  If you want more than mover active, you should use multiple
encp commands when you are reading lists of files from more than one
volume.  The reason why encp uses just one mover is to limit the amount of
forking it does (and if you want to many forks, then just use many encp
commands).

Consider the following example (P=/pnfs/enstore/airedale)

There are the following files on flop301: ran-1, ran-2 ran-3, ran-4<br>
There are the following files on flop302: ran-5, ran-6 ran-7, ran-8<br>
There are the following files on flop302: ran-9, ran-10 ran-11, ran-12<br>

Encp submits the requests for all files on flop301 and reads back those
files and then does the same for flop302 and flop303. Encp does the volumes
in any order it chooses.

Here is the output of an actual test:
<pre>
$ ecmd encp $P/ran-1       $P/ran-2        $P/ran-3        $P/ran-4 \
            $P/test2/ran-5 $P/test2/ran-6  $P/test2/ran-7  $P/test2/ran-8 \
            $P/test3/ran-9 $P/test3/ran-10 $P/test3/ran-11 $P/test3/ran-12 .
/pnfs/enstore/airedale/test2/ran-5 -> /home/bakken/enstore/src/ran-5 : 102400 bytes copied from flop302 at 0.190780317582 MB/S  requestor:bakken     cum= 3.534426
/pnfs/enstore/airedale/test2/ran-6 -> /home/bakken/enstore/src/ran-6 : 102400 bytes copied from flop302 at 0.426923710317 MB/S  requestor:bakken     cum= 3.787567
/pnfs/enstore/airedale/test2/ran-7 -> /home/bakken/enstore/src/ran-7 : 102400 bytes copied from flop302 at 0.462479436263 MB/S  requestor:bakken     cum= 3.999860
/pnfs/enstore/airedale/test2/ran-8 -> /home/bakken/enstore/src/ran-8 : 102400 bytes copied from flop302 at 0.462523304015 MB/S  requestor:bakken     cum= 4.212137
/pnfs/enstore/airedale/test3/ran-9 -> /home/bakken/enstore/src/ran-9 : 102400 bytes copied from flop303 at 0.129460905635 MB/S  requestor:bakken     cum= 5.479408
/pnfs/enstore/airedale/test3/ran-10 -> /home/bakken/enstore/src/ran-10 : 102400 bytes copied from flop303 at 0.491317380712 MB/S  requestor:bakken     cum= 5.679356
/pnfs/enstore/airedale/test3/ran-11 -> /home/bakken/enstore/src/ran-11 : 102400 bytes copied from flop303 at 0.454401235403 MB/S  requestor:bakken     cum= 5.895427
/pnfs/enstore/airedale/test3/ran-12 -> /home/bakken/enstore/src/ran-12 : 102400 bytes copied from flop303 at 0.455049173862 MB/S  requestor:bakken     cum= 6.111198
/pnfs/enstore/airedale/ran-1 -> /home/bakken/enstore/src/ran-1 : 102400 bytes copied from flop301 at 0.124436148701 MB/S  requestor:bakken     cum= 7.414610
/pnfs/enstore/airedale/ran-2 -> /home/bakken/enstore/src/ran-2 : 102400 bytes copied from flop301 at 0.45439695057 MB/S  requestor:bakken     cum= 7.630682
/pnfs/enstore/airedale/ran-3 -> /home/bakken/enstore/src/ran-3 : 102400 bytes copied from flop301 at 0.422761844505 MB/S  requestor:bakken     cum= 7.862842
/pnfs/enstore/airedale/ran-4 -> /home/bakken/enstore/src/ran-4 : 102400 bytes copied from flop301 at 0.454517460805 MB/S  requestor:bakken     cum= 8.078855
</pre>

Here is the same transfer, but this time with the "--list" option enabled,
so more details of the transfers are printed out:
<pre>
$ ecmd encp --list $P/ran-1       $P/ran-2        $P/ran-3        $P/ran-4 \
                   $P/test2/ran-5 $P/test2/ran-6  $P/test2/ran-7  $P/test2/ran-8 \
                   $P/test3/ran-9 $P/test3/ran-10 $P/test3/ran-11 $P/test3/ran-12 .
Storing/checking  local info   cum= 0.00021493434906
  dt: 0.00107896327972    cum= 0.00155103206635
Checking input pnfs files: ['/pnfs/enstore/airedale/ran-1', '/pnfs/enstore/airedale/ran-2', '/pnfs/enstore/airedale/ran-3', '/pnfs/enstore/airedale/ran-4', '/pnfs/enstore/airedale/test2/ran-5', '/pnfs/enstore/airedale/test2/ran-6', '/pnfs/enstore/airedale/test2/ran-7', '/pnfs/enstore/airedale/test2/ran-8', '/pnfs/enstore/airedale/test3/ran-9', '/pnfs/enstore/airedale/test3/ran-10', '/pnfs/enstore/airedale/test3/ran-11', '/pnfs/enstore/airedale/test3/ran-12']    cum= 0.00179898738861
  dt: 0.834562063217    cum= 0.836575031281
Checking output unix files: ['/home/bakken/enstore/src/.']    cum= 0.836683034897
  ['/home/bakken/enstore/src/ran-1', '/home/bakken/enstore/src/ran-2', '/home/bakken/enstore/src/ran-3', '/home/bakken/enstore/src/ran-4', '/home/bakken/enstore/src/ran-5', '/home/bakken/enstore/src/ran-6', '/home/bakken/enstore/src/ran-7', '/home/bakken/enstore/src/ran-8', '/home/bakken/enstore/src/ran-9', '/home/bakken/enstore/src/ran-10', '/home/bakken/enstore/src/ran-11', '/home/bakken/enstore/src/ran-12']
  dt: 0.80575799942    cum= 1.64285504818
Requesting callback ports    cum= 1.64292299747
  airedale.fnal.gov 7613
  dt: 0.0070469379425    cum= 1.65014898777
Calling Config Server to find file clerk    cum= 1.6502250433
  pcfarm9 7511
  dt: 0.00744593143463    cum= 1.65786099434
Calling file clerk for file info    cum= 1.65792798996
  dt: 0.747581005096    cum= 2.40566301346
Sending ticket to file clerk    cum= 2.4057289362
  Q'd: /pnfs/enstore/airedale/test2/ran-5 90121648900000L bytes: 102400 on flop302 (516608, 619520) dt: 0.125765   cum=2.531657
  Q'd: /pnfs/enstore/airedale/test2/ran-6 90121649400000L bytes: 102400 on flop302 (619520, 722432) dt: 0.251316   cum=2.657218
  Q'd: /pnfs/enstore/airedale/test2/ran-7 90121651400000L bytes: 102400 on flop302 (722432, 825344) dt: 0.452826   cum=2.858725
  Q'd: /pnfs/enstore/airedale/test2/ran-8 90121652100000L bytes: 102400 on flop302 (825344, 928256) dt: 11.345965   cum=13.751878
  dt: 11.3462849855    cum= 13.752177
Waiting for mover to call back    cum= 13.7522759438
  airedale.fnal.gov 7614 cum: 13.7816929817
  dt: 0.0294079780579    cum= 13.7819700241
Receiving data for file  /home/bakken/enstore/src/ran-5    cum= 13.7820539474
  bytes: 102400  Socket read Rate =  1.99837046939  MB/s
  dt: 0.0488679409027    cum= 13.8313089609
Waiting for final mover dialog    cum= 13.8313920498
  dt: 0.119197010994    cum= 13.9508379698
/pnfs/enstore/airedale/test2/ran-5 -> /home/bakken/enstore/src/ran-5 : 102400 bytes copied from flop302 at 0.491777789917 MB/S  requestor:bakken     cum= 13.951014
Waiting for mover to call back    cum= 13.9696760178
  airedale.fnal.gov 7615 cum: 13.9908440113
  dt: 0.0211460590363    cum= 13.9911179543
Receiving data for file  /home/bakken/enstore/src/ran-6    cum= 13.9912029505
  bytes: 102400  Socket read Rate =  2.06947075913  MB/s
  dt: 0.0471889972687    cum= 14.0387710333
Waiting for final mover dialog    cum= 14.0388560295
  dt: 0.119550943375    cum= 14.1586530209
/pnfs/enstore/airedale/test2/ran-6 -> /home/bakken/enstore/src/ran-6 : 102400 bytes copied from flop302 at 0.516757071819 MB/S  requestor:bakken     cum= 14.158835
Waiting for mover to call back    cum= 14.1599019766
  airedale.fnal.gov 7616 cum: 14.2008810043
  dt: 0.0409680604935    cum= 14.2011679411
Receiving data for file  /home/bakken/enstore/src/ran-7    cum= 14.2012529373
  bytes: 102400  Socket read Rate =  2.36295877514  MB/s
  dt: 0.0413279533386    cum= 14.2429490089
Waiting for final mover dialog    cum= 14.2430330515
  dt: 0.114647984505    cum= 14.3579289913
/pnfs/enstore/airedale/test2/ran-7 -> /home/bakken/enstore/src/ran-7 : 102400 bytes copied from flop302 at 0.493126210693 MB/S  requestor:bakken     cum= 14.358107
Waiting for mover to call back    cum= 14.359153986
  airedale.fnal.gov 7617 cum: 14.3949129581
  dt: 0.0357400178909    cum= 14.3951740265
Receiving data for file  /home/bakken/enstore/src/ran-8    cum= 14.3953809738
  bytes: 102400  Socket read Rate =  2.15405489264  MB/s
  dt: 0.0453360080719    cum= 14.4412109852
Waiting for final mover dialog    cum= 14.4412950277
  dt: 0.122217059135    cum= 14.5637539625
/pnfs/enstore/airedale/test2/ran-8 -> /home/bakken/enstore/src/ran-8 : 102400 bytes copied from flop302 at 0.477291649552 MB/S  requestor:bakken     cum= 14.563928
udp_client.send: read old info: (16, {'unique_id': 901216851.972, 'bfid': '90121652100000L', 'file_clerk': {'bof_space_cookie': '(825344, 928256)', 'sanity_cookie': '(5000, 33638)', 'external_label': 'flop302', 'bfid': '90121652100000L', 'complete_crc': 30442}, 'sanity_cookie': '(5000, 33638)', 'complete_crc': 30442, 'pnfs_info': {'minor': 5, 'mode': 33268, 'gid': 0, 'gname': 'root', 'rminor': 0, 'uid': 5406, 'pnfsFilename': '/pnfs/enstore/airedale/test2/ran-8', 'pstat': (33204, 38375816, 5, 1, 5406, 0, 102400, 901216512, 901216512, 901216521), 'rmajor': 0, 'uname': 'bakken', 'major': 0}, 'user_info': {'gname': 'g023', 'uid': 5406, 'gid': 1530, 'fullname': '/home/bakken/enstore/src/ran-8', 'uname': 'bakken', 'machine': ('Linux', 'airedale', '2.0.35', '#1 Mon Jul 20 08:54:09 CDT 1998', 'i686')}, 'bof_space_cookie': '(825344, 928256)', 'sanity_size': 5000, 'external_label': 'flop302', 'user_callback_port': 7613, 'work': 'read_from_hsm', 'user_callback_host': 'airedale.fnal.gov', 'status': 'ok'}, 901216872.506) ('131.225.97.10', 7511)
  Q'd: /pnfs/enstore/airedale/test3/ran-9 90121653300000L bytes: 102400 on flop303 (928256, 1031168) dt: 0.136023   cum=14.701104
  Q'd: /pnfs/enstore/airedale/test3/ran-10 90121653900000L bytes: 102400 on flop303 (1031168, 1134080) dt: 0.287928   cum=14.853003
  Q'd: /pnfs/enstore/airedale/test3/ran-11 90121655100000L bytes: 102400 on flop303 (1134080, 1236992) dt: 0.413479   cum=14.978561
  Q'd: /pnfs/enstore/airedale/test3/ran-12 90121655600000L bytes: 102400 on flop303 (1236992, 1339904) dt: 0.541382   cum=15.106496
  dt: 12.7008770704    cum= 15.1067709923
Waiting for mover to call back    cum= 15.1068719625
  airedale.fnal.gov 7618 cum: 15.6871869564
  dt: 0.580304026604    cum= 15.6874320507
Receiving data for file  /home/bakken/enstore/src/ran-9    cum= 15.6875170469
  bytes: 102400  Socket read Rate =  1.34824877428  MB/s
  dt: 0.0724319219589    cum= 15.7603620291
Waiting for final mover dialog    cum= 15.7604500055
  dt: 0.0972409248352    cum= 15.8579269648
/pnfs/enstore/airedale/test3/ran-9 -> /home/bakken/enstore/src/ran-9 : 102400 bytes copied from flop303 at 0.130022665084 MB/S  requestor:bakken     cum= 15.858102
Waiting for mover to call back    cum= 15.859210968
  airedale.fnal.gov 7619 cum: 15.9228279591
  dt: 0.0635979175568    cum= 15.9230870008
Receiving data for file  /home/bakken/enstore/src/ran-10    cum= 15.9231729507
  bytes: 102400  Socket read Rate =  1.52604351981  MB/s
  dt: 0.0639930963516    cum= 15.9875530005
Waiting for final mover dialog    cum= 15.9876400232
  dt: 0.0923020839691    cum= 16.0801730156
/pnfs/enstore/airedale/test3/ran-10 -> /home/bakken/enstore/src/ran-10 : 102400 bytes copied from flop303 at 0.441947460475 MB/S  requestor:bakken     cum= 16.080348
Waiting for mover to call back    cum= 16.0814180374
  airedale.fnal.gov 7620 cum: 16.1459280252
  dt: 0.0644949674606    cum= 16.1461839676
Receiving data for file  /home/bakken/enstore/src/ran-11    cum= 16.1462689638
  bytes: 102400  Socket read Rate =  1.42897512202  MB/s
  dt: 0.0683400630951    cum= 16.2150000334
Waiting for final mover dialog    cum= 16.2150889635
  dt: 0.0967879295349    cum= 16.3121169806
/pnfs/enstore/airedale/test3/ran-11 -> /home/bakken/enstore/src/ran-11 : 102400 bytes copied from flop303 at 0.423289390535 MB/S  requestor:bakken     cum= 16.312295
Waiting for mover to call back    cum= 16.3133749962
  airedale.fnal.gov 7621 cum: 16.3780419827
  dt: 0.0646479129791    cum= 16.3782949448
Receiving data for file  /home/bakken/enstore/src/ran-12    cum= 16.3783789873
  bytes: 102400  Socket read Rate =  1.48648697691  MB/s
  dt: 0.0656960010529    cum= 16.4444799423
Waiting for final mover dialog    cum= 16.4445669651
  dt: 0.0912410020828    cum= 16.5361510515
/pnfs/enstore/airedale/test3/ran-12 -> /home/bakken/enstore/src/ran-12 : 102400 bytes copied from flop303 at 0.438329425538 MB/S  requestor:bakken     cum= 16.536341
  Q'd: /pnfs/enstore/airedale/ran-1 90121542000000L bytes: 102400 on flop301 (2048, 104960) dt: 0.129802   cum=16.667315
  Q'd: /pnfs/enstore/airedale/ran-2 90121542700000L bytes: 102400 on flop301 (104960, 207872) dt: 0.260114   cum=16.797613
  Q'd: /pnfs/enstore/airedale/ran-3 90121543200000L bytes: 102400 on flop301 (207872, 310784) dt: 0.390148   cum=16.927652
  Q'd: /pnfs/enstore/airedale/ran-4 90121543600000L bytes: 102400 on flop301 (310784, 413696) dt: 0.517002   cum=17.054500
  dt: 14.6489070654    cum= 17.0548000336
Waiting for mover to call back    cum= 17.0549010038
  airedale.fnal.gov 7622 cum: 17.6872760057
  dt: 0.632364988327    cum= 17.687525034
Receiving data for file  /home/bakken/enstore/src/ran-1    cum= 17.6876089573
  bytes: 102400  Socket read Rate =  1.47265556183  MB/s
  dt: 0.0663130283356    cum= 17.7543139458
Waiting for final mover dialog    cum= 17.7544020414
  dt: 0.0931440591812    cum= 17.8477829695
/pnfs/enstore/airedale/ran-1 -> /home/bakken/enstore/src/ran-1 : 102400 bytes copied from flop301 at 0.123163392809 MB/S  requestor:bakken     cum= 17.847959
Waiting for mover to call back    cum= 17.8490309715
  airedale.fnal.gov 7623 cum: 17.9164079428
  dt: 0.0673609972    cum= 17.9166640043
Receiving data for file  /home/bakken/enstore/src/ran-2    cum= 17.9167480469
  bytes: 102400  Socket read Rate =  0.66466315728  MB/s
  dt: 0.146925926208    cum= 18.0640599728
Waiting for final mover dialog    cum= 18.0641460419
  dt: 0.0902429819107    cum= 18.1546230316
/pnfs/enstore/airedale/ran-2 -> /home/bakken/enstore/src/ran-2 : 102400 bytes copied from flop301 at 0.319555867275 MB/S  requestor:bakken     cum= 18.154798
Waiting for mover to call back    cum= 18.1558859348
  airedale.fnal.gov 7624 cum: 18.2239129543
  dt: 0.0680110454559    cum= 18.224167943
Receiving data for file  /home/bakken/enstore/src/ran-3    cum= 18.2242510319
  bytes: 102400  Socket read Rate =  1.40887460294  MB/s
  dt: 0.0693150758743    cum= 18.293956995
Waiting for final mover dialog    cum= 18.2940440178
  dt: 0.0940099954605    cum= 18.3883080482
/pnfs/enstore/airedale/ran-3 -> /home/bakken/enstore/src/ran-3 : 102400 bytes copied from flop301 at 0.42014781079 MB/S  requestor:bakken     cum= 18.388487
Waiting for mover to call back    cum= 18.3895900249
  airedale.fnal.gov 7625 cum: 18.4557709694
  dt: 0.0661630630493    cum= 18.4560240507
Receiving data for file  /home/bakken/enstore/src/ran-4    cum= 18.4561079741
  bytes: 102400  Socket read Rate =  1.43308480601  MB/s
  dt: 0.0681440830231    cum= 18.524641037
Waiting for final mover dialog    cum= 18.5247290134
  dt: 0.0877720117569    cum= 18.6127489805
/pnfs/enstore/airedale/ran-4 -> /home/bakken/enstore/src/ran-4 : 102400 bytes copied from flop301 at 0.437588604748 MB/S  requestor:bakken     cum= 18.612926
Complete:  1228800  bytes in  12  files  in 18.6140960455 S.  Overall rate =  0.0629563206903  MB/s
</pre>

<h2><a name="servers">
1.3 Enstore servers
</h2>

Enstore servers are software entities which handle media, and in a
future release, disk caches.
The high level  concepts are as follows:
<dl>
<dt><em>Physical library</em>
<dd>Physical Library represents a real, tangible collection of media
along with software drivers/utilities to manipulate, read and write
and organize them.

A physical library can be thought of as consisting of
<ul>
        <li>one or more virtual libraries
        <li>a media changer
        <li>one of more media export/import slots
        <li>one of more drives (tape, cdrom, disk, etc.)
        <li>volumes (tape cartridges, cdroms, etc.)
</ul>
<p>
<em>Virtual Library</em> -- A virtual library contains one and only one
kind of media.
For example, Enstore divides an STK powderhorn
library holding 50, 20 and 10 GB redwood media into at least three
virtual libraries.  In common usage, the term "library" in Enstore
refers to a virtual library. Writes are directed to a specific (virtual)
library, thus selecting the media.
<p>
<em>Drives</em> -- Drives are bound to special processes called Mover
clients.
The drives can be dynamically assigned allowing the number of drives
to be less than the number of virtual libraries.
<p>
<em>Volumes</em> -- Are uniquely identified by an external label,
which is known to the Media Changer.
<p>
<dt><em>Quota Family</em>
<dd>A quota family is a set of pairs of media names and maximum number of volumes.
All files are created with respect to a quota family.
Creation of a file is not allowed if the maximum number of volumes
in that family would be exceeded.
<p>
<dt><em>File family</em>:
<dd>A file family is specified by a name and an integer "width".
A file family is associated with every file creation.
Within a given library, <em>Enstore</em> keeps no more
than <em>"width"</em> volumes open for writing, and loads volumes on
no more than <em>"width"</em>
number of drives at any given moment.
This is not striping, but rather, the number of different volumes,
and hence different files, which can be active at one time.
Once a volume is associated with a file family, only files in that
family will be placed on the volume.
By design, there is no pre-set limit on the number of file families.
Clever use of file families will allow volumes to be faulted out to
"shelf", and also to decrease access times for subsequent reads.  When a file
family has filled all of its "width" media, new media, limited by a quota,
are drawn out of pool of blanks.
</dl>
Media ejected to shelf are put into a shelf virtual library and
are controlled by a shelf Library Manager.
Users are informed that this data is currently unavailable, and if
they really want the data, arrangements should be made to have the
media placed in a library which is accessible,
or get it manually later.
<p>
<h3><a name="volume_clerk">
1.3.1 Volume Clerk
</h3>
The Volume Clerk has a single table database.
There is one record for each volume known to the system.
The record is looked up by a key.
The key is the volume's external label.
The information tracked for each volume is described in the table below:
The default values are shown in parentheses ().
<table>
<td valign=top><b>Column Name</b>
<td valign=top><b>Type</b>
<td valign=top><b>Comments</b>
<tr>
<td valign=top>external_label
<td valign=top>string [primary_key]
<td valign=top>Volume name specified by user on volume creation; is used to
display volume meta-data.
<tr>
<td valign=top>file_family
<td valign=top>string ("none")
<td valign=top>File family name, specified by user on volume creation; only
files that belong to this family will be stored on this volume.
<tr>
<td valign=top>media_type
<td valign=top>string
<td valign=top>Specified at volume creation; implies the block-size; used for
writing.
<tr>
<td valign=top>library
<td valign=top>string
<td valign=top>Specified by user on volume declaration; defines which (virtual)
library currently holds the volume
<tr>
<td valign=top>first_access
<td valign=top>int (-1)
<td valign=top>Unix time when user issues the first write command to copy data to the volume.
Set by the Volume Clerk.
<tr>
<td valign=top>last_access
<td valign=top>int (-1)
<td valign=top>Unix time when  user last accessed the volume. Set by the Volume Clerk.
<tr>
<td valign=top>declared
<td valign=top>int
<td valign=top>Unix time when the volume is declared available to the system. Set by the Volume Clerk.
<tr>
<td valign=top>capacity_bytes
<td valign=top>64-bit int
<td valign=top>Specified by user on volume creation; estimate of the number of
bytes that would fit on the volume.
<tr>
<td valign=top>blocksize
<td valign=top>int
<td valign=top>Set by the Volume Clerk; derived from the the media type.
<tr>
<td valign=top>remaining_bytes
<td valign=top>64-bit int
<td valign=top>Specified by the user on volume creation; estimate of the number
of bytes that would fit on the volume; updated by the Volume Clerk every time
data are written to the media.
<tr>
<td valign=top>eod_cookie
<td valign=top>string ("none")
<td valign=top>Tells the driver how to space to the end of the volume; it is
driver specific; updated by the Volume Clerk when data are written on the
media.
<tr>
<td valign=top>wrapper
<td valign=top>string ("cpio")
<td valign=top>Wrapper method; currently specifies the format of the files on
the volume.
<tr>
<td valign=top>sum_rd_err
<td valign=top>int (0)
<td valign=top>Read error count; Volume Clerk increments this field when the Mover
receives an error while reading from the volume.
<tr>
<td valign=top>sum_rd_access
<td valign=top>int (0)
<td valign=top>Read access count; Volume Clerk increments this field
every time a file is read.
<tr>
<td valign=top>sum_wr_err
<td valign=top>int (0)
<td valign=top>Write error count; Volume Clerk increments this field when the
Mover receives an error while writing to the volume.
<tr>
<td valign=top>sum_wr_access
<td valign=top>int (0)
<td valign=top>Write access count; Volume Clerk increments this field every
time a file is written.
<tr>
<td valign=top>user_inhibit
<td valign=top>string (d:"none" or "readonly", "all")
<td valign=top>Specified by user at volume creation; access level for this
volume, updated by Volume Clerk.
<tr>
<td valign=top>system_inhibit
<td valign=top>string (d:"none" or "writing", "readonly", "full")
<td valign=top>Administrator generated limitation on the kind of access permitted to
this volume; update by Volume Clerk when data are written on the volume, an
error occurred while data were being written or the file size exceeded the
remaining number of bytes on the volume.
</table>
<p>
The Volume Clerk does the following operations:
<ul>
<li>show the name of all the volumes
<li>show volume information
<li>add a volume
<li>delete a volume
<li>find an appropriate volume on which to write the file
<li>change the number of remaining bytes on the volume
<li>set the number of read/write errors
<li>set the current status of the volume
<li>set the volume as readonly
<li>start/stop backup of volume journals
</ul>

<h3><a name="file_clerk">
1.3.2 File Clerk
</h3>
The File Clerk tracks files in the system. There is one record for each file
in the system.
The records are keyed.
The key is the string version of the bit file ID
The default values are shown in parentheses ().
The fields tracked are as follows:
<table>
<td valign=top><b>Column Name</b>
<td valign=top><b>Type</b>
<td valign=top><b>Comments</b>
<tr>
<td valign=top>bfid
<td valign=top>string [primary_key]
<td valign=top>bit file id; uniquely identifies every file in the system.
<tr>
<td valign=top>external_label
<td valign=top>string
<td valign=top>Volume name on which the file has been written; same as the
external_label in the volume table.
<tr>
<td valign=top>bof_space_cookie
<td valign=top>string
<td valign=top>Driver specific string telling how to space to the file on the
media. A lexical sort of all bof_space_cookies for a given volume will yield a optimized
traversal of the volume.
<tr>
<td valign=top>complete_crc
<td valign=top>int
<td valign=top>crc of all the bits sent by the user.
<tr>
<td valign=top>sanity_cookie
<td valign=top>string ("(0,0)")
<td valign=top>Number of bytes used for a sanity crc and the sanity crc
itself.  The sanity crc is just the normal crc but only for the 1st N bytes in
the file. This allows the Mover to check early in the transfer process that it
probably has the right user file selected; it at least will know if it has the
wrong file.
</table>
<p>
The File Clerk supports the following requests:
<ul>
<li>show bfid of all the files
<li>show file information
<li>start/stop backup of file journals
<li>assist in processing file read requests
</ul>

<h3><a name="library_manager">
1.3.3 Library Manager
</h3>
The Library Manager is a server which queues up and dispatches work for
a virtual library. There is one Library Manager for each virtual library.
It has three types of clients
<ol>
<li><em>Users</em> -- seeking to have their files read or written files.
<li><em>Movers</em> -- seeking to actually read or write files.
<li><em>Publishers</em> -- seeking HTML describing the library's current work.
</ol>
<h3><a name="user_request">
1.3.3.1 Users' Requests
</h3>
<dl>
<dt>Writes into the system
<dd>Based on the user's mss destination filename, a <em>pnfs</em> tag
associated with the destination directory, identifies
    the library for a write request allowing the <em>encp</em> program to compose
    a write request and contact the appropriate Library Manager
    directly.
    The Library Manager queues the work, and acknowledges the request.

<dt>Read from the system
<dd>Given the fact that users may mv the pnfs files, on reads from the system,
     <em>pnfs</em> can only provide the bit
    file ID associated with the file. In this case, <em>encp</em> contacts the
     bit File Clerk, and that
    software ultimately contacts the appropriate Library Manager to
    queue up the work.
</dl>
Work can be prioritized.
Larger priority numbers means higher priority.
Currently, write and read are both priority 1 for our test purposes.
Any priority mechanism should be able to be developed.
However, the system will exhaust all work for a volume,
given that it has been mounted, regardless of priority.
<p>
<h3><a name="mover_request">
1.3.3.2 Movers' Requests
</h3>
Movers seek to transport data between media and users over a TCP socket.
Movers contact Library Managers seeking work. If the Library Manager
has work, the Mover is requested to mount a volume, and report
back. When reporting back, the Mover may be told to contact a waiting
encp program and read or write a file.
The Mover may be told to unmount a volume as there is no more work for
the volume.
A Mover may have dismounted a volume unilaterally because it ran into
trouble.  This is summarized in the tables below
<table>
<td><b>Mover sends</b>
<td><b>Library Manager may respond</b>
<tr>
<td valign=top>idle_mover
<td>if work needs to be done - bind a volume; <br>or if no work - just acknowledge
<tr>
<td valign=top>have_bound_volume
<td>if reads/writes pending for the volume - read/write;  <br>or if no work - unbind volume
<tr>
<td>unilateral_unbind
<td>just acknowledge
</table>

<table>
<td valign=top><b>Library has just responded</b>
<td valign=top><b>Mover sends</b>
<td valign=top><b>Library Manager presumes</b>
<tr>
<td valign=top>bind or...<br>read or...<br>write
<td valign=top>idle_mover
<td valign=top>Mover crashed and was re-started
<tr>
<td valign=top>
<td valign=top>have_bound_volume
<td valign=top>look for work on that volume<br>
    if work, give it<br>
    if none, give unbind volume
<tr>
<td valign=top>
<td valign=top>unilateral_unbind
<td valign=top>take any work reserved for that Mover and put it back
   in the unassigned work queue
<tr>
<td valign=top>acknowledged a...<br>unilateral unbind or..<br>idle Mover
<td valign=top>idle_mover
<td valign=top>Mover is available for work, If more work available, bind a
    volume
<tr>
<td valign=top>
<td valign=top>have_bound_volume
<td valign=top>it has restarted, the Mover had a volume from a previous
    instance of me, tell it to unbind
<tr>
<td valign=top>
<td valign=top>Unilateral_unbind
<td valign=top>Just acknowledge
</table>

Note that if a Mover should crash holding a volume, the worst that can
happen is that the Library Manager will be unable to schedule work for
that volume. If the physical library has more than one drive, the system
should be able to continue servicing requests.
<p>
<h3><a name="pub_request">
1.3.3.3 Publisher Requests
</h3>
It is possible to query the Library Manager and get information about its
internal queues thereby knowing where work is.  The prototype has a
rudimentary implementation of this feature. More work needs to be done before
this can be considered functional.

<h3><a name="mover">
1.3.4 Mover
</h3>
A Mover task is bound to a single drive, and seeks to use that drive to service
read and write requests.  It communicates with the Library Manager in a
defined protocol, as just described.
<p>
The Mover is responsible for efficient data movement and as such is an
integral part of the system.  The architecture allows for performance critical
code to be written in "C" thus allowing efficient access to fundamental OS
features such as forking with minimal to no language overhead.
<p>
Although a Mover is bound to a drive, and a drive may serve more than one
virtual library, i.e., the Mover has a dynamic list of of Library Managers
that it is supposed to service.  This has two benefits.
First, since a Library Manager handles only one type of media, a drive which
handles multiple types of media (i.e. different capacity media) can be shared
without a static partitioning of the system.
Second, if we are partitioning resources in a library, we can assign a Library
Manager to each type of use.  For example, suppose Group A and Group B want to share
the capacity of a library.
Suppose half the tapes belong to Group A  and the half to Group B. We want to guarantee that
Group A have one third of the tape drives, Group B have one third, and the last third
be shared.  The Movers can be configured to do this easily.  And with some slight
changes, this is how we can guarantee resources to data acquisition.
<p>
The Mover hunts Library Managers that have work when it is idle by
consulting a Configuration Server.
A Mover's configuration gives a list of Library Managers to hunt among
for work.
If there is no work at any Library Manager,
the Mover sleeps for a while and begins the hunt again.
While sleeping the Mover is sensitive to datagrams at a
specific UDP address. A Library Manager uses this mechanism to try to
hasten a Mover when the Library Manager has work queued up.
<p>
When a Mover has found a Library Manager that has work, it attempts
to mount the salient volume, by contacting the physical library's Media
Changer.  If there is some error, it issues a unilateral unbind to the
Library Manager. If all is well, it issues a have_bound_volume to the
Library Manager.
<p>
Reads -- Once a volume is bound the Mover may read a volume and send data
to a waiting encp program. Two tcp ports are involved.
        The steps are:
<ol>
<li>Arbitrate for a TCP port for the data transfer.
<li>Open the tcp port (control port) specified in the request,
    send the encp program the data transfer port address,
    hold the control port open. If it is dropped, abort.
<li>Read the data from a volume, (stripping any wrapper like tar headers)
    verify the sanity crc. send the data to the user.
<li>Close the data port.
<li>Tell the user done and all is well.
</ol>

If any errors occur while reading the volume, an attempt is made to
characterize them as either media or drive.  This is discussed more completely
in the section on Error control.
<p>
Writes -- Once a volume is bound the Mover may receive data and write it to
the volume. Two tcp ports are involved.
        The steps are:
<ol>
<li>Arbitrate for a TCP port for the data transfer.
<li>Open the tcp port (control port) specified in the request,
    send the encp program the data transfer port address,
    hold the control port open. If it is dropped, abort.
<li>Mark the volume as "writing". That will cause the volume to
    not be selected for subsequent writes, should we crash.
<li>Using the eod_space_cookie, space to end of volume. Try
    to verify that we are actually at the end of volume.
<li>Receive data from encp, wrapper it, and write it to the volume
<li>Close the data port.
<li>Compute new eod_cookie and tell Volume Clerk that the
    volume is writable. Update remaining bytes as well.
<li>Compute the file location cookie, and tell the bit
    File Clerk about the new file. Get a bit file ID in return.
<li>Give the bit file ID to encp. We are done.
</ol>

If any errors occur while writing the volume, an attempt is made to
characterize
them as either media or drive.  This is discussed more completely in the
section on Error control.  If the user drops the control tcp
channel unilaterally, assume he has aborted.
<p>
<h3><a name="config_server">
1.3.5 Configuration Server
</h3>
The Configuration Server maintains the crucial information about
system configuration, such as the location and parameters of each server.
The first thing that each server does when it starts up is to
ask the Configuration Server for information (e.g. the location
of any other server with which to communicate).  New configurations can be
loaded into the Configuration Server without disturbing the current running
system.
<h3><a name="log_server">
1.3.6 Log Server
</h3>
The Log Server receives messages from other processes and logs them into
formatted log files.
Basically, these messages are transactional records.
Log files are labeled by dates.
At midnight each day, the currently opened log file gets closed and another
one is opened.
<p>
Additionally it is expected that a trace module will be developed.
<h3><a name="media_changer">
1.3.7 Media Changer
</h3>
The Media Changer mounts and dismounts the media into and from the drive
according to a request from the Mover.  One Media Changer can serve multiple
drives and libraries.  When the drives are in the robot, the Media Changer
is the interface to the robotic software.
<h2><a name="db">
2 Databases in Enstore
</h2>
The database used in <em>Enstore</em> must provide the following:
<ul>
<li>
Support journaling of the database to record all changes and support full
     database recovery.
<li>
Support database check-pointing in order to enable full database recovery.
<li>
Support performing daily backups of the database, log, and journal files.
<li>
Support recovery of corrupted databases using the journal or log files.
<li>
Support "python dictionary" interface.
</ul>

<h2><a name="backup_recovery">
2.1 Backup and Recovery Procedures
</h2>
<h3><a name="backup">
2.1.1 Backup
</h3>
The backup procedure is a cron job that is performed routinely. It copies
the database files, log files and journal files to a designated
directory on a remote host. Two environment variables should be set
up for this purpose: ENSTORE_BCKP_HST (default="localhost") and
ENSTORE_DB_BACKUP (default="tmp/backup"). The backup procedure will
perform the following actions:
<p>
<table>
<tr>
<td valign=top>Libtp database
<td valign=top><ul>
<li>defines what log files are involved in active transactions<br>
<li>creates the tar file of database files and all log files <br>
<li>deletes all log files that are not involved in active transactions
</ul>
<tr>
<td valign=top>Volume journal files
<td valign=top><ul>
<li>does journal file checkpointing (hold database access,
move current file to volume.jou.time_stamp, open empty
journal file, release database access)
<li>creates tar file of volume database file and journal files
<li>deletes old journal files
</ul>
<tr>
<td valign=top>File journal file
<td valign=top><ul>
<li>does journal file checkpointing
<li>creates tar file of file database file and journal files
<li>deletes old journal files
</ul>
<tr>
<td valign=top>Archives creation
<td valign=top><ul>
<li>creates new directory on remote host under designated
"root archival" director (name dbase.time_stamp)
<li>moves all the tar files to this area
</ul>
<tr>
<td valign=top>Archives cleanup
<td valign=top><ul><li>deletes all the archival directories created more then
N days ago (default is 10 days)</ul>
</table>
<h3><a name="recovery">
2.2.2 Recovery
</h3>
Recovery should be a job initiated manually in case of database corruption.
There are two ways to recover from a crash:
<p>
<table>
<tr>
<td valign=top>libtp utility
<td valign=top><ul>
<li>copy database tar file in designated directory, untar the file
<li>copy the latest log file from $ENSTORE_DB,
<li>run db_recover utility
<li>delete all related files from $ENSTORE_DB
<li>move the resulting database files and the latest log file to $ENSTORE_DB
</ul>
<tr>
<td valign=top>recovery from the journal files (not implemented yet)
<td valign=top><ul>
<li>copy database (database files and journal files) tar file
<li>in designated directory, untar the file
<li>copy the latest jouurnal file from the $ENSTORE_DB,
<li>run jou_recover utility
<li>delete all related files from $ENSTORE_DB
<li>move the resulting database files to $ENSTORE_DB
</ul>
</table>

<h3><a name="current_db">
2.2 Current Underlying Database Implemented in Enstore
</h3>

The current Enstore implementation uses LIBTP (BSD DB v2.3) as the underlying
database product.
LIBTP is free for non-profit organizations like Fermilab, and has the
following features:

<ul>
<li>
one key dictionary-like database. It is designed to
store/retrieve binary large objects (BLOBs) of arbitrary length, by text key.
<li>
ability to store data items of unlimited size
<li>
support for various data storage structures: hash table, binary tree,
numbered records
<li>
allows duplicate keys (enstore doesn't use them)
<li>
data scanning with cursors, multiple cursors may be opened at the same time
<li>
different levels of cursor stability
<li>
transactions
<li>
transaction logging
<li>
check-pointing
<li>
backup and recovery tools
<li>
custom locks
<li>
deadlock detection
</ul>


A LIBTP-Python shelve-like interface was developed. It provides access to:
<ul>
<li>All three data structures: hash table, binary tree, numbered records
<li>Cursors
<li>Transactions
<li>Locks
</ul>

LIBTP was chosen based on the following considerations:

<ul>
<li>
Nimbleness to allow us to set up test stands while developing and not being encumbered by
     database licensing issues.
<li>
It is similar to dbm-like databases used for the initial Enstore design.
This made it easy to develop a Python interface for it and any necessary
changes to the Enstore code were localized and relatively easy to make.
<li>
Database maintenance is relatively inexpensive. It requires only two
processes to run. One for check-pointing and the other for deadlock detection.
<li>
It is simple and fast enough.
<li>
It provides tools for database transaction logging, database backup and
recovery.
<li>
It is readily obtainable and free.
</ul>

We don't consider the current selection of the LIBTP database to be
necessarily the final one.  For example, although it may not be necessary
for Enstore's internal needs, implementation of maintenance functionality
is not realistically possible without a way to select data using attributes
other than the primary database key. There are two possible ways to solve
this problem. One way is to create additional database index tables. This
is a straightforward extension on top of LIBTP, although probably not the
most elegant solution. Another possible method we are considering is to use
relational databases in Enstore.  For the Enstore project, databases are a
modular component and switching databases means modifying a module only. If
a new database is chosen, we are expecting that the database servers and
clients will have to be redesigned to take advantage of the relational
features.  Python interfaces will also have to be developed to allow the
current framework to keep working.

<h2><a name="admin_tools">
2.3 Administrative Tools
</h2>

Administrative tools will provide the following operations:
<ul>
<li>Display all volumes for a specified media
<li>List all the files and their location on a single or set of media
<li>List file/files on the media by creation date
<li>List all the media that belongs to a specified file family
<li>List files that belongs to a specified file family
<li>Display the date of the last mount for a specified volume
<li>List all media belonging to a file family sorted by the most recent media
mount date
<li>List all media belonging to a file family where the last access date is
before a specified date
<li>Export meta-data of ejected media into a flat file
<li>Import meta-data from a flat file when importing the media from outside the
Enstore system
<li>List all files/volumes that belong to user/group
<li>Mark the volume as readonly if all of the files on the media are older
than a specified date
<li> Delete specified files in the pnfs trashbin.
<li>Find and recycle volumes where all files have been deleted.
<li>Check for files known to the File Clerk but unknown to pnfs
</ul>

<h2><a name="protocol">
3 Communication Protocols
</h2>
The base protocol for Enstore is UDP for "brief" messages and TCP for data
transfers.

UDP message sizes are all less than size of maximum UDP packet size so the
protocol is very simple.

<p>
The base server protocol is the same for all servers.  State-fullness is
minimized, not eliminated.
<p>
Each transmission has a unique ID, timeout and maximum number of retries
associated with it.  The timeout allows for debugging.
For each reception, the "message" is checked against messages received to see
if the reception is a repeat. If the reception is a *repeat request*, send a
saved copy of the response; if the reception is *repeat response*, just
ignored it. This will take care of the case when a timeout/retry happens just
before a response is received.

<h2><a name="read_protocol">
3.1 Read Protocol
</h2>
The communications performed during a read operation are illustrated in the
diagram below and described more fully in the following text.
<p>
NOTE: The communications between the Mover and the Configuration Server happens
approximately every two minutes.  It has been added to the following drawing
to show that this communication is important, but it can occur anywhere in the
communications flow before the Mover contacts the Library Manager.
<p>
<img src=read.gif>
<p>
<a href="read.ps">(also available in Postscript)</a>
<p>
<ul>
<li>
The user (through <em>encp</em>) contacts PNFS asking for a bit file ID (bfid)
for the named file.
<li>
PNFS returns the bfid to <em>encp</em>.
<li>
<em>Encp</em> asks the Configuration Server with which File Clerk should it be
communicating.
<li>
The Configuration Server returns the location of the appropriate File Clerk.
<li>
<em>encp</em> sends the read request to the File Clerk.
<li>
The file Clerk asks the Volume Clerk with which Library manger should it be
communicating.
<li>
The Volume Clerk returns the location of the appropriate Library Manager.
<li>
The File Clerk asks the Library Manager to read the file.
<li>
The Library Manager tells the File Clerk that the request has been placed in
it's queue.
<li>
The File Clerk informs <em>encp</em> that the read request has been queued.
<li>
The Mover asks the Configuration Server with which Library Manager should it
be communicating.
<li>
The Configuration Server returns the location of the appropriate Library
Manager.
<li>
The Mover asks the Library Manager if there is any work for it to do.
<li>
The Library Manager tells the Mover which volume to mount.
<li>
The Mover asks the Media Changer to mount a particular volume.
<li>
The Media Changer responds once the volume is mounted.
<li>
The Mover tells the Library Manager that the volume is mounted.
<li>
The Library Manager tells the Mover which file to read.
<li>
The Mover tells <em>encp</em> from which host and port to read the data.
<li>
The Mover send the data to <em>encp</em>.
<li>
The Mover tells <em>encp</em> when all the data has been transferred and sends
the crc information.
<li>
The read has completed.
<li>
The Mover tells the Library Manager that he still has the volume mounted.
</ul>

<h2><a name="write_protocol">
3.2 Write Protocol
</h2>
The communications performed during a write operation are illustrated in the
diagram below and described more fully in the following text.
<p>
NOTE: The communications between the Mover and the Configuration Server happens
approximately every two minutes.  It has been added to the following drawing
to show that this communication is important, but it can occur anywhere in the
communications flow before the Mover contacts the Library Manager.
<p>
<img src=write.gif>
<p>
<a href="write.ps">(also available in Postscript)</a>
<p>
<ul>
<li>
The user (through <em>encp</em>) contacts PNFS with a request to create a file.
<li>
PNFS returns the file family and volume library information to <em>encp</em>.
<li>
<em>encp</em> asks the Configuration Server with which Library Manager should
it be communicating.
<li>
The Configuration Server returns the location of the appropriate Library
Manager.
<li>
<em>encp</em> sends the write request to the Library Manager, including file
family and number of bytes.
<li>
The Library Manager tells the <em>encp</em> that the request has been placed in
it's queue.
<li>
The Mover asks the Configuration Server with which Library Manager should it
be communicating.
<li>
The Configuration Server returns the location of the appropriate Library
Manager.
<li>
The Mover asks the Library Manager if there is any work for it to do.
<li>
The Library Manager asks the Volume Clerk for a volume for the file with the
specified size and file family.
<li>
The Volume Clerk returns the volume to the Library Manager.
<li>
The Library Manager moves the file internally from one queue to another.
<li>
The Library Manager tells the Mover which volume to mount.
<li>
The Mover asks the Media Changer to mount a particular volume.
<li>
The Media Changer responds once the volume is mounted.
<li>
The Mover tells the Library Manager that the volume is mounted.
<li>
The Mover tells <em>encp</em> to which host and port to write the data.
<li>
The Mover tells the Volume Clerk that he is appending to this volume.
<li>
The Volume Clerk acknowledges this.
<li>
<em>Encp</em> sends the data to the Mover.
<li>
The Mover tells the Volume Clerk that the append operation is done and how much
space is left on the volume.
<li>
The Volume Clerk acknowledges this.
<li>
The Mover tells the File Clerk which file has been created.
<li>
The File Clerk responds with the bit file id.
<li>
The Mover tells <em>encp</em> that the file has been written and sends the bit
file ID and the crc.
<li>
<em>Encp</em> tells PNFS that the file has been created, and the bfid should
be stored.
<li>
The write has completed.
<li>
The Mover tells the Library Manager that he still has the volume mounted.
<li>
The Library Manager tells the Mover that there is no work to be done.
<li>
The Mover tells the Media Changer to dismount the volume.
</ul>

<h2><a name="error">
4 Error Control
</h2>

This section represents our error control development plan -- error control
was only partially implemented in the prototype. <p>

Error control in Enstore is simple, because much of the system state
is stored in the <em>encp</em> client. The <em>encp</em> client can be given a
"retryable error" and will resend data for a write into the system, or the
system can restart a read from the system from scratch. This retry
is given a very high priority when it is received by the Library
Manager -- put on the head of the unassigned work queue.

The Enstore system keeps unassigned read and write requests in a queue of
unallocated work in the Library Manager. Once a read or write requests'
turn comes, the Library Manager puts the request in a "work awaiting
mount" or "work at mover" queue. The reason for this is to track the
volumes for scheduling : the Library Manager must not command a Mover
to mount a volume already in use by another Mover. It is the Mover, and not the
Library Manager which completes requests.

The three Library Manager queues are:
<ul>
<li>Unscheduled work
<li>work awaiting a volume mount
<li>work at a Mover.
</ul>
<p>
It is important to keep these queues consistent. Volume and reading
errors are handled in the Mover, and can be handled by detailed design in
the Mover.
<p>
Many interesting errors are related to when the volume cannot be written
or read, or when it is suspected that volume is jammed, etc.
<p>
We need experience with the actual hardware. In the interim we make the
following assumptions:
<ul>
<li>
If we have trouble during a load or unload operation, we assume that
the volume is physically jammed. Until a human looks at the problem, we
will permit no future operations on the drive or the volume.
<li>
If several drives have fatal errors on writing a volume, we will not write on that
volume anymore.
<li>
If a drive has errors on "consecutive" volumes, we should "stop
using the drive". The number of errors allowed should be picked
up through the Mover configuration, since we may have "more
temperamental" and "less temperamental" drives, we may want to
configure this parameter for each Mover individually.
<li>
If volume reads or spaces give error, we should track the errors. If
the recent history of the volume shows many errors, we should mark the
volume "no access" and bring it to the attention of an administrator.
Therefore, the Volume Clerk needs to track the "recent history" of
errors, and have a per-volume figure of merit which will mark the volume
for no access, given many recent errors.
<li>
Library Manager:
<br>
Two Features should be added to the tickets to support retries.
<ul>
<li>Tickets should be marked as retries or not by <em>encp</em>. Retries go
to the head of the queue.
<li>Tickets should be marked with an "avoid this mover" field for read
retries, so the same drive can be avoided.
</ul>
<em>encp</em> should generate these fields appropriately, and the queuing logic
in the Library Manager ought to implement the logic associated with these
fields.
<p>
</ul>
The Enstore system should check the read/write state of a volume read very
early when the request is queued, to give a prompt error if the access
is not allowed. However, an error may change the state of a volume while
its corresponding work is in the Library Manager queue. Therefore, a
late check is required. The system already performs a "late check"
for writes, since a volume must be selected for every file. The system
needs a "late check" for reads. Failing a late check means contacting
<em>encp</em> to give it the bad news. The contact should be done in the Mover,
not in the Library Manager, just before the Mover goes off to mount the volume
for read, since the Mover is better able to tolerate a very slow or
non-existent <em>encp</em>. It is good to contact
<em>encp</em> before a mount, too, since people will ^C <em>encp</em>'s and
it is best to discover this before incurring the expense of a mount.
<ul>
<li>
"Freeze the volume in the drive" means:
<ul>
<li>Not unloading the volume from the drive
<li>Freezing the volume
<li>Off-lining the Drive
</ul>
<li>
"Freezing the volume" means:
<ul>
<li>mark the volume as "system noaccess"
<li>log that this happened and let an administrator look at the
problem in the morning.
<li><em>encp</em> shall not retry.
</ul>
<li>
"Off-lining the drive" means:
<ul>
<li>Preserving as much state as possible.
<li>Writing a complete description in an error log.
<li>Leaving the problem until business hours unless the capacity of
the system falls below a threshold.
</ul>
</ul>

<table>
<th><font size=+1><b>Volume Write Errors</b></font>
<tr>
<td valign=top><b>WRITE_NOTAPE</b>
<td valign=top>"no such tape" error in library mount failure on write.
    If this happens, the volume data base is inconsistent with reality.
    Mover shall freeze the volume.
    <em>encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_TAPEBUSY</b>
<td valign=top>"The Media Changer says that the volume is in another drive"
    Enstore has a bug, or some other system has mounted the volume or
    library micro put the volume somewhere else.
    Mover shall freeze the volume.
    <em>encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_DRIVEBUSY</b>
<td valign=top>"The Media Changer says that some other volume is in the drive"
    Need to investigate if this could ever be a clean volume otherwise a bug in
    Enstore or a misconfiguration.     Mover shall offline the drive.
    <em>encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_BADMOUNT</b>
<td valign=top>Other mount failure on write, or load operation failed.
    Must assume jammed volume.
    Mover shall freeze the volume in the drive.
    <em>encp</em> shall retry
<tr>
<td valign=top><b>WRITE_BADSPACE</b>
<td valign=top>"EOD cookie does not produce EOD"
    Wrong volume, software bug or drive space error.
    Mover shall freeze the volume in the drive.
    <em>encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_ERROR</b>
<td valign=top>"Error when writing data block or file mark"
    Run of the mill write error.
    If the drive has had many recent errors, Mover shall
    "offline the drive"
    The volume shall be marked read-only.
    <em>encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_EOT</b>
<td valign=top>"Hit EOT while writing data block or file mark"
    Hit EOT. Mover shall mark volume full.
    <em>encp</em> shall retry.
<tr>
<td valign=top><b>WRITE_UNLOAD</b>
<td valign=top>"error when unloading volume from drive"
    Must assume that the volume is physically jammed.
    Mover shall freeze the volume in the drive.
    <em>encp</em> shall retry.
    No <em>encp</em> is associated with the volume at this time.
<tr>
<td valign=top><b>WRITE_UNMOUNT</b>
<td valign=top>"Media Changer gives error when unmounting volume"
    Should not happen. Maybe the volume is hanging in the drive. Does no harm.
    Mover shall freeze the volume in the drive.
    No <em>encp</em> is associated with the volume at this time.
<tr>
<td valign=top><b>WRITE_NOBLANKS</b>
<td valign=top>The Volume Clerk has no blank volumes to give.
    The requests shall return an error.
    (do not "wait forever" -- DAQ should switch to an alternate library)
    An administrator shall be paged.
<tr>
<th><font size=+1><b>Volume Read Errors</b></font>
<tr>
<td valign=top><b>READ_NOTAPE</b>
<td valign=top>"No such volume in library" mount failure on read.
    If this happens, the volume data base is inconsistent with reality.
    Mover shall freeze the volume.
<tr>
<td valign=top><b>READ_TAPE_BUSY</b>
<td valign=top>"The Media Changer says that the volume is in another drive"
    Enstore has a bug, or some other system has mounted the volume or
    the robotic software put the volume somewhere else.
    Mover shall freeze the volume.
<tr>
<td valign=top><b>READ_DRIVEBUSY</b>
<td valign=top>"The Media Changer says that some other volume is in the drive"
    Need to investigate if this could ever be a clean volume otherwise a bug in
    Enstore or a misconfiguration.     Mover shall offline the drive.
    <em>encp</em> shall retry.
<tr>
<td valign=top><b>READ_BADMOUNT</b>
<td valign=top>Other Mount failure on read, or load operation failed.
    We have to assume that the volume is jammed in the drive.
    Mover shall freeze the volume in the drive.
<tr>
<td valign=top><b>READ_BADLOCATE</b>
<td valign=top>Failed space or initial CRC's don't match on initial read.
    We have attempted to space to a file on the volume. Either the file
    location
    cookie is somehow corrupted, we have the wrong volume in the drive or
    the volume drive cannot space properly.
    Mover shall freeze the volume in the drive.
    <em>encp</em> shall not retry
<tr>
<td valign=top><b>READ_ERROR</b>
<td valign=top>Error when reading data block.
    Run of the mill read error.
    Mover shall consider the recent error history of the drive and the volume.
    If the drive has had many recent errors, Mover shall offline the drive.
    <em>encp</em> shall retry.
    If the volume has had many recent errors, Mover shall freeze the volume.
    <em>encp</em> shall not retry.
    If the volume has had few recent errors,
    Mover shall fail the transfer,
    <em>encp</em> shall retry, and annotate that this Mover shall be avoided.
<tr>
<td valign=top><b>READ_COMP_CRC</b>
<td valign=top>Failure of complete CRC.
    This needs investigating, as it should not happen. The drive and the volume
    are suspicious. Corrupt file location cookie, drive space error, wrong
    volume in the drive, etc.
    Mover shall freeze the volume in the drive.
    <em>encp</em> shall not retry.
<tr>
<td valign=top><b>READ_EOT</b>
<td valign=top>Hit EOT when reading.
    Should not happen. Corrupt file location cookie, drive space error, or
    wrong volume in the drive. Should have hit an EOF.
    Mover shall freeze the volume in the drive.
    <em>encp</em> shall not retry.
<tr>
<td valign=top><b>READ_EOD</b>
<td valign=top>Hit EOD when reading.
    Should not happen. Corrupt file location cookie, drive space error, or
    wrong volume in the drive. Should have hit an EOF.
    Mover shall freeze the volume in the drive.
    <em>encp</em> shall not retry.
<tr>
<td valign=top><b>READ_UNLOAD</b>
<td valign=top>Error when unloading volume.
    Must assume that the volume is physically jammed.
    Mover shall freeze the volume in the drive.
    No <em>encp</em> is associated with the volume at this time.
<tr>
<td valign=top><b>READ_UNMOUNT</b>
<td valign=top>Error from Media Changer when unloading volume.
    Should not happen. Maybe the volume is hanging in the drive. Does no harm.
    Mover shall freeze the volume in the drive.
    No <em>encp</em> is associated with the volume at this time
<tr>
<th><font size=+1><b>Other Errors</b></font>
<tr>
<td valign=top><b>ENCP_GONE</b>
<td valign=top><em>encp</em> has gone away while request is queued.
    The Mover contacts <em>encp</em> to ping it before a mount, and if
    <em>encp</em> is not there, the Mover does not go on to contact the Media
    Changer, but instead returns a UNILATERAL_UNBIND to the Library Manager.
<tr>
<td valign=top><b>TCP_HUNG</b>
<td valign=top>DESY reports that
<ul>
<li>sometimes machines can crash....
<li><em>encp</em> processes can be killed in a way that...
<li><em>encp</em> client's disks many fill up ...
<li>etc...
</ul>
which are all seen as the data TCP link is seen to
just hang.
<p>
DESY's treatment is "harsh" -- They compute an anticipated transfer
time for every socket operation and abort the transfer if the
actual transfer takes more than several times the expected value.
Right now, Enstore does not consider this to be an error and just
waits. However, we are vulnerable to disaster. It is possible to see
circumstances where we might tie all the Movers up if, say, one clients
system's disk fills up.
<tr>
<td valign=top><b>LM_CRASH</b>
<td valign=top>If a Library Manager crashes, and loses its queue of pending work,
    the <em>encp</em>'s will never be called back, and will wait
    forever.
    This is a problem. A basis for the solution is for a waiting
    <em>encp</em> to ping its Library Manager,
    every 30 mins to see if its request has gotten lost.
    It would also be good if a --timeout switch was available to
    <em>encp</em>,
    since the users may wish to get an error rather than suffer a long
    delay.
<tr>
<td valign=top><b>MOVER_CRASH</b>
<td valign=top>
Case 1) If a Mover is connected to an <em>encp</em>, then <em>encp</em> will
notice its sockets being torn down prematurely. <em>encp</em> shall retry. This
is not good enough.
<p>
For reads, the volume is tied up because the Library Manager has the volume as
being "at a mover". The retry will be hung in the Library Manager queue
because the Library Manager is unaware of the Mover crash.
<p>
For writes, 1 is effectively deducted from the file family width,
since the Library Manager is unaware of the Mover crash.
<p>
This is all a bug, the minimal effect of a Mover crash should be:
<ul>
<li>
Since the status of the volume is unknown, the status should be akin
to "freeze drive and volume". However, subsequent reads will not produce an
error, but will be queued until the Mover is restarted.
<li>
Allow writes to continue on a different volume of the same file
family, without affecting the "width".
</ul>
<p>
Case 2) Mover has a volume, but is not associated with an <em>encp</em> and
crashes.
Same problems as case 1) but <em>encp</em> will notice.
The total amount of time spent in this possibility is small compared to
the time corresponding to case 1 and case 3.
<p>
Case 3) Mover is idle.
This is O.K. The system degrades as specified.
</table>
</body>
