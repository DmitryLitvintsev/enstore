<body BGCOLOR=#FFFFFF>
<font size=7><b><i>
<center>Enstore Technical Design Document</center>
</i></b></font>
<center>Jon Bakken</center>
<center>Eileen Berman</center>
<center>Chih-Hao Huang</center>
<center>Alexander Moibenko</center>
<center>Don Petravick</center>
<center>Ron Rechenmacher</center>
<center>Kurt Ruthmansdorfer</center>
<p>
<p>
<p>
<font size=+1><b>Table of Contents</b></font>
<ul>
<li><a href="#overview">1 Enstore Architecture</a>
<li><a href="#scope">1.0.1 Scope and Overview</a>
<li><a href="#assum">1.0.2 Assumptions and Constraints</a>
<li><a href="#aintro">1.0.3 Components</a>
<li><a href="#pnfs">1.1 The DESY <em>pnfs</em> Namespace</a>
<li><a href="#ptests">1.1.1 <em>Pnfs</em> Tests</a>
<li><a href="#pserver">1.1.2 <em>Pnfs</em> Server Installation</a>
<li><a href="#pclient">1.1.3 <em>Pnfs</em> Client Installation</a>
<li><a href="#pcmd">1.1.4 pcmd: Enstore related <em>pnfs</em> commands</a>
<li><a href="#volmaps">1.1.5 <em>Pnfs</em> Volume Maps</a>
<li><a href="#encp">1.2 <em>Encp</em></a></li>
<li><a href="#encp_options">1.2.1 <em>Encp</em> command line options</a></li>
<li><a href="#encp_throttle">1.2.2 Throttling in <i>encp</i></a></li>
<li><a href="#encp_list_from hsm">1.2.3 Specifying lists of files when reading from or writing to the HSM</a>
<li><a href="#encp_queue">1.2.4 <em>Encp</em> Transfer Lists</a>
<li><a href="#encp_errors">1.2.5 <em>Encp</em> Return Status</a>
<li><a href="#encp_client">1.2.6 <em>Encp</em> Client Installation</a>
<li><a href="#servers">1.3 Enstore Servers</a>
<li><a href="#servers_cmds">1.3.0.1 General Command Line Control of Servers</a>
<li><a href="#volume_clerk">1.3.1 Volume Clerk</a>
<li><a href="#file_clerk">1.3.2 File Clerk</a>
<li><a href="#library_manager">1.3.3 Library Manager</a>
<li><a href="#user_request">1.3.3.1 Users' Requests</a>
<li><a href="#mover_request">1.3.3.2 Movers' Requests</a>
<li><a href="#lm_commands">1.3.3.3 Library Manager Commands</a>
<li><a href="#mover">1.3.4 Mover</a>
<li><a href="#mover_cmds">1.3.4.1 Command Line Control of the Mover</a>
<li><a href="#mover_config">1.3.4.2 Mover Config File Values</a>
<li><a href="#config_server">1.3.5 Configuration Server</a>
<li><a href="#config_server_cmds">1.3.5.1 Command Line Control of the Configuration Server</a>
<li><a href="#log_server">1.3.6 Log Server</a>
<li><a href="#media_changer">1.3.7 Media Changer</a>
<li><a href="#inquisitor">1.3.8 Inquisitor</a>
<li><a href="#inquisitor_cmds">1.3.8.1 Command Line Control of the Inquisitor</a>
<li><a href="#inquisitor_config">1.3.8.2 Inquisitor Config File Values</a>
<li><a href="#inquisitor_outputs">1.3.8.3 Example Inquisitor Reports</a>
<li><a href="#inquisitor_ascii_status">1.3.8.3.1 Example Ascii Status File</a>
<li><a href="#inquisitor_html_status">1.3.8.3.2 Example Html Status Snapshot File</a>
<li><a href="#inquisitor_encp_status">1.3.8.3.3 Example Encp History Snapshot File</a>
<li><a href="#inquisitor_plot_ita">1.3.8.3.4 Example Individual Transfer Activity Plot</a>
<li><a href="#inquisitor_plot_btd">1.3.8.3.5 Example Bytes Transferred/Day Plot</a>
<li><a href="#alarm_server">1.3.9 Alarm Server</a>
<li><a href="#server_protocols">1.4 Server Protocols</a>
<li><a href="#trace">1.5 Trace</a>
<li><a href="#db">2 Databases in Enstore</a>
<li><a href="#current_db">2.1 Current Underlying Database Implemented in Enstore</a>
<li><a href="#backup_recovery">2.2 Backup and Recovery Procedures</a>
<li><a href="#backup">2.2.1 Backup </a>
<li><a href="#recovery">2.2.2 Recovery</a>
<li><a href="#admin_tools">2.3 Administrative Tools</a>
<li><a href="#protocol">3 Communication Protocols</a>
<li><a href="#read_protocol">3.1 Read Protocol</a>
<li><a href="#write_protocol">3.2 Write Protocol</a>
<li><a href="#error">4 Error Control</a>
<li><a href="#eass">4.1 Assumptions about Errors</a>
<li><a href="#eover">4.2 Error Overview</a>
<li><a href="#edet">4.3 Detailed Error Discussion</a>
<li><a href="#impexp">5 Volume Import and Export</a>
<li><a href="#volexp">5.1 Volume Export</a>
<li><a href="#volimp">5.2 Volume Import</a>
<li><a href="#test_system">6 Test System</a>
<li><a href="#ts_results">6.1 Test System Results</a>
<li><a href="#interface">7 Interfaces and Integration</a>
<li><a href="#d0req">8 D0 Requirements</a>
<li><a href="#d0reqa">8.1 Summary of D0 Functional Specifications</a>
<li><a href="#d0reqb">8.2 Sam/Enstore Interface Notes</a>

</ul>

<p>
<hr>
<h2><a name="overview">
1 Enstore Architecture
</h2>
Enstore provides a generic interface for experimenters to
efficiently use mass storage systems as easily as if they were native file
systems.
<p>
<center><img src="enstore_at_fnal2.gif"></center>
<p>
<div align=right><a href="enstore_at_fnal2.ps">(also available in Postscript)</a><div align=left>
<p>

<h3><a name="scope">
1.0.1 Scope and Overview
</h3>


Enstore provides distributed access to and management of petabytes of data
stored on tape. The data can be made of up to billions of files of varying
sizes - typically of between 1 and 2 Gigabytes in size. At any time, many of
the tapes are accessible through automated tape libraries. The system supports
migration of tapes to and from shelves - export and import - where operator
intervention is required to move the tapes between the shelves and roboticly
accessible tape drives. The system treats robot's shelves as a scarce
commodity.
Enstore is a system to provide mass storage for large Run II data sets.  As
such it is not a general purpose mass storage system, but optimized to
allow access to large datasets made of many files.  The system supports
random access of files, but also streaming, the sequential access of
successive files on tape. 
<p>

The Enstore system provides for access to the data by user/client applications
which are distributed across an IP network. It supports tape
drives attached locally to the users' computer, as well as those
remotely accessible over the network.
<p>

The Enstore system provides resource management of the available tape drives
such that, for example, logging of data from the data acquisition systems can
be given guaranteed access to the tape bandwidth whatever other user accesses
are being requested.
Enstore is designed to be used by Fermilab experiments data acquisition, data
processing and analysis systems. Well defined interfaces will be provided to
these data handling systems to allow them to easily use the services provided.
The writing and reading of tapes
must therefore be reliable and efficient, and the system must be robust
enough to support this critical application without compromising data
taking.  Enstore's goal is to provide a system that can be extended as
needed for the experiments actual data taking needs, as well as be easily
maintainable for the duration of several data taking runs.
<p>

Enstore is based on a client-server model that allows hot swapping of
hardware components and dynamic software configuration, is platform
independent, runs on heterogeneous environments and is easily extendable.
Most of the operations are transparent to the user.  System performance is
monitored and fine tunable.  A great deal of care has been taken to ensure
that it is able to prevent or to recover from a worst case scenario.  The
system has layers around it to customize and address problems as they
occur.  When possible, these layers are expected to use already existing
components (e.g. FTT, <em>pnfs</em>).
<p>

The Enstore system is designed to provide for the needed Run II data access
throughput requirements within the budget assigned.
The system software is layered and accessible to
the Run II developers such that needed modifications can be made in a timely
manner to meet the needs of commissioning and running of the Run II detectors.
<p>

Enstore is designed to support "lights out" operation of the Run II
automated tape library systems. To this end, the design is targeted
towards requiring operator intervention at no more than 8 hour
intervals - for example, to import/export requests are queued and need only be
handled only within the daytime operator shifts. Careful attention
is paid to error reporting, handling and recovery in order to
require the minimal possible load on the operations and support staff.
<p>


To summarize, Enstore provides the following features:

<menu>

  <li>Support for several types of serial media accessed through Automated
       Tape Libraries or locally mounted on the client or host computers.

  <li>Support for distributed access to data on these tapes.

  <li>Reliable, efficient and prioritized write access from the experiment
       data acquisition systems for the logging of raw data.

  <li>Optimized access to large (petabyte) datasets made up of many (100s of
       millions of) files of 1-5 GB in size.

  <li>Efficient and flexible support for "write streaming" of data to tape,
       where data is physically clustered on tapes according to a simple
       classification scheme - typically the trigger number associated with
       the event data written.

  <li>Management of hardware and software resources, e.g., a limited number
       of available tape drives to allow prioritized access to the data.

  <li>Swapping of hardware components - tapes, tape drives, and computers -
       without bringing down the complete system.

  <li>Complete error reporting and configurable response to error
       conditions.

  <li>The use of already tested components as far as possible.

  <li>Easy mechanisms for testing of the system.

  <li>Import and export of tapes between the Automated Tape Library(ies)
       and shelf storage without bringing down the complete system.

  <li>Support for distributed clients to access data through standard
       network protocols, and to a great extent transparently.

  <li>Sequential access of data in files, for event reconstruction and
       other large data processing requirements.

  <li>Random access to files on tapes, to support general event analysis.

  <li>Functionality in layers such that the primitive tape read/write
       software can be used in situations where the full functionality of
       resource management, name translation and access optimization is
       not needed.

</menu>


<h3><a name="assum">
1.0.2 Assumptions and Constraints
</h3>

A constraint is a factor that limits our implementations.
Assumptions are factors that, for planning purposes, will be considered to be
true, real, or certain.

<menu>

<LI> A usable system must be available in time for vertical slice tests
     in June, 1999.

<LI> The system must be apropos for Run II use, and have a place in the
     overall storage strategy for the laboratory.

<LI> Binaries should be distributed to the users. We should not expect users
     to have to deploy a large infrastructure on their computers to use Enstore.

<LI> The DESY name space is usable for Enstore, available to FNAL, and usable
     for Run II sized applications.

<LI> Fermilab FTT, a platform-independent SCSI tape package is suitable for
     the selected hardware, and useful over Run II projects.

<li> OCS will be available and usable for operator mounts.

<li> OCS will be available and usable as the repository of the tape and
     drive statistics.  Enstore, itself will not store long term statistics.
    

<LI> Tape drives may be attached to user's computers via IP, and an adequate
     IP based network is constructed for Run II.

<LI> A non-transparent interface FMSS-like interface is
     appropriate for the users.

<li> Python is an appropriate choice for implementing the project.  Loadable C
     modules will be used where appropriate, for example, to increase
     performance.

<li> Enstore does not yet support a disk cache or buffer in front of the media.
     However, one is conceivable and planned for a future release.
<p>
</menu>

<h3><a name="aintro">
1.0.3 Components
</h3>


Enstore uses a client-server architecture to provide a generic interface for users
to efficiently use mass storage systems.
Enstore supports multiple distributed media robots,
each of which may handle multiple media types or individual directly attached
drives, and multiple distributed mover nodes.
The system architecture does not dictate an exact hardware architecture.
Rather, it specifies a set of general and generic networked hardware
and software components.
These components are loosely coupled in the sense that each one can be
replaced easily without affecting the rest of the system and
each class of components can be easily expanded to accommodate
the increased demand in performance or capacity.
<p>
The system is written in <em>python</em>, a scripting
language that has advanced object-oriented features.
Python provides a sound environment for quick turn-around
and a seamless integration/migration path to
fully compiled languages, such as C and C++, if there is a demand
for even better performance.
<p>
Enstore has four major kinds of software components:
<ul>
<li><em>namespace</em>, implemented by the <em>pnfs</em> package from
        <em>DESY</em>
<li><em>encp</em>, a program used to copy files to and from media libraries
<li><em>servers</em>
        <ul>
        <li>Configuration Server
        <li>Volume Clerk
        <li>File Clerk
        <li>Multiple, distributed Library Managers
        <li>Multiple, distributed Movers
        <li>Media Changer
        <li>Log Server
        <li>Inquisitor
        <li>Alarm Server
        </ul>
<li><em>administration tools</em>
</ul>
These software components, as well as hardware components,
are shown schematically in the following system context diagram.
Hardware components are connected via IP.
Great care has been taken to ensure that the system will function well
under extreme load conditions.
By design, there is no preset limit on the number of concurrent user
computers nor on the number of physical media libraries or drives.
The system is only limited by the availability of physical resources.
We control all of the source code for the system except for that of
<em>pnfs</em> (which is a well supported product from DESY).
<p>
<center><img src="enstoreSimple.gif"></center>
<p>
<div align=right><a href="enstoreSimple.ps">(also available in Postscript)</a><div align=left>
<p>
Like <em>tcp</em>, the system is architected with distributed and
peer-to-peer reliability.
Each request originating from the <em>encp</em> program is branded
with a unique ID.
<em>Encp</em> retries under well-defined circumstances, issuing
an equivalent request with a new unique ID.
The system can instruct <em>encp</em> to retry if it needs to back out
of an operation.

<hr>
<h2><a name="pnfs">
1.1 The DESY <em>pnfs</em> Namespace
</h2>

<em>Pnfs</em> is a DESY written and supported package.  Detailed
information about <em>pnfs</em> can be found on the DESY
<a href=http://mufasa.desy.de/pnfs/Welcome.html>http://mufasa.desy.de/pnfs/Welcome.html</a>
and
<a href=http://watphrakeo.desy.de/pnfs/>http://watphrakeo.desy.de/pnfs/</a>
web pages. The <em>pnfs</em> servers can not be installed without the permission of DESY.
<p>
The DESY <em>pnfs</em> package  implements an <em>nfs-v2</em> daemon and mount daemon.
These daemons do not actually serve a file system, but, instead make a
collection of database entries looks like a file system, and provide
control information for the system. Each file that is created in <em>pnfs</em> has 8
layers that Enstore uses to store metadata information about the file transfers.
Normal UNIX permissions and administered export points
are used to prevent unauthorized access to the name space.
<p>
To inspect files, users mount their portion of the <em>pnfs</em> file
system on their own computers,
and interact with it using the native operating system utilities.
For example, users can <em>ls</em>, <em>stat</em>,
<em>mv</em>, <em>rm</em> or <em>touch</em> existing "files",
but are given errors on attempts to read
or write the content of the files.
Users can also <em>mkdir</em> and <em>rmdir</em>, and <em>ln</em> files. Hard links should be used to ensure all
the metadata information is linked; symbolic links will not give the user what he naively expects.
<p>
There are also some special <em>pnfs</em> files which act as normal UNIX
files. Administrators can write data to these files and the users can read
from them.  These files are the exception rather than the rule.  Enstore
plans on using them to distribute service information that everyone, who
has <em>pnfs</em> mounted, can read.  <p>
Enstore uses <em>pnfs</em> for three different kinds of access and information:
<ol>
<li>Administration Interactions
    <br>
    An administrator can create special files, called wormholes, in the <em>pnfs</em> name space.
    For example, one special file signifies that the system needs to be
    drained (maybe due to an impending shutdown). Existence of this file
    causes <em>encp</em> to stall,
    preventing users from submitting additional jobs and thus draining the Enstore system of transfers.
    The name of the Enstore-draining wormhole file is "local-pnfs-mountpoint/.(config)(flags)/disabled".
    Additional wormholes can be created as needed.
    <p>
<li>Configuration Information
    <br>
    Some creation details
    need to be provided before the user can write files to media. Enstore uses <em>pnfs</em> tag files
    (usually just called tags)for these purposes.
    Tags are
    associated with a directory and not any specific file.
    Examples of configuration information that is specified with tags include the file family name, file
    family width, Library Manager.
     <p>
<li>User File information
    <br>
    The rest of the system identifies a file by a 64-bit numeric
    identifier, dubbed a "bit file ID".
    After a file is written, the File Clerk generates a bfid and
    <em>encp</em> stores this information
    in one of the <em>pnfs</em> file layers.
    <em>Encp</em> then reads this bit file ID and gives it to the Enstore servers
    when fetching data.
    Other <em>encp</em> file transfer details, such as time of last access or location of
    where the file was copied to or transfer rates, are stored in a different
    metadata layers of the same <em>pnfs</em> file.
</ol>

<h3><a name="ptests">
1.1.2 <em>Pnfs</em> Tests
</h3>
<em>Pnfs</em> was tested in the prototype with very good results. The code was run
on a 200 MHz Pentium Pro Linux machine with SCSI disks.  The <em>pnfs</em> code had
not been run extensively on Linux machines before and there were a few minor
glitches found during initial running. Support from DESY was outstanding
and all problems were solved quickly.

As a test of <em>pnfs'</em> capabilities, the February 1998 set of 250K HPSS filenames
were put into <em>pnfs</em>. Approximately 20 <em>pnfs</em> databases were used for this
test, with each database corresponding to existing HPSS "experimental"
separation. [That is, D0 had its own database, SDSS had its own, and so
forth.] Name lookup was done on each database simultaneously from 3
machines (IRIX, Linux, and AIX) for several days.  <em>Pnfs</em> performed
flawlessly and was able to provide names at a rate of 3-15 names/S. The
database can be further optimized, but this performance is already adequate
for Enstore.
<p>
*** These tests will have to be repeated *** under the final hardware configuration,
but there is no indication of any problems.

<h3><a name="pserver">
1.1.2 <em>Pnfs</em> Server Installation
</h3>

<em>Pnfs</em> Server installation should be done by experienced administrator. It
 is not something that a user should ever have to do.
Further, permission to install <em>pnfs</em> must be granted by the MSS
group at DESY. These requests should be made to
<a href="mailto:patrick@watphrakeo.desy.de">Patrick Furhmann</a> or
<a href="mailto:gasthuber@desy.de">Martin Gasthuber</a> at DESY.

DESY has granted Enstore permission to fully use, but not change, its
<em>pnfs</em> package at Fermilab.  DESY has also granted Enstore the right
to distribute a binary + necessary scripts version of <em>pnfs</em>
(<it>i.e.</it>, no source code) via the normal Fermilab UPD product
distribution mechanism. For example, the current (Jan 99) version in UPD is
<pre>$ upd list pnfs

DATABASE=/ftp/upsdb
        Product=pnfs    Version=v3_1_3a-f4      Flavor=Linux+2
                Qualifiers=""   Chain=current

</pre>

The UPD version can be decoded as follows: "v3_1_3a" is the DESY version of
<em>pnfs</em>, and the "-f4" signifies the 4th Fermi "release". None of the
DESY code is modified - Fermilab only adds its UPS packaging framework and
some local installation instructions. All fixes, changes, or updates to
<em>pnfs</em>, will always come from DESY.

It is expected that there will be only a few <em>pnfs</em> servers at
Fermilab. To date, <em>pnfs</em> servers have been installed on 3 Linux
nodes without any difficulty. Each time, a set of installation instructions
has been improved; however the <em>pnfs</em> server installation is still
not completely automatic.  The Fermilab installation instructions are
distributed along with the UPD product.

On the node that is serving <em>pnfs</em>, <em>pnfs</em> takes over the normal
function of exporting nfs.  Otherwise the machine is general
purpose.  To be explicit, the only 2 processes <em>pnfs</em> server machine can not
run are rpc.mountd and rpc.nfsd. It runs the <em>pnfs</em> versions of these
instead. These processes are only concerned with exporting <em>pnfs</em>.
For example, Rip6 is the current (Jan 99) Enstore <em>pnfs</em>server. Here is its /etc/fstab
<pre>
rip6$ cat /etc/fstab
/dev/sda6               /                       ext2    defaults        1 1
/dev/sda5               swap                    swap    defaults        0 0
/dev/sdc1               /rip6a                  ext2    defaults,grpid  2 1
/dev/fd0                /mnt/floppy             ext2    noauto          0 0
none                    /proc                   proc    defaults        0 0
rip8:/fnal              /fnal                   nfs     soft,rsize=8192,wsize=8192      0 0
rip8:/home              /home                   nfs     soft,rsize=8192,wsize=8192      0 0
rip8:/usr/local         /usr/local              nfs     soft,rsize=8192,wsize=8192      0 0
localhost:/fs           /pnfs/fs                nfs     noauto,intr,bg,hard,rw,noac       0 0
rip6:/grau-ait          /pnfs/grau/ait          nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/grau-dlt          /pnfs/grau/dlt          nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/grau-mammoth      /pnfs/grau/mammoth      nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/stk-red20         /pnfs/stk/red20         nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/stk-red50         /pnfs/stk/red50         nfs     noauto,user,intr,bg,hard,rw,noac 0 0
rip6:/rip6disk1         /pnfs/rip6              nfs     noauto,user,intr,bg,hard,rw,noac 0 0
</pre>
<p>
As you can see, rip6 is nfs mounting 3 disks from rip8 and mounting the
<em>pnfs</em> disks it is exporting as well as the local disks.
There are also numerous Enstore processes running on rip6, for example:
<pre>
USER       PID %CPU %MEM  SIZE   RSS TTY STAT START   TIME COMMAND
bakken    3280  0.0  7.3 20240  9448  ?  S    00:17   0:14 python /home/bakken/enstore/src/configuration_server.py
bakken    3334  0.0  4.9 20112  6356  ?  S    00:17   0:01 python /home/bakken/enstore/src/log_server.py
bakken    3366  0.0  5.7 37980  7352  ?  S    00:17   0:02 python /home/bakken/enstore/src/volume_clerk.py
bakken    3398  0.0  5.3 37760  6832  ?  S    00:17   0:01 python /home/bakken/enstore/src/file_clerk.py
bakken    3433  0.0  5.1 36472  6560  ?  S    00:17   0:00 python /home/bakken/enstore/src/media_changer.py
bakken    3465  0.0  8.8 33456 11300  ?  S    00:17   0:01 python /home/bakken/enstore/src/mover.new.py
bakken    3515  0.0  0.3  1140   492  ?  S    00:18   0:00 db_checkpoint -h
bakken    3520  0.0  0.3  1612   420  ?  S    00:18   0:00 db_deadlock -h
bakken    3523  0.0  5.3 30508  6808  ?  S    00:18   0:01 python /home/bakken/enstore/src/admin_clerk.py
bakken    3673  0.0  8.1 36760 10432  ?  S    00:20   0:34 python /home/bakken/enstore/src/inquisitor.py
bakken   12178  0.0  0.8  1552  1048  p2 S    19:38   0:00 /bin/login -h willow fnal.gov -p bakken
</pre>
<p>
The main point, often confused, is that the <em>pnfs</em> server node
remains a general purpose and usable machine.
<p>
Permission to mount the <em>pnfs</em> namespace is granted using a
mechanism similar to the normal Unix nfs export permission scheme.  There
are DESY commands (the pmount command) that make this entire process very
simple.
<p>
<em>Pnfs</em> can be started automatically on a boot-up. This allows other nodes to
easily mount the namespaces after reboots.
<p>
Finally, it should be noted that a Run II <em>pnfs</em> server will need 
a SCSI RAID level 5 disk system for its databases.
<p>
*** Live Backups of database and recovery procedures *** - to be discussed during March trip to
DESY. This has not been a priority yet.

<h3><a name="pclient">
1.1.3 <em>Pnfs</em> Client Installation
</h3>

No <em>pnfs</em> client software is needed -- the <em>pnfs</em> file system
(really namespace) just has to be mounted!  Enstore has been written such
that it recognizes all <em>pnfs</em> namespace if it has a local mounting
point beginning with /pnfs/... . Enstore uses /pnfs as convenient key.

Typical steps for mounting a new <em>pnfs</em> namespace (in this example
the <em>pnfs</em> namespace is called "grau-ait" and it is served from the
rip6 node) are:

<ol>
  <li> As root, mkdir -p /pnfs/grau/ait
  <li> As root, append to /etc/fstab:
       <pre>rip6:/grau-ait /pnfs/grau/ait nfs user,intr,bg,hard,rw,noac 0 0</pre>
       The "intr,bg,hard,rw,noac" mount options should not be changed as they are needed for proper operation.
  <li> mount /pnfs/grau/ait This can be done as a normal user if the "user" mount option
       is specified in the /etc/fstab file.
</ol>
Of course, the actual steps depend on the <em>pnfs</em> installation.
<p>
*** Automounting will be investigated in early Feb 99 *** It is expected to work without
problems. This is deemed especially important since it will streamline the production
farm nodes disk mount administration.
<p>

Finally, it should be noted that mounting <em>the </em>pnfs namespace does
not restrict the node in any other way - it can import and mount any other
file systems and run any tasks as it normally would.

<h3><a name="pcmd">
1.1.4 pcmd: Enstore related <em>pnfs</em> commands
</h3>

All non-I/O Unix commands that operate on normal file systems can also
(typically) be used on the <em>pnfs</em> namespace. DESY has provided
several examples on their web pages and tools in their <em>pnfs</em>
package that allow users to view and control the special features of
<em>pnfs</em>.  Enstore has tailored these tools into a single script,
called pcmd, that allows users to control, manipulate and query <em>pnfs</em> files
that Enstore creates. The pcmd tool is distributed along with the <em>encp</em>
client. It is almost entirely written in shell, and therefore, it is
stand alone and doesn't require any other products, including python.
<p>
Commonly used commands are:
<P>
<center><TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>FUNCTION</B></TD>
<TD NOSAVE><B>COMMAND</B></TD>
<TD NOSAVE><B>OUTPUT</B></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the online help</TD>
<TD NOSAVE>pcmd help</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists "important" info about the file<br>*** Needs work to be fully functional ***</TD>
<TD NOSAVE>pcmd info file</TD>
<TD NOSAVE><pre>
$ pcmd info M1
bfid="91184924000000L";
volume="flop309";
location_cookie="68608";
size="1252";
file_family="jon4";
filename="/pnfs/enstore/airedale/jon4/M1";
orig_name="/pnfs/enstore/airedale/jon4/M1";
map_file="/pnfs/enstore/volmap/jon4/flop309/000000068608";
pnfsid_file="00020000000000000050AE88";
pnfsid_map="00020000000000000050AEA0"</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the tags in the directory</TD>
<TD NOSAVE>pcmd tags directory</TD>
<TD NOSAVE><pre>
$ pcmd tags .
.(tag)(library)  =  ait
.(tag)(file_family)  =  jon-ait-3
.(tag)(file_family_width)  =  2</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>sets/lists library tag to value <br>(must have correct cwd)</TD>
<TD NOSAVE>pcmd library [value]</TD>
<TD NOSAVE><pre>
$ pcmd library
ait
$ pcmd library xxx
$ pcmd library
xxx</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>sets/lists library tag to value <br>(must have correct cwd)</TD>
<TD NOSAVE>pcmd file_family [value]</TD>
<TD NOSAVE><pre>
$ pcmd file_family
jon-ait-3
$ pcmd file_family xxx
$ pcmd file_family
xxx
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>sets/lists library tag to value <br>(must have correct cwd)</TD>
<TD NOSAVE>pcmd file_family_width [value]</TD>
<TD NOSAVE><pre>
$ pcmd file_family_width    
2
$ pcmd file_family_width 10
$ pcmd file_family_width
10
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists all the files on specified tape in volmap <br>*** Needs work to be fully functional ***</TD>
<TD NOSAVE>pcmd files volmap-tape</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the volmap-tape for the specifed volumename <br>*** Needs work to be fully functional ***</TD>
<TD NOSAVE>pcmd volume volumename</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the bit file id of the file</TD>
<TD NOSAVE>pcmd bfid   file</TD>
<TD NOSAVE><pre>
$ pcmd bfid testfile
91551931700000L
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the last parked location of the file <br>parked feature is not implemented</TD>
<TD NOSAVE>pcmd parked file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the debug info about the file transfer</TD>
<TD NOSAVE>pcmd debug  file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the cross-reference info about the file</TD>
<TD NOSAVE>pcmd xref   file</TD>
<TD NOSAVE><pre>
$ pcmd xref testfile
CA2902 (tape label)
'0000_000000000_0000132' (positioning info)
104857600 (file size)
jon-ait-1 (file family)
/pnfs/grau/ait/jon1/100MB.trand__Jan05005204rip8.fnal.gov16199 (original name)
/pnfs/grau/ait/volmap/jon-ait-1/CA2902/0000_000000000_0000132 (volume map name)
0001000000000000000928D0 (pnfs id of file)
0001000000000000000928E0 (pnfs id of volume map file)
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>does an ls on the named layer in the file</TD>
<TD NOSAVE>pcmd ls   file [layer]</TD>
<TD NOSAVE><pre>
$ pcmd ls testfile 3
4 -rw-rw-r--   1 bakken   g023         3692 Jan  5 00:55 ./.(use)(3)(testfile)
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the layer of the file<br>it is easier to use pcmd bfid|parked|debug|xref commands</TD>
<TD NOSAVE>pcmd {cat|more|less} file layer</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists the tag in the directory<br>it is easier to use pcmd library|file_family|file_family_width commands</TD>
<TD NOSAVE>pcmd {tagcat|tagmore|tagless} tag  directory</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists whether Enstore is still accepting transfers</TD>
<TD NOSAVE>pcmd enstore_state  </TD>
<TD NOSAVE><pre>
$ pcmd enstore_state
Enstore enabled
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>lists whether <em>pnfs</em> mount point is up <BR>*** Not fully functional yet ***</TD>
<TD NOSAVE>pcmd pnfs_state mount-point</TD>
<TD NOSAVE><pre>
$ pcmd pnfs_state /pnfs/grau/ait
Pnfs up
</pre></TD>
</TR>

</TABLE></center>
<P>

Don't use these unless you know what you are doing:
<P>
<center><TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>FUNCTION</B></TD>
<TD NOSAVE><B>COMMAND</B></TD>
<TD NOSAVE><B>OUTPUT</B></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>echos text to named layer of the file</TD>
<TD NOSAVE>pcmd echo text file layer</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>deletes (clears) named layer of the file</TD>
<TD NOSAVE>pcmd rm file layer </TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>copies Unix file to named layer of file</TD>
<TD NOSAVE>pcmd cp unixfile file layer</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>copies Unix file to named layer of file</TD>
<TD NOSAVE>pcmd cp unixfile file layer</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>sets the size of the file</TD>
<TD NOSAVE>pcmd size file size</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>echos text to the named tag</TD>
<TD NOSAVE>pcmd tagecho text tagname</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>removes the tag (tricky, see DESY documents)</TD>
<TD NOSAVE>pcmd tagrm tag </TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>sets io mode (can't clear it easily)</TD>
<TD NOSAVE>pcmd io file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

</TABLE></center>
<P>


Don't use these unless you can interpret the results:
<P>
<center><TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>FUNCTION</B></TD>
<TD NOSAVE><B>COMMAND</B></TD>
<TD NOSAVE><B>OUTPUT</B></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the pnfs id</TD>
<TD NOSAVE>pcmd id file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the showid information</TD>
<TD NOSAVE>pcmd showid id</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the const information</TD>
<TD NOSAVE>pcmd const file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the filename</TD>
<TD NOSAVE>pcmd nameof id </TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the complete file path</TD>
<TD NOSAVE>pcmd path id</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the parent</TD>
<TD NOSAVE>pcmd parent id</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the counters</TD>
<TD NOSAVE>pcmd counters  file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows of the counters</TD>
<TD NOSAVE>pcmd counterN dbnum <br>(must have cwd in <em>pnfs</em></TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the cursor</TD>
<TD NOSAVE>pcmd cursor file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the directory position</TD>
<TD NOSAVE>pcmd position file</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the database information</TD>
<TD NOSAVE>pcmd database file </TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>shows the database information</TD>
<TD NOSAVE>pcmd databaseN dbnum <br>(must have cwd in <em>pnfs</em>)</TD>
<TD NOSAVE><pre>
</pre></TD>
</TR>

</TABLE></center>
<P>

<h3><a name="volmaps">
1.1.5 <em>Pnfs</em> Volume Maps
</h3>

In order to request files from a tape in an optimal transversal, it is
necessary for the user to know the order of the files on a tape.  It is expected that an
experiment's catalog will keep this information.  As an additional aid for the user,
<em>encp</em> creates a duplicate file entry in <em>pnfs</em> (in addition to storing
the filename and its metadata).  This duplicate entry is organized by file
families and volumes instead of by the user's directory structure.  The naming
convention for the duplicate entries is such that the file order on the tape
could be readily known.  For example, for sequential files on tape, the name
is just the file number; for files on a disk, it is just the byte
offset. When a user wanted to know what was on a particular tape, he would
just look in the duplicate tree to get the files - then he would read the
necessary information about the linked file from one of the <em>pnfs</em> layers (such
as the bfid, original file name...) to make his transfer request to the mass
storage.

Let's say a user transferred a file to mass storage and called it
/pnfs/enstore/airedale/one.  The file was stored on disk floppy named
"flop301"  with a byte offset=0 and in a file family named "jon".

<pre>
$ ls -alsFt /pnfs/enstore/airedale/one
   0 -rw-rw-r--   1 bakken   root           26 Jul 21 12:09 /pnfs/enstore/airedale/one
</pre>

The duplicate entry is /pnfs/enstore/volmap/jon/flop301/000000000000
In the general case, "jon" would be replaced by the file family name,
"flop301" would become the volume name and "000000000000" would be the file
order on tape.

<pre>
$ ls -alsFt pnfs/enstore/volmap/jon/flop301/000000000000
   0 -rw-r--r--   1 root     root           26 Jul 21 12:09 /pnfs/enstore/volmap/jon/flop301/000000000000
</pre>

The original file and the duplicate file contain cross reference information
to allow the user to get to the other one and enough information to setup a
transfer. An example of this was shown in the previous section in "pcmd info file"

Consider a few cases.
<ul>
<li> If a user wants dozens of files, she can lookup up the volume name where
each file is stored and group her requests so files on one tape are grouped
together.
<li>If a user wants a dozen files from a certain file family but doesn't care
which ones, she can look into the volmap area and pick out files on a specific
volume and set up the request that way.
</ul>

Finally, since the user will not have delete authority to the duplicate
volmap file, this mechanism gives us an automatic trash can facility.  If the
user deletes his entry in <em>pnfs</em>, we still have the duplicate entry and can
"easily" restore the original.

<hr>
<h2><a name="encp">
1.2 <em>Encp</em>
</h2>
Reading and writing files means interacting with media.
Most users will just want to get their data file and do their work. They do
not want want all the "baggage" products (python, ftt libraries, libtp...)
that is required to run a complete Enstore system.  To this end, we
distribute a separate product, called <em>encp</em> along with
Enstore. This product basically consists of 1 standalone binary executable,
<em>encp</em> and one UPS table file. This executable, and the mounting of
<em>pnfs</em> is the only thing that clients need to access their data on
the robot.

<em>Encp</em> is very similar to the <em>cp</em> command in UNIX, and we
have tried to duplicate its behavior wherever possible.
The syntax is:
<pre>
% encp [options] src_file dst_file
</pre>
Currently there is no wildcarding allowed, but this is a straight forward
extension to <em>encp</em>.

<h3>
<a NAME="encp_options"></a>1.2.1<i>Encp</i> command line options</h3>

<center><table BORDER COLS=3 WIDTH="100%" NOSAVE >
<tr VALIGN=CENTER NOSAVE>
<td NOSAVE><b>OPTION</b></td>
<td NOSAVE><b>SWITCH</b></td>
<td NOSAVE><b>DEFAULTS</b></td>
</tr>

<tr NOSAVE>
<td NOSAVE>print short help message abouting using <em>encp</em></td>
<td NOSAVE>--help</td>
<td NOSAVE>None</td>
</tr>

<tr NOSAVE>
<td NOSAVE>perform CRC check on the local user machine</td>
<td NOSAVE>--crc</td>
<td NOSAVE>CRC check is only performed on the mover computers</td>
</tr>

<tr NOSAVE>
<td NOSAVE>set the base priority = value</td>
<td NOSAVE>--pri=value</td>
<td NOSAVE>1</td>
</tr>

<tr NOSAVE>
<td NOSAVE>change the base priority by value after a period specified by the agetime switch</td>
<td NOSAVE>--delpri=value</td>
<td NOSAVE>0</td>
</tr>

<tr NOSAVE>
<td NOSAVE>specify the time period, in minutes, after which the base priority could change</td>
<td NOSAVE>--agetime=value</td>
<td NOSAVE>0 (no aging of priority)</td>
</tr>

<tr NOSAVE>
<td NOSAVE>turn on special status printing requested by D0</td>
<td NOSAVE>--d0sam</td>
<td NOSAVE>D0 printing is off</td>
</tr>

<tr NOSAVE>
<td NOSAVE>change the amount of information printed about the transfer</td>
<td NOSAVE>--verbose=value</td>
<td NOSAVE>0 (no printing)</td>
</tr>

<tr NOSAVE>
<td NOSAVE>list the active and pending transfers for the specified node</td>
<td NOSAVE>encp --queue nodename</td>
<td NOSAVE>None</td>

<tr NOSAVE>
<td NOSAVE>specifies the hostname where the configuration server is running</td>
<td NOSAVE>--config_host=value</td>
<td NOSAVE>environmental variable, ENSTORE_CONFIG_HOST, set by the UPS setup command</td>
</tr>

<tr NOSAVE>
<td NOSAVE>specifies the port number that the configuration server responds to</td>
<td NOSAVE>--config_port=value</td>
<td NOSAVE>environmental variable, ENSTORE_CONFIG_PORT, set by the UPS setup command</td>
</tr>

</table></center>
The d0sam option provides the output of <em>encp</em> in format requred by d0 SAM
system. The example output is below:
<pre>
[enter command] ecmd encp --d0sam /pnfs/rip6/M1 .
INFILE=/pnfs/rip6/M1
OUTFILE=/home/moibenko/enstore/src/tst/M1
FILESIZE=21036
LABEL=rip6-01
DRIVE=/rip6a/rip6/rip6.fake
TRANSFER_TIME=0.008138
SEEK_TIME=0.000161
MOUNT_TIME=0.043744
QWAIT_TIME=0.023627
TIME2NOW=0.888241
STATUS=ok
</pre>

This output is easily parsable and provides information about input and
output files, file size, volume label, drive used to read/write data,
transfer time, file seek time, volume mount time, wait time in the request
queue, operation completion status, total time sinse the invocation of <em>encp</em>
till the end of the operation.

<h3><a name="encp_throttle">
1.2.2 Throttling in <em>encp</em>
</h3>

It is important not to swamp any system.
In Enstore, a first level of throttling is implemented in
<em>encp</em>.
Control communications in Enstore uses a simple reliable
request-response protocol implemented in UDP, whereas data transfers
are implemented using two TCP ports.
A fixed number, currently 30, of pre-allocated TCP ports are
arbitrated among all instances of <em>encp</em> on a given machine.
Consequently, the system will survive the worst sort of abuse,
for example, someone forking off 200 copy requests,
since at most 15 will be active in the system at any time.

<h3><a name="encp_list_from hsm">
1.2.3 Specifying lists of files when reading from or writing to the HSM
</h3>

<em>Encp</em> duplicates unix cp's behaviour when one tries to read or write
multiple input files from the HSM: <br> ecmd <em>encp</em> [options]
source... directory<br> That is, the final item has to be a directory when
specifying an input list.  There is one caveat when specifying multiple files
using a single <em>encp</em> command: there is at most one mover processing your
request list at a time. If you want more than one mover active, you should use
multiple <em>encp</em> commands when you are reading lists of files from more than
one volume.  The reason <em>encp</em> uses just one mover is to keep the system
simple (if you want to use multiple movers, then use many <em>encp</em>
commands).
<p>

On reads from the HSM, <em>encp</em> scans all specified files and groups them according
to which volume they are on, orders them according location on a tape, and then submits all the file requests that are
on one specific volume, reads all the files for that volume from the hsm,
and then proceeds to the next volume.  <em>Encp</em> processes the volumes in any
order it chooses.
<p>

On writes to the HSM, <em>encp</em> processes each input file sequentially. Since
the user must specify a single output directory, all input files must belong to
the same file family, and hence could all go to the same tape (if
possible). <em>Encp</em> sets a flag,"don't dismount the volume too quickly -
there's more files coming for the same family", that the mover uses to
postpone the dismount and thereby avoid the extra times involved in the
volume manipulations.  Please note that there is no guarantee that all the
files will go to one tape (there might not be room) or that they will be
grouped together on the tape (there may be other writes to the same family
that get intermixed).
<p>

Consider the following example (P=/pnfs/enstore/airedale) reading from
the HSM:
<p>

The following files on flop301: ran-1, ran-2 ran-3, ran-4<br>
The following files on flop302: ran-5, ran-6 ran-7, ran-8<br>
The following files on flop302: ran-9, ran-10 ran-11, ran-12<br>
<p>

<em>Encp</em> submits the requests for all files on flop301 and reads back those
files and then does the same for flop302 and flop303.
<p>

Here is the output of an actual test:
<pre>
$ ecmd encp $P/ran-1       $P/ran-2        $P/ran-3        $P/ran-4 \
            $P/test2/ran-5 $P/test2/ran-6  $P/test2/ran-7  $P/test2/ran-8 \
            $P/test3/ran-9 $P/test3/ran-10 $P/test3/ran-11 $P/test3/ran-12 .
/pnfs/enstore/airedale/test2/ran-5 -> /home/bakken/enstore/src/ran-5 : 102400 bytes copied from flop302 at 0.190780317582 MB/S  requestor:bakken     cum= 3.534426
/pnfs/enstore/airedale/test2/ran-6 -> /home/bakken/enstore/src/ran-6 : 102400 bytes copied from flop302 at 0.426923710317 MB/S  requestor:bakken     cum= 3.787567
/pnfs/enstore/airedale/test2/ran-7 -> /home/bakken/enstore/src/ran-7 : 102400 bytes copied from flop302 at 0.462479436263 MB/S  requestor:bakken     cum= 3.999860
/pnfs/enstore/airedale/test2/ran-8 -> /home/bakken/enstore/src/ran-8 : 102400 bytes copied from flop302 at 0.462523304015 MB/S  requestor:bakken     cum= 4.212137
/pnfs/enstore/airedale/test3/ran-9 -> /home/bakken/enstore/src/ran-9 : 102400 bytes copied from flop303 at 0.129460905635 MB/S  requestor:bakken     cum= 5.479408
/pnfs/enstore/airedale/test3/ran-10 -> /home/bakken/enstore/src/ran-10 : 102400 bytes copied from flop303 at 0.491317380712 MB/S  requestor:bakken     cum= 5.679356
/pnfs/enstore/airedale/test3/ran-11 -> /home/bakken/enstore/src/ran-11 : 102400 bytes copied from flop303 at 0.454401235403 MB/S  requestor:bakken     cum= 5.895427
/pnfs/enstore/airedale/test3/ran-12 -> /home/bakken/enstore/src/ran-12 : 102400 bytes copied from flop303 at 0.455049173862 MB/S  requestor:bakken     cum= 6.111198
/pnfs/enstore/airedale/ran-1 -> /home/bakken/enstore/src/ran-1 : 102400 bytes copied from flop301 at 0.124436148701 MB/S  requestor:bakken     cum= 7.414610
/pnfs/enstore/airedale/ran-2 -> /home/bakken/enstore/src/ran-2 : 102400 bytes copied from flop301 at 0.45439695057 MB/S  requestor:bakken     cum= 7.630682
/pnfs/enstore/airedale/ran-3 -> /home/bakken/enstore/src/ran-3 : 102400 bytes copied from flop301 at 0.422761844505 MB/S  requestor:bakken     cum= 7.862842
/pnfs/enstore/airedale/ran-4 -> /home/bakken/enstore/src/ran-4 : 102400 bytes copied from flop301 at 0.454517460805 MB/S  requestor:bakken     cum= 8.078855
</pre>


<h3><a name="encp_queue">
1.2.4 <em>Encp</em> Transfer Lists
</h3>

One of the requests of the Fermilab Farms group was the ability to query
Enstore to determine what transfers were in progress and pending on a
certain node. If they need to take a farm node out of service and they
want to know what transfers are active on the node before it is taken
down.  [Some users submit jobs even when they know the node is going to be
down and then complain when their jobs fail. This feature will allow the
farms group to send and automated message saying that the node was taken
down and explain why their transfers failed.]
<p>
This is straightforward and easy command, except for 2 complications:
<ul>
  <li> Python is (currently) required to get the transfer queue
  <li> The command must be available everywhere the <em>encp</em> clients run and
       python isn't typically available everywhere.
</ul>

Normally <em>encp</em> is involved only with copying files.  It is also the
only task distributed with the <em>encp</em> client software.  We chose to
extend the <em>encp</em> client code so it could determine the transfer
queues in addition to its main responsibility of copying files.  This got
around both complications.
<p>
Here's an example of how it is used:
<pre>
$ ecmd encp --queue rip8.fnal.gov
rip8.fnal.gov bakken /raid/1MB.trand /pnfs/grau/ait/jon1/1MB.trand P
rip8.fnal.gov bakken /raid/1GB.trand /pnfs/grau/ait/jon2/1GB.trand M
rip8.fnal.gov bakken /raid/1GB.trand /pnfs/grau/ait/jon1/1GB.trand M

$ ecmd encp --queue rip4.fnal.gov
rip4.fnal.gov bakken /raid/1MB.trand /pnfs/grau/ait/jon2/1MB.trand M
</pre>

The 1st field in the output is the node name, the 2nd is the requester, the
3rd is the input filename, and the 4th is the output filename.  The 5th and
last field can have 2 values: "P" denotes a <u>P</u>ending transfer still
in the library manager queues and "M"
signifies a active transfer at a <u>M</u>over.

<h3><a name="encp_errors">
1.2.5 <em>Encp</em> Return Status 
</h3>
Every individual encp request returns a status allowing to identify (and in some
cases resolve)
occured problem, if any. <em>Encp</em> processes the status internally and ether
rertires request (providing user with intermediateate status) request or
terminates with appropraite status. The only successful status returned by <em>encp</em> is <b>"OK"</b> and the only successful <em>encp</em> exit code is <b>0</b>. Below is the list of statuses reuterned to user:
<center><table BORDER COLS=2 WIDTH="80%" NOSAVE >
<tr VALIGN=CENTER NOSAVE>
<td NOSAVE><b>STATUS</b></td>
<td NOSAVE><b>DESCRIPTION</b></td>
</tr>

<tr NOSAVE>
<td NOSAVE>OK</td>
<td NOSAVE>Operation Cmopleted Successfuly</td>
</tr>

<tr NOSAVE>
<td NOSAVE>KEYERROR</td>
<td NOSAVE>Not existing reference key</td>
</tr>

<tr NOSAVE>
<td NOSAVE>DOESNOTEXIST</td>
<td NOSAVE>Object (file name, etc.) does not exist</td>
</tr>

<tr NOSAVE>
<td NOSAVE>NOMOVERS</td>
<td NOSAVE>No Movers to process request</td>
</tr>

<tr NOSAVE>
<td NOSAVE>MOUNTFAILED</td>
<td NOSAVE>Mount of requred volume failed</td>
</tr>

<tr NOSAVE>
<td NOSAVE>DISMOUNTFAILED</td>
<td NOSAVE>Dismount of requred volume failed</td>
</tr>

<tr NOSAVE>
<td NOSAVE>MEDIA_IN_ANOTHER_DEVICE</td>
<td NOSAVE>Requested Media is in another Device</td>
</tr>

<tr NOSAVE>
<td NOSAVE>MEDIAERROR</td>
<td NOSAVE>Bad Media</td>
</tr>

<tr NOSAVE>
<td NOSAVE>USERERROR</td>
<td NOSAVE>User Error</td>
</tr>

<tr NOSAVE>
<td NOSAVE>DRIVEERROR</td>
<td NOSAVE>Drive Error</td>
</tr>

<tr NOSAVE>
<td NOSAVE>UNKNOWNMEDIATYPE</td>
<td NOSAVE>Unknown media type</td>
</tr>

<tr NOSAVE>
<td NOSAVE>NOVOLUME</td>
<td NOSAVE>Volume does not exist</td>
</tr>

<tr NOSAVE>
<td NOSAVE>NOACCESS</td>
<td NOSAVE>Volume marked as no access</td>
</tr>

<tr NOSAVE>
<td NOSAVE>CONFLICT</td>
<td NOSAVE>Configuration conflict detected</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_NOTAPE</td>
<td NOSAVE>Requested volume was not found in the library</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_TAPEBUSY</td>
<td NOSAVE>Requested volume is in another drive.</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_DRIVEBUSY</td>
<td NOSAVE>A volume is already in drive.</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_BADMOUNT</td>
<td NOSAVE>Mount failure or load operation failed.</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_BADSPACE</td>
<td NOSAVE>EOD cookie does not produce EOD.</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_ERROR</td>
<td NOSAVE>Error writing data block or file mark</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_EOT</td>
<td NOSAVE>Hit EOT while writing data block or file mark</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_NOBLANKS</td>
<td NOSAVE>No more blank volumes</td>
</tr>

<tr NOSAVE>
<td NOSAVE>WRITE_MOVER_CRASH</td>
<td NOSAVE>Mover crash during write operation</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_NOTAPE</td>
<td NOSAVE>Requested volume was not found in the library</td>
</tr>

<<tr NOSAVE>
td NOSAVE>READ_TAPE_BUSY</td>
<td NOSAVE>Requested volume is in another drive</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_DRIVEBUSY</td>
<td NOSAVE>A volume is already in drive</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_BADMOUNT</td>
<td NOSAVE>Mount failure or load operation failed</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_BADLOCATE</td>
<td NOSAVE>Failed space or initial CRC's don't match</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_ERROR </td>
<td NOSAVE>Error reading data block</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_COMP_CRC</td>
<td NOSAVE>CRC mismatch</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_EOT</td>
<td NOSAVE>Hit EOD when reading</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_EOD</td>
<td NOSAVE>Hit EOD when reading</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_UNLOAD</td>
<td NOSAVE>Error unloading volume</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_UNMOUNT</td>
<td NOSAVE>Error when unmounting volume</td>
</tr>

<tr NOSAVE>
<td NOSAVE>READ_MOVER_CRASH</td>
<td NOSAVE>Mover crash during read operation</td>
</tr>
</table></center>
<br>
More detailed description of these and other statuses and how they are processed inside of the system can be found in section 4 of this document.

<h3><a name="encp_client">
1.2.6 <em>Encp</em> Client Installation
</h3>

<em>Encp</em> is distributed via the Fermilab UPD mechanism. It is a static
binary product that does not depend on any other products or executable.
The binary <em>encp</em> product is made using the standard Python freeze
tool.  Basically, the Python freeze tool uses the regular Python parser to
parse the Python code and all its modules to produce a binary that people who
don't have Python can run.  <p>

Typically 2 <em>encp</em> products are available in UPD, one for general
use and one explicitly tailored for D0/SAM.
<pre>
$ upd list -a encp

DATABASE=/ftp/upsdb 
        Product=encp    Version=v0_11-sam       Flavor=Linux+2
                Qualifiers=""   Chain=current

        Product=encp    Version=v0_11   Flavor=Linux+2
                Qualifiers=""   Chain=""
</pre>




The installation procedure is straightforward:
<pre>
$ upd install -G"-c" encp
informational: beginning install of encp.
informational: transferred /ftp/products/encp/v0_11/Linux+2/encp_v0_11_Linux+2
        from fnkits.fnal.gov to
        /home/products/encp/v0_11
informational: transferred /ftp/products/encp/v0_11/Linux+2/encp_v0_11_Linux+2.table
        from fnkits.fnal.gov:/ to
        /home/products/upsdb/encp/v0_11.table.new
informational: ups declare succeeded
informational: ups declare succeeded
</pre>

The entire product consists of the <em>encp</em> binary, pcmd (a <em>pnfs</em> script
described in the <em>pnfs</em> section), and some UPS tables.  The <em>encp</em> binary is
large since it is statically linked.
<pre>
$ ls
     179 Nov 24 09:49 .manifest.encp
 2976514 Nov 24 09:24 encp*
    1690 Nov 24 09:24 encp.table
       9 Dec  2 15:20 enstore_variables.table -> rip.table
   11313 Nov 24 09:24 pcmd*
     398 Nov 24 09:24 rip.table
     399 Nov 24 09:24 sam.table
</pre>

Two environmental variables, ENSTORE_CONFIG_PORT, and ENSTORE_CONFIG_HOST,
control which Enstore system the <em>encp</em> requests go to. In order to
allow a user to override the default control environmental
variables distributed with the product, the <em>encp</em> product uses the
UPS concept of "virtual" products.  The basic idea is that everything in
the <em>encp</em> table file is general, and everything in the virtual
product enstore_variables.table file is user/installation specific.
<p>
Finally, when <em>encp</em> is setup, it creates a directory in the /tmp
area where it stores debugging information and other non-user
information. The user can ignore the files in the /tmp area. 
<p>
As an aside, it should be noted that since Enstore is still
in development, no versions are cut.  The complete UPS product structure is
finished.  For new installations, typically we CVS checkout code, issue one
make command, and the product is ready to be used.  We expect to cut
versions of Enstore when it is appropriate. These versions will not be
frozen, <it>i.e.</it>, they will need Python and the other dependent products.

<hr>
<h2><a name="servers">
1.3.0 Enstore Servers
</h2>
Enstore servers are software entities which handle media.
The high level  concepts are as follows:
<dl>
<dt><em>Physical library</em>
<dd>Physical Library represents a real, tangible collection of media
along with software drivers/utilities to manipulate, read and write
and organize them.

A physical library can be thought of as consisting of
<ul>
        <li>one or more virtual libraries
        <li>a media changer (robot arm)
        <li>one of more media export/import slots
        <li>one of more drives (tape, cdrom, disk, etc.)
        <li>volumes (tape cartridges, cdroms, etc.)
</ul>
<p>
<em>Virtual Library</em> -- A virtual library contains one and only one
kind of media.
For example, Enstore divides an STK powderhorn
library holding 50, 20 and 10 GB redwood media into at least three
virtual libraries.  In common usage, the term "library" in Enstore
refers to a virtual library. Writes are directed to a specific (virtual)
library, thus selecting the media.
<p>
<em>Drives</em> -- Drives are bound to special processes called Mover
clients.
The drives can be dynamically assigned allowing the number of drives
to be less than the number of virtual libraries.
<p>
<em>Volumes</em> -- Are uniquely identified by an external label,
which is known to the Media Changer.
<p>
<dt><em>Quota Family</em>
<dd>A quota family is a set of pairs of media names and maximum number of volumes.
All files are created with respect to a quota family.
Creation of a file is not allowed if the maximum number of volumes
in that family would be exceeded.  *** Quotas are not implemented in Enstore yet ***
<p>
<dt><em>File family</em>:
<dd>A file family is specified by a name and an integer "width".
A file family is associated with every file creation.
Within a given library, Enstore keeps no more
than <em>"width"</em> volumes open for writing, and loads volumes on
no more than <em>"width"</em>
number of drives at any given moment.
This is not striping, but rather, the number of different volumes,
and hence different files, which can be active at one time.
Once a volume is associated with a file family, only files in that
family will be placed on the volume.
By design, there is no pre-set limit on the number of file families.
Clever use of file families will allow volumes to be faulted out to
"shelf", and also to decrease access times for subsequent reads.  When a file
family has filled all of its "width" media, new media
are drawn out of a pool of blanks.
</dl>
Media ejected to shelf are put into a shelf virtual library and
are controlled by a shelf Library Manager.
Users are informed that this data is currently unavailable, and if
they really want the data, arrangements should be made to have the
media placed in a library which is accessible,
or get it manually later.
<p>
Each of the servers listed below is discussed in further detail in its own
section.  Please refer to these sections for information on detailed 
functionality and specific command line interfaces.
<ul>
 <li>Configuration Server - administers Enstore system configuration information.
 <li>Library Manager - queues and dispatches work for a virtual library.
 <li>Mover - handles the actual transfer of data from a volume to the user.
 <li>Media Changer - represents a physical device or operator who performs mounts/dismounts of volumes on drives.
 <li>Inquisitor - monitors Enstore system status and activity.
 <li>File Clerk - administers file information (<it>e.g.</it> - location cookies).
 <li>Volume Clerk - administers volume information (<it>e.g.</it> bytes left).
 <li>Log Server - formats and records Enstore system log messages.
 <li>Admin Clerk - allows administrative access to the Enstore system.
 <li>Alarm Server - analyzes Enstore system status and activity information to determine system health.
</ul>
<h3><a name="servers_cmds">
1.3.0.1 General Command Line Control of Servers
</h3>
Enstore servers implement a common uniform interface.  Common parsing is used
to determine command line options.  All servers support a set of general options
as well as those specific in functionality.  Control of the servers is done via
<em>ecmd</em>.  This command provides a general way of accessing the individual
servers as well as the Enstore system as a whole.  Examples of using this command
to accomplish this are given following the table of general options. <i>ecmd</i>
recognizes the following server names when sending commands to the server:
<p>
<ul>
 <li>Configuration Server - config
 <li>Library Manager - lmc
 <li>Mover - mov
 <li>Media Changer - mcc
 <li>Inquisitor - inq
 <li>File Clerk - fcc
 <li>Volume Clerk - vcc
 <li>Log Server - log
 <li>Admin Clerk - adm
 <li>Alarm Server - alarm
</ul>
For example : <i>ecmd inq --help</i>
<p>
When <i>ecmd</i> is used to start/stop servers, the server must be specified by
using the full Ascii name specified in the configuration file. For example:<br>
<ul>
<li>config
<li>rip6.library_manager
<li>rip6.mover
<li>rip6.media_changer
<li>file_clerk
<li>volume_clerk
<li>logserver
<li>admin_clerk
<li>alarm_server
</ul>

<p>
<table BORDER COLS=3 WIDTH="100%" NOSAVE >
<TH COLSPAN=3 VALIGN=CENTER>General Server Options</TH>
<tr VALIGN=CENTER NOSAVE>
<td NOSAVE><b>OPTION</b></td>
<td NOSAVE><b>SWITCH</b></td>
<td NOSAVE><b>DEFAULTS</b></td>
</tr>
<tr NOSAVE>
<td NOSAVE>check if the server process exists</td>
<td NOSAVE>--alive</td>
<td NOSAVE>None</td>
</tr>
<tr NOSAVE>
<td NOSAVE>specify the time to wait for a response from the server</td>
<td NOSAVE>--alive_rcv_timeout=value</td>
<td NOSAVE>10</td>
</tr>
<tr NOSAVE>
<td NOSAVE>specify the number of times to retry sending a message to a server</td>
<td NOSAVE>--alive_retries=value</td>
<td NOSAVE>None</td>
</tr>
<tr NOSAVE>
<td NOSAVE>specify the hostname where the configuration server is running</td>
<td NOSAVE>--config_host=value</td>
<td NOSAVE>environmental variable, ENSTORE_CONFIG_HOST, set by the UPS setup command</td>
</tr>
<tr NOSAVE>
<td NOSAVE>specify the port number that the configuration server responds to</td>
<td NOSAVE>--config_port=value</td>
<td NOSAVE>environmental variable, ENSTORE_CONFIG_PORT, set by the UPS setup command</td>
</tr>
<tr NOSAVE>
<td NOSAVE>print a short help message abouting using the server</td>
<td NOSAVE>--help</td>
<td NOSAVE>None</td>
</tr>
<tr NOSAVE>
<td NOSAVE>change the amount of information printed about the transfer (in the server)</td>
<td NOSAVE>--server_verbose=value</td>
<td NOSAVE>None</td>
</tr>
<tr NOSAVE>
<td NOSAVE>change the amount of information printed about the transfer (in the client)</td>
<td NOSAVE>--verbose=value</td>
<td NOSAVE>0 (no printing)</td>
</tr>
</table>
<p>
It should be noted that currently the <i>help</i> option produces insufficient 
and inelegant information.  This will be fixed in the future.
<p>
<table BORDER COLS=3 WIDTH="100%" NOSAVE >
<TH COLSPAN=3 VALIGN=CENTER>Enstore System Command Line Control</TH>
<tr VALIGN=CENTER NOSAVE>
<td NOSAVE><b>OPTION</b></td>
<td NOSAVE><b>COMMAND</b></td>
</tr>
<tr NOSAVE>
<td NOSAVE>start the Enstore system on the current node</td>
<td NOSAVE>ecmd start</td>
</tr>
<tr NOSAVE>
<td NOSAVE>start only the file_clerk on the current node</td>
<td NOSAVE>ecmd start --just file_clerk</td>
</tr>
<tr NOSAVE>
<td NOSAVE>stop the Enstore system on the current node</td>
<td NOSAVE>ecmd stop</td>
</tr>
<tr NOSAVE>
<td NOSAVE>stop only the log_server on the current node</td>
<td NOSAVE>ecmd stop --just logserver</td>
</tr>
<tr NOSAVE>
<td NOSAVE>stop and then start the Enstore system</td>
<td NOSAVE>ecmd restart</td>
</tr>
<tr NOSAVE>
<td NOSAVE>stop and then restart only the inquisitor on the current node</td>
<td NOSAVE>ecmd restart --just inq</td>
</tr>
</table>
<p>
<h3><a name="volume_clerk">
1.3.1 Volume Clerk
</h3>
The Volume Clerk keeps and administers volume information that it stores in a single table database.
There is one record for each volume known to the system. The record is looked up by a key, which  is the volume's external label.
The information tracked for each volume is described in the table below.
The default values are shown in parentheses ().
<center><table>
<td valign=top><b>Column Name</b>
<td valign=top><b>Type</b>
<td valign=top><b>Comments</b>
<tr>
<td valign=top>external_label
<td valign=top>string [primary_key]
<td valign=top>Volume name specified by user on volume creation; is used to
display volume metadata.
<tr>
<td valign=top>file_family
<td valign=top>string ("none")
<td valign=top>File family name, specified by user on volume creation; only
files that belong to this family will be stored on this volume.
<tr>
<td valign=top>media_type
<td valign=top>string
<td valign=top>Specified at volume creation; implies the block-size; used for
writing.
<tr>
<td valign=top>library
<td valign=top>string
<td valign=top>Specified by user on volume declaration; defines which (virtual)
library currently holds the volume
<tr>
<td valign=top>first_access
<td valign=top>int (-1)
<td valign=top>Unix time when user issues the first write command to copy data to the volume.
Set by the Volume Clerk.
<tr>
<td valign=top>last_access
<td valign=top>int (-1)
<td valign=top>Unix time when  user last accessed the volume. Set by the Volume Clerk.
<tr>
<td valign=top>declared
<td valign=top>int
<td valign=top>Unix time when the volume is declared available to the system. Set by the Volume Clerk.
<tr>
<td valign=top>capacity_bytes
<td valign=top>64-bit int
<td valign=top>Specified by user on volume creation; estimate of the number of
bytes that would fit on the volume.
<tr>
<td valign=top>blocksize
<td valign=top>int
<td valign=top>Set by the Volume Clerk; derived from the the media type.
<tr>
<td valign=top>remaining_bytes
<td valign=top>64-bit int
<td valign=top>Specified by the user on volume creation; estimate of the number
of bytes that would fit on the volume; updated by the Volume Clerk every time
data are written to the media.
<tr>
<td valign=top>eod_cookie
<td valign=top>string ("none")
<td valign=top>Tells the driver how to space to the end of the volume; it is
driver specific; updated by the Volume Clerk when data are written on the
media.
<tr>
<td valign=top>wrapper
<td valign=top>string ("cpio")
<td valign=top>Wrapper method; currently specifies the format of the files on
the volume.
<tr>
<td valign=top>sum_rd_err
<td valign=top>int (0)
<td valign=top>Read error count; Volume Clerk increments this field when the Mover
receives an error while reading from the volume.
<tr>
<td valign=top>sum_rd_access
<td valign=top>int (0)
<td valign=top>Read access count; Volume Clerk increments this field
every time a file is read.
<tr>
<td valign=top>sum_wr_err
<td valign=top>int (0)
<td valign=top>Write error count; Volume Clerk increments this field when the
Mover receives an error while writing to the volume.
<tr>
<td valign=top>sum_wr_access
<td valign=top>int (0)
<td valign=top>Write access count; Volume Clerk increments this field every
time a file is written.
<tr>
<td valign=top>user_inhibit
<td valign=top>string (d:"none" or "readonly", "noaccess")
<td valign=top>Specified by user at volume creation; access level for this
volume, updated by Volume Clerk.
<tr>
<td valign=top>system_inhibit
<td valign=top>string (d:"none" or "writing", "readonly", "full", "noaccess")
<td valign=top>Administrator generated limitation on the kind of access permitted to
this volume; updated by Volume Clerk when data are written on the volume, an
error occurred while data were being written or the file size exceeded the
remaining number of bytes on the volume.
</table></center>
<p>
The Volume Clerk does the following operations:
<ul>
<li>show the name of all the volumes
<li>show volume information
<li>add a volume
<li>delete a volume
<li>find an appropriate volume on which to write the file
<li>change the number of remaining bytes on the volume
<li>set the number of read/write errors
<li>set the current status of the volume
<li>set the volume as readonly
<li>start/stop backup of volume journals
</ul>

User may interact with Volume Clerk directly through <em>ecmd vcc</em>
command.
<center><TABLE BORDER=1 CELLPADDING=2 CELLSPACING=0>
<th> Description
<th> Command
<tr>
<td valign=top>show the name of all the volumes
<td valign=top>ecmd vcc --vols
<tr>
<td valign=top>show volume information
<td valign=top>ecmd vcc --vol volume_name
<tr>
<td valign=top>add a volume
<td valign=top>ecmd vcc --addvol library file_family media_type volume_name capacity remaining_capacity
<tr>
<td valign=top>delete a volume
<td valign=top>ecmd vcc --delvol volume_name
<tr>
<td valign=top>find an appropriate volume on which to write the file
<td valign=top>ecmd vcc --nextvol library_name minimal_remaining_bytes file_family
<tr>
<td valign=top>put volume into a new library
<td valign=top>ecmd vcc --newlib volume_name library_name
<tr>
<td valign=top>clear system inhibitors to the volume
<td valign=top>ecmd vcc -clrvol volume_name
<tr>
<td valign=top>mark no access to this volume
<td valign=top>ecmd vcc --noavol volume_name
<tr>
<td valign=top>set the volume as read only
<td valign=top>ecmd vcc --rdovol volume_name
<tr>
<td valign=top>start/stop backup of volume journals
<td valign=top>ecmd vcc --backup
</table></center>

<hr>
<h3><a name="file_clerk">
1.3.2 File Clerk
</h3>
The File Clerk tracks files in the system. There is one record for each file
in the system.
The records are keyed.
The key is the string version of the bit file ID.
The default values are shown in parentheses ().
The fields tracked are as follows:
<center><table>
<td valign=top><b>Column Name</b>
<td valign=top><b>Type</b>
<td valign=top><b>Comments</b>
<tr>
<td valign=top>bfid
<td valign=top>string [primary_key]
<td valign=top>bit file ID; uniquely identifies every file in the system.
<tr>
<td valign=top>external_label
<td valign=top>string
<td valign=top>Volume name on which the file has been written; same as the
external_label in the volume table.
<tr>
<td valign=top>bof_space_cookie
<td valign=top>string
<td valign=top>Driver specific string telling how to space to the file on the
media. A lexical sort of all bof_space_cookies for a given volume will yield a optimized
traversal of the volume.
<tr>
<td valign=top>complete_crc
<td valign=top>int
<td valign=top>crc of all the bits sent by the user.
<tr>
<td valign=top>sanity_cookie
<td valign=top>string ("(0,0)")
<td valign=top>Number of bytes used for a sanity crc and the sanity crc
itself.  The sanity crc is just the normal crc but only for the 1st N bytes in
the file. This allows the Mover to check early in the transfer process that it
probably has the right user file selected; it at least will know if it has the
wrong file.
</table></center>
<p>
The File Clerk supports the following requests:
<ul>
<li>show bfid of all the files
<li>show file information
<li>start/stop backup of file journals
<li>assist in processing file read requests
</ul>

Users may interact with File Clerk directly through <em>ecmd fcc</em>
command
<center><TABLE BORDER=1 CELLPADDING=2 CELLSPACING=0>
<th> Description
<th> Command
<tr>
<td valign=top>show bfid of all the files
<td valign=top>ecmd fcc --bfids
<td valign=top>
<tr>
<td valign=top>show file information
<td valign=top>ecmd fcc --bfid=bit-field-ID
<tr>
<td valign=top>start/stop backup of volume journals
<td valign=top>ecmd fcc --backup
</table></center>
<hr>
<h3><a name="library_manager">
1.3.3 Library Manager
</h3>
The Library Manager is a server which queues up and dispatches work for
a virtual library. There is one Library Manager for each virtual library.
It has two types of clients: 
<ol>
<li><em>Users</em> -- requesting to have their files read or written.
<li><em>Movers</em> -- seeking to actually read or write files.
</ol>
<p>
It can be also accessed from a command line interface
<p>
Enstore does not limit the number of Library Managers and the
relation between Library Managers and Movers is many to many.  That is, one
Library Manger may have many Movers associated with it and, one Mover may have
many Library Managers associated with it.

Information about Library Managers is contained in the Enstore configuration
dictionary and is available to clients via the Configuration Server.
Each Library Manager is specified in the configuration dictionary as follows:
<pre>
configdict['rip6.library_manager']   = { 'host':'rip5',
                                         'port':7506,
                                          'logname':'RP6LBM' }
configdict['ait.library_manager']    = { 'host':'rip5',
                                         'port':7507,
                                          'logname':'AITLBM' }
configdict['mam.library_manager']    = { 'host':'rip5',
                                         'port':7508,
                                         'logname':'MAMLBM' }
configdict['dlt.library_manager']    = { 'host':'rip5',
                                         'port':7509,
                                         'logname':'DLTLBM' }
configdict['red50.library_manager'] = { 'host':'rip5',
                                         'port':7510,
                                         'logname':'RD5LBM' }
configdict['red20.library_manager'] = { 'host':'rip5',
                                         'port':7511,
                                         'logname':'RD2LBM' }
where:

<b>*.library_manager</b> is the name of the Library Manager.
<b>host</b> is the host name where the Library Manager runs.
<b>port</b> is the Library Manager's command communication port.
<b>logname</b> is a name identifying the Library Manger in the log file.
</pre>
<p>
When the Library Manager starts it requests from Configuration Server a list
of Movers associated with it and stores them into internal Mover List. Each
Mover has an entry in the configuration dictionary describing the Library
Manager(s) associated with it. This entry can be a single name or a list of
names:
<pre>
configdict['DE14DLT.mover'] = { 'host':'rip1',
                                'port':7527,
                                'logname':'DE14MV',
                                'library':'dlt.library_manager',
                                'device':'/dev/rmt/tps2d2n',
                                'driver':'FTTDriver', 
                                'mc_device':'DE14',
                                'media_changer':'de14.media_changer' }
</pre>

<p>
The Mover List is used to select Movers to do work and evenly distribute
work between all Movers associated with a Library Manager.

<p>
When the Mover starts it sends the command("idle_mover") to all Library Managers
associated with it to declare itself active and available. If the Library Manager
does not have this Mover in its Mover List, it will be added to the list upon
receipt of this command. This Mover - Library Manager initial communication
allows to the dynamic configuration of Movers.

<h3><a name="user_request">
1.3.3.1 Users' Requests
</h3>
<dl>
<dt>Writes into the system
<dd>Based on the user's mss destination filename, a <em>pnfs</em> tag
associated with the destination directory, identifies
    the library for a write request allowing the <em>encp</em> program to compose
    a write request and contact the appropriate Library Manager
    directly.
    The Library Manager queues the work, and acknowledges the request.
    When a write request is completed the Library Manager will dismount
     corresponding volume. Library Manager does not know that there is another
     request for the same volume coming a little bit later. However the User
     knows. To inform the Library Manger that there may be another request for
     the same volume user supplies a "delayed_dismount" flag in a current
     request. If this flag is set, Library Manager will not dismount the
     volume even if there are no current requests for it. Instead it will
     postpone the dismount for a certain period of time and, if there is
     still no requests for this volume after time out, Library Manager will
     communicate with corresponding Mover to dismount the volume.

<dt>Reads from the system
<dd>Given the fact that users may mv the pnfs files, on reads from the system,
     <em>pnfs</em> can only provide the bit
    file ID associated with the file. In this case, <em>encp</em> contacts the
     File Clerk, which returns bit file ID and additional information about
     requested file as well as the Library Manager associated with this file.
     Then encp sends read request to this Library Manager.
<dt>Inquiries
<dd>Currently there is only one kind of such a request allowing to observe
     requests queued in the Library Manager queues. This command asks the Library Manager to provide information about all current requests from user node sphinx.fnal.gov. The output format is: <br>
[user node] [user name] [input file] [output file] [request status]
     request status can be either P (pending) or M (at Mover)<br>
     The exmple of the output is given in 1.2.4
</dl>
<p>
Work can be prioritized.
Smaller priority number means higher priority.
Currently, write and read are both priority 1 for our test purposes.
Any priority mechanism should be able to be developed.
However, the system will exhaust all work for a volume,
given that it has been mounted, regardless of priority.
<p>
The Library Manager tries to sort read requests according to file location on
the tape. If read request has been already sent to the Mover the next request
to this Mover for the same tape will be for the file which location number is
higher than the current one. If location number is less than current it will
be placed at the end of the request list.

<p>
Once a User request comes the Library Manager tries to pick up the next
available(marked as "idle") Mover an send a "summon" message to it. The purpose
of this message is to cause a Mover to send a Mover Request to the Library
Manger. Mover Requests are described in the next section.
The mechanism of the selection of a particular Mover allows to control some
error conditions and implement a retry logic. For this purpose there is dynamic
list of volumes on which write or read requests failed - Suspected Volumes
List.
It is keyed by the volume external label and contains sublists of Movers on
which the request for this volume failed. This allow to not use the same Mover
when User retries its request.

When the Library Manager "summons" a Mover it changes the Mover state into the
Mover List to "summoned" and puts in into the Summoned Movers List. Every time
the Library Manager sends a message there is a time out handler that is being
invoked if a response does not arrive before the time out expires. Time out
handler will retry to "summon" the mover which time out has expired and,
eventually remove the Mover for which "summon" retries expire, from the Mover
List.

<h3><a name="mover_request">
1.3.3.2 Movers' Requests
</h3>
<p>
The Enstore system keeps unassigned read and write requests in a queue of
unallocated (pending) work in the Library Manager. Once a request for the next work comes from the Mover ("idle" or "have bound volume: idle"), the Library Manager puts the request in a "work at mover" queue and responds to the Mover with appropriate ticket. The reason for this is to track the
volumes for scheduling : the Library Manager must not submit to a Mover
request a volume for which is already in use by another Mover. It is the Mover, and not the Library Manager which completes requests.
The two Library Manager request queues are:
<ul>
<li>pending work
<li>work at a Mover.
</ul>
<p>
It is important to keep these queues consistent. Volume and reading
errors are handled in the Mover and partially in the Library Manager.
<p>
Movers seek to transport data
between media and users over a TCP socket.
When "summoned" or have completed a work Movers contact Library Managers
seeking work. If the Library Manager
has work, it send send a corresponding ticket to the Mover, which in turn
mounts the volume if necessary and transfers the data between user and media
When Mover completes a work it sends to the Library Manager request for another
work and if it gets a reply that there no more work for it, it dismount a
volume. 
A Mover may also have decided to dismount a volume unilaterally because it
ran into trouble. But it actually does it receiving no_work reply from the
Library Manager. Library Manager - Mover communications are in the tables
below
<table>
<td><b>Library Manager sends </b>
<td><b>Mover Sends</b>
<tr>
<td valign=top>summon
<td>idle - ready to do a work;
<br>have bound volume:busy - doing work;
<br>or have bound volume:idle - volume is mounted but no work
</table>
<table>
<td><b>Mover sends</b>
<td><b>Library Manager may respond</b>
<tr>
<td valign=top>idle_mover
<td>if work needs to be done - read/write; <br>or no_work
<tr>
<td valign=top>have_bound_volume
<td>if reads/writes pending for the volume - read/write;  <br>or if no work - unbind_volume
<tr>
<td>unilateral_unbind
<td>no work
</table>

<table>
<td valign=top><b>Library has just responded</b>
<td valign=top><b>Mover sends</b>
<td valign=top><b>Library Manager presumes</b>
<tr>
<td valign=top>read or...<br>write
<td valign=top>idle_mover
<td valign=top>Mover crashed and was re-started
<tr>
<td valign=top>
<td valign=top>have_bound_volume
<td valign=top>look for work on that volume<br>
    if work, give it<br>
    if none, unbind_volume
<tr>
<td valign=top>
<td valign=top>unilateral_unbind
<td valign=top>update Suspected Volumes List and respond with no_work
<tr>
<td valign=top>acknowledged a...<br>unilateral unbind or..<br>idle Mover <br>
no_work
<td valign=top>idle_mover
<td valign=top>Mover is available for work, If more work available, bind a
    volume
<tr>
<td valign=top>
<td valign=top>have_bound_volume
<td valign=top>it has restarted, the Mover had a volume from a previous
    instance of me, tell it to unbind
<tr>
<td valign=top>
<td valign=top>unilateral_unbind
<td valign=top>no work
</table>

Note that if a Mover should crash holding a volume, the worst that can
happen is that the Library Manager will be unable to schedule work for
that volume. If the physical library has more than one drive, the system
should be able to continue servicing requests.
<p>
<h3><a name="lm_commands">
1.3.3.3 Library Manager Query Commands
</h3>
The Library Manager supports queries that provide information about its
internal queues.
Some of these commands are general to all servers and are described else where. Library Manager specific commands are:
<ul>
<li><b>getwork</b> - returns all work requests currently in the Library Manager
queues in two lists. 
The first is a list of pending work requests and the second is a list works at Movers.
<li><b>getmoverlist</b> - returns a list of the Movers currently known to this Library Manager and their status.
<li><b>get_suspect_vols</b> - returns a list of suspect volumes. 
A description and the purpose of this list has been discussed above. 
This list contains volumes requests which have failed but are retryable.
</ul>
The format of these lists are python dictionaries.  Other programs retieve these lists and format them
for presentation.
     Sample output from these commands:<br><br>
<b>getwork</b>
<pre>
[enter command] ecmd lmc --getwork sphinxdisk.library_manager
[{'callback_addr': ('131.225.81.23', 7600),
  'encp': {'adminpri': -1,
           'agetime': 0,
           'basepri': 1,
           'curpri': 1,
           'delayed_dismount': 0,
           'delpri': 0},
  'fc': {'bfid': '91548494800000L',
         'complete_crc': 2048910256,
         'external_label': 'flop1',
         'location_cookie': '000000063488',
         'pnfsid': '000200000000000000514A98',
         'sanity_cookie': (9045, 2048910256),
         'size': 9045},
  'lm': {'address': ('131.225.81.23', 7503)},
  'retry_cnt': 0,
  'status': ('ok', None),
  'times': {'t0': 915731751.891, 'job_queued': 915731758.979},
  'unique_id': 'sphinx.fnal.gov-915731758.238293-3793',
  'vc': {'blocksize': 512,
         'capacity_bytes': 1400000L,
         'declared': 915469931.313,
         'eod_cookie': '000000108032',
         'external_label': 'flop1',
         'file_family': 'sphinx',
         'first_access': 915469958.425,
         'last_access': 915728933.0,
         'library': 'sphinxdisk',
         'media_type': 'diskfile',
         'remaining_bytes': 1291968L,
         'status': ('ok', None),
         'sum_rd_access': 0,
         'sum_rd_err': 0,
         'sum_wr_access': 0,
         'sum_wr_err': 0,
         'system_inhibit': 'none',
         'user_inhibit': 'none',
         'wrapper': 'cpio'},
  'work': 'read_from_hsm',
  'wrapper': {'fullname':'/usr/hppc_home/moibenko/enstore_test/enstore/src/tst/
admin_clerk_client.pyc',
              'gid': 5440,
              'gname': 'hppc',
              'inode': 0,
              'machine': ('Linux',
                          'sphinx.fnal.gov',
                          '2.0.35',
                          '#1 Thu Jul 23 14:01:04 EDT 1998',
                          'i686'),
              'major': 0,
              'minor': 5,
              'mode': 33268,
              'pnfsFilename': '/pnfs/enstore/sphinx/t1/admin_clerk_client.pyc',
              'pstat': (33204,
                        38881944,
                        5,
                        1,
                        6849,
                        5440,
                        9045,
                        915484948,
                        915484948,
                        915485267),
              'rmajor': 0,
              'rminor': 0,
              'sanity_size': 65535,
              'size_bytes': 9045,
              'uid': 6849,
              'uname': 'moibenko'}},
 {'callback_addr': ('131.225.81.23', 7600),
  'encp': {'adminpri': -1,
           'agetime': 0,
           'basepri': 1,
           'curpri': 1,
           'delayed_dismount': 0,
           'delpri': 0},
  'fc': {'bfid': '91548792000000L',
         'complete_crc': -1493930591,
         'external_label': 'flop1',
         'location_cookie': '000000073216',
         'pnfsid': '000200000000000000514B40',
         'sanity_cookie': (4538, -1493930591),
         'size': 4538},
  'lm': {'address': ('131.225.81.23', 7503)},
  'retry_cnt': 0,
  'status': ('ok', None),
  'times': {'t0': 915731751.891, 'job_queued': 915731759.085},
  'unique_id': 'sphinx.fnal.gov-915731758.243841-3793',
  'vc': {'blocksize': 512,
         'capacity_bytes': 1400000L,
         'declared': 915469931.313,
         'eod_cookie': '000000108032',
         'external_label': 'flop1',
         'file_family': 'sphinx',
         'first_access': 915469958.425,
         'last_access': 915728933.0,
         'library': 'sphinxdisk',
         'media_type': 'diskfile',
         'remaining_bytes': 1291968L,
         'status': ('ok', None),
         'sum_rd_access': 0,
         'sum_rd_err': 0,
         'sum_wr_access': 0,
         'sum_wr_err': 0,
         'system_inhibit': 'none',
         'user_inhibit': 'none',
         'wrapper': 'cpio'},
  'work': 'read_from_hsm',
  'wrapper': {'fullname':'/usr/hppc_home/moibenko/enstore_test/enstore/src/tst/
backup.py',
              'gid': 5440,
              'gname': 'hppc',
              'inode': 0,
              'machine': ('Linux',
                          'sphinx.fnal.gov',
                          '2.0.35',
                          '#1 Thu Jul 23 14:01:04 EDT 1998',
                          'i686'),
              'major': 0,
              'minor': 5,
              'mode': 33268,
              'pnfsFilename': '/pnfs/enstore/sphinx/t1/backup.py',
              'pstat': (33204,
                        38882112,
                        5,
                        1,
                        6849,
                        5440,
                        4538,
                        915487920,
                        915487920,
                        915488239),
              'rmajor': 0,
              'rminor': 0,
              'sanity_size': 65535,
              'size_bytes': 4538,
              'uid': 6849,
              'uname': 'moibenko'}},]

[{'callback_addr': ('131.225.81.23', 7600),
  'encp': {'adminpri': -1,
           'agetime': 0,
           'basepri': 1,
           'curpri': 1,
           'delayed_dismount': 0,
           'delpri': 0},
  'fc': {'bfid': '91548494100000L',
         'complete_crc': 1614017314,
         'external_label': 'flop1',
         'location_cookie': '000000055296',
         'pnfsid': '0002000000000000005149F8',
         'sanity_cookie': (7581, 1614017314),
         'size': 7581},
  'lm': {'address': ('131.225.81.23', 7503)},
  'mover': 'sphinxdisk.mover',
  'retry_cnt': 0,
  'status': ('ok', None),
  'times': {'in_queue': 1.77947795391,
            'lm_dequeued': 915731760.655,
            't0': 915731751.891},
  'unique_id': 'sphinx.fnal.gov-915731758.236400-3793',
  'vc': {'blocksize': 512,
         'capacity_bytes': 1400000L,
         'declared': 915469931.313,
         'eod_cookie': '000000108032',
         'external_label': 'flop1',
         'file_family': 'sphinx',
         'first_access': 915469958.425,
         'last_access': 915728933.0,
         'library': 'sphinxdisk',
         'media_type': 'diskfile',
         'remaining_bytes': 1291968L,
         'status': ('ok', None),
         'sum_rd_access': 0,
         'sum_rd_err': 0,
         'sum_wr_access': 0,
         'sum_wr_err': 0,
         'system_inhibit': 'none',
         'user_inhibit': 'none',
         'wrapper': 'cpio'},
  'work': 'read_from_hsm',
  'wrapper': {'fullname':'/usr/hppc_home/moibenko/enstore_test/enstore/src/tst/
admin_clerk_client.py',
              'gid': 5440,
              'gname': 'hppc',
              'inode': 0,
              'machine': ('Linux',
                          'sphinx.fnal.gov',
                          '2.0.35',
                          '#1 Thu Jul 23 14:01:04 EDT 1998',
                          'i686'),
              'major': 0,
              'minor': 5,
              'mode': 33268,
              'pnfsFilename': '/pnfs/enstore/sphinx/t1/admin_clerk_client.py',
              'pstat': (33204,
                        38881784,
                        5,
                        1,
                        6849,
                        5440,
                        7581,
                        915484942,
                        915484942,
                        915485261),
              'rmajor': 0,
              'rminor': 0,
              'sanity_size': 65535,
              'size_bytes': 7581,
              'uid': 6849,
              'uname': 'moibenko'}}]

</pre>
<b>getmoverlist</b>
<pre>
[enter command] ecmd lmc --getmoverlist sphinxdisk.library_manager
[{'address': ('131.225.81.23', 7508),
  'last_checked': 915731762.917,
  'mover': 'sphinxdisk.mover',
  'state': 'idle_mover',
  'summon_try_cnt': 0,
  'tr_error': 'ok'}]
</pre>
<b>get_suspect_vols</b> 
<pre>
[enter command] ecmd lmc --get_suspect_vols sphinxdisk.library_manager
[{'external_label': 'flop1', 'movers': ['sphinxdisk.mover']}]
</pre>

<hr>
<h3><a name="mover">
1.3.4 Mover
</h3>
A Mover task is bound to a single drive, and seeks to use that drive to service
read and write requests.  It communicates with the Library Manager in a
defined protocol, as just described.
<p>
The Mover is responsible for efficient data movement and as such is an
integral part of the system.  The architecture allows for performance critical
code to be written in C thus allowing efficient access to fundamental OS
features such as forking with minimal to no language overhead.
<p>
Although a Mover is bound to a drive, a drive may serve more than one
virtual library, i.e., the Mover has a dynamic list of of Library Managers
that it is supposed to service.  This has two benefits.
First, since a Library Manager handles only one type of media, a drive which
handles multiple types of media (i.e. different capacity media) can be shared
without a static partitioning of the system.
Second, if we are partitioning resources in a library, we can assign a Library
Manager to each type of use.  For example, suppose Group A and Group B want to share
the capacity of a library.
Suppose half the tapes belong to Group A  and the half to Group B. We want to guarantee that
Group A have one third of the tape drives, Group B have one third, and the last third
be shared.  The Movers can be configured to do this easily.  And with some slight
changes, this is how we can guarantee resources to data acquisition.
<p>


DUPLICATE DATA SECTION NEEDED.

<!-- ----------------------------------------------------->
When the Mover starts up, the 'idle_mover' request/command is sent to each
Library Manager configured and the responses from the Library Managers are
acted upon.  After the startup, the mover waits until it is 'summoned' by a
Library Manager.

<p>
When a Mover is summoned by a Library Manager, it will send one of three
request/commands to the Library Manager that summoned the mover:
<ol>
  <li> idle_mover
  <li> have_bound_volume, idle
  <li> have_bound_volume, busy
</ol>

When the mover is busy, the Library Manager should respond with 'no_work.'
Otherwise, the Library Manager can respond with 'no_work,'
'read_from_hsm,' or 'write_to_hsm.'

When the mover receives 'read_from_hsm' or 'write_to_hsm' it forks a
subprocess which handles the transfer utilizing a shared memory buffer. The
main Mover process can read shared memory locations to get the status of
the transfer.

The first thing the transfer process does is check to see if the waiting
<em>encp</em> is responsive. This involves contacting encp on the designated TCP
control port and sending along the TCP port designation for the data transfer.
If <em>encp</em> is responsive, the Mover proceeds with making
sure the proper volume is loaded in it's tape drive.

<p>
Reads -- Once a volume is bound the Mover may read a volume and send data
to a waiting <em>encp</em> program.
        The steps are:
<ol>
<li> Using the file_location_cookie, space to beginning of data.
<li> Read any wrappering information that precedes the actual data.
<li> Fork a process that reads and crc's the data from the volume verifing
     the sanity crc and placing the
     data in a 4 MB shared memory buffer.
<li> Write data from the shared memory to the user.
<li> Read any wrappering information that comes after the data.
<li> Close the data port.
<li> Tell the user done and all is well.
<li> Close the control port.
</ol>

<p>
Writes -- Once a volume is bound the Mover may receive data and write it to
the volume.
        The steps are:
<ol>
<li> Mark the volume as "writing". That will cause the volume to
     not be selected for subsequent writes, should we crash.
<li> Using the eod_space_cookie, space to end of volume. Try
     to verify that we are actually at the end of volume.
<li> Write any wrappering information that precedes the data.
<li> Fork a process that reads and crc's data from the user calculating the
     sanity crc and placing the data in a 4 MB shared memory buffer.
<li> Write data from the shared memory to the tape device.
<li> Close the data port.
<li> Write any wrappering information after the data.
<li> Compute new eod_cookie and tell Volume Clerk that the
     volume is writable. Update remaining bytes as well.
<li> Compute the file location cookie, and tell the bit
     File Clerk about the new file. Get a bit file ID in return.
<li> Give the bit file ID to <em>encp</em>. We are done.
</ol>

If any errors occur while reading or writing a volume, an attempt is made
to characterize them as either media or drive.  Depending upon the error, the
Mover will issue either have_bound_volume or unilateral_unbind to the
Library Manager.
This is discussed more completely in the section on Error control.
If the user drops the control tcp channel unilaterally, the Mover assumes he has
aborted the transfer.

If all is well with the entire transfer, the Mover issues a
have_bound_volume to the Library Manager and wait for further instructions.


<p>

<h4><a name="mover_cmds">
1.3.4.1 Command Line Control of the Mover
</h4>

The Mover is started and controlled through a command line interface using
ecmd. Option commands for "ecmd mov [--option_command] <mover>" include the
general option commands
supported by all other servers:
alive, alive_rcv_timeout=, alive_retries=, config_host=, config_port=,
help, server_verbose, verbose. Additionally the "status" option commands
is supported and produces the following:
<br>
$ ecmd mov --status fndaprdisk.mover<br>
{'wr_bytes': 6398896, 'rd_bytes': 6398896, 'no_xfers': 11, 'mode': 'w',
'bytes_to_xfer': 6398896, 'crc_func': '<built-in function ECRC>',
'state': 'idle', 'status': ('ok', None)}
<br>where
<dl>
<dt>wr_bytes
     <dd> the number of bytes written to the tape device when 'mode' is 'w'
	  else the number of bytes written to the user device.
<dt>rd_bytes
     <dd> the number of bytes read from the user device when 'mode' is 'w'
	  else the number of bytes read from the tape device.
<dt>no_xfers
     <dd> the number of completed transfers.
<dt>mode
     <dd> 'r' for reading from HSM, 'w' for writing to HSM.
<dt>bytes_to_xfer
     <dd> number of bytes to be transfer for current transfer if state is
	  'busy' or the last transfer is state is idle.
<dt>crc_func
     <dd>active crc function.
<dt>state
     <dd>'idle' if no transfer active, else 'busy.'
<dt>status
     <dd>should always be: ('ok', None)
</dl>

<h4><a name="mover_config">
1.3.4.2 Mover Config File Values
</h4>
<center><TABLE BORDER COLS=4 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>DICTIONARY ELEMENT</B></TD>
<TD NOSAVE><B>DEFINITION</B></TD>
<TD NOSAVE><B>DEFAULT</B></TD>
<TD NOSAVE><B>EXAMPLE VALUE</B></TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>host</TD>
<TD NOSAVE>node where mover runs</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>hppc</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>port</TD>
<TD NOSAVE>UPDport for mover communication</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>7516</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>logname</TD>
<TD NOSAVE>ascii value used for id in messages to log server</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>FMOV</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>library</TD>
<TD NOSAVE>list of libraries that the mover will contact when it starts up.</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>['fndaprdisk.library_manager']</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>media_changer</TD>
<TD NOSAVE>the name of the media changer server that will be communicated
with in order to load and unload tape cartidges.</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>'fndaprdisk.media_changer</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>mc_device</TD>
<TD NOSAVE>a device name or number to include with communications with the
media changer.</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>1</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>do_eject</TD>
<TD NOSAVE>used when testing stand alone tape drive (no robot).</TD>
<TD NOSAVE>'yes'</TD>
<TD NOSAVE>'no'</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>driver</TD>
<TD NOSAVE>the hsm driver</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>'FTTDriver'</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>device</TD>
<TD NOSAVE>device name used as parameter to certain driver function calls.</TD>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>'/dev/rmt/tps2d2n' (make sure this is a no-rewind device)</TD>
</TR>
</TABLE></center>


<p>
<hr>
<h3><a name="config_server">
1.3.5 Configuration Server
</h3>
The Configuration Server maintains the crucial information about
system configuration, such as the location and parameters of each server.
Upon startup, each server asks the Configuration Server for the information 
pertaining to itself (e.g. the location of any other server with which to 
communicate).  New configurations can be
loaded into the Configuration Server without disturbing the current running
system. Configurations are stored in a file called the Enstore configuration
file in Python dictionary format.  An example of this file is given below:<br>
<p>
<pre>
configdict['blocksizes'] = { 'diskfile'  : 512, \
                             'redwood'   : 131072, \
                             'floppy'    : 512, \
                             'cassette'  : 512, \
                             'cartridge' : 512, \
                             'exabyte'   : 131072, \
                             '8MM'       : 131072, \
                             'DECDLT'    : 131072 }

configdict['file_clerk']   = { 'host':'rip6', 'port':7501, 'logname':'FILSRV' }
configdict['volume_clerk'] = { 'host':'rip6', 'port':7502, 'logname':'VOLSRV' }
configdict['admin_clerk']  = { 'host':'rip6', 'port':7503, 'logname':'ADMSRV' }
configdict['logserver']    = { 'host':'rip6', 'port':7504, \
                               'log_file_path':'/rip6a/enstore/log' }
configdict['database']     = { 'db_dir':'/rip6a/enstore/db' }
configdict['backup']       = { 'host':'rip6', 'dir':'/rip6a/enstore/db_backup'}

configdict['inquisitor']   = { 'host':'rip6', 'port':7505, 'logname':'INQSRV', \
                               'timeout':10, 'alive_rcv_timeout': 5, \
                               'alive_retries':1, \
                               'ascii_file':'/rip6a/enstore/inquisitor/', \
                               'html_file':'/fnal/ups/prd/www_pages/enstore/', \
                               'default_server_timeout': 15, \
                               'timeouts' : { 'ait.library_manager': 15} }

configdict['rip6.library_manager']  = { 'host':'rip5', 'port':7506, \
                                        'logname':'RP6LBM' }
configdict['dlt.library_manager']   = { 'host':'rip5', 'port':7509, \
                                        'logname':'DLTLBM' }
configdict['rip6.media_changer']    = { 'host':'rip6',  'port':7512, \
                                        'logname':'R6MC  ', \
                                        'type':'RDD_MediaLoader'  }
configdict['de13.media_changer']    = { 'host':'rip10', 'port':7517, \
                                        'logname':'DE13MC', \
                                        'type':'EMASS_MediaLoader' }
configdict['rip6.mover']    = { 'host':'rip6', 'port':7525, 'logname':'R6MOV ', \
                                'library':'rip6.library_manager', \
                                'device':'/rip6a/rip6/rip6.fake', \
                                'driver':'RawDiskDriver', \
                                'mc_device':'-1', \
                                'media_changer':'rip6.media_changer' }
configdict['DE13DLT.mover'] = { 'host':'rip1', 'port':7526, 'logname':'DE13MV', \
                                'library':'dlt.library_manager', \
                                'device':'/dev/rmt/tps2d1n', \
                                'driver':'FTTDriver', \
                                'mc_device':'DE13', \
                                'media_changer':'de13.media_changer' }
</pre>
<p>
The keys/values used in the above example are typical of a running system.
The <i>blocksizes</i> dictionary element specifies the size of a block on the
different devices known to the system. The <i>database</i> dictionary element
specifies where the Enstore database files are located.  The <i>backup</i>
dictionary element specifies the node and directory of where the database backups
will go.
<p>
Please see the individual server sections for more in depth descriptions of
all the server keywords.
<h4><a name="config_server_cmds">
1.3.5.1 Command Line Control of the Configuration Server
</h4>
Configuration Server functionality may be controlled through a command line 
interface using
<B>ecmd</B>.  A summary of the supported commands is given below.  In addition
to the following commands, the Configuration Server command line interface 
supports the
general commands supported by all other servers; <i>alive</i>,
<i>config_host</i>, <i>config_port</i>, <i>help</i>, <i>server_verbose</i>,
<i>verbose</i>, <i>alive_rcv_timeout</i> <i>alive_retries</i>.
<P>
<center><TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>FUNCTION</B></TD>
<TD NOSAVE><B>COMMAND</B></TD>
<TD NOSAVE><B>OUTPUT</B></TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>load the specified Enstore config file into the configuration server</TD>
<TD NOSAVE>ecmd config --config_file=/path/to/config_file --load</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>output the currently loaded Enstore configuration file</TD>
<TD NOSAVE>ecmd config --dict</TD>
<TD NOSAVE>(same as the example in the previous section)</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>output the keys in the currently loaded Enstore configuration file</TD>
<TD NOSAVE>ecmd config --get_keys</TD>
<TD NOSAVE><pre>
['DE13DLT.mover',
 'admin_clerk',
 'backup',
 'blocksizes',
 'database',
 'de13.media_changer',
 'dlt.library_manager',
 'file_clerk',
 'inquisitor',
 'logserver',
 'rip6.library_manager',
 'rip6.media_changer',
 'rip6.mover',
 'volume_clerk']
</pre>
</TD>
</TR>
</TABLE></center>
<P>

<hr>
<h3><a name="log_server">
1.3.6 Log Server
</h3>
The Log Server receives messages from other processes and logs them into
formatted log files.
Basically, these messages are transactional records.
Log files are labeled by dates.
At midnight each day, the currently opened log file gets closed and another
one is opened. Below is excerpt from the log file:
<pre>
10:03:42 sphinx.fnal.gov 006849 moibenko I FILC  File Clerk (re)starting
10:03:46 sphinx.fnal.gov 006849 moibenko I HLIBM  Library Manager sphinxdisk.lib
rary_manager(re)starting
10:03:50 sphinx.fnal.gov 006849 moibenko I HMC  Media Changersphinxdisk.media_ch
anger(re) starting
10:03:55 sphinx.fnal.gov 006849 moibenko I HMOV  Mover starting - contacting lib
man
10:03:59 sphinx.fnal.gov 006849 moibenko I ADMC  Admin Clerk (re)starting
10:09:34 sphinx.fnal.gov 006849 moibenko I HLIBM  read Q'd /pnfs/enstore/sphinx/
t1/mover.py -> '/usr/hppc_home/moibenko/enstore_test/enstore/src/tst/mover.py' :
 vol=flop1 bfid=91546995800000L requester:moibenko
10:09:34 sphinx.fnal.gov 006849 moibenko I HLIBM  read_from_hsm work on vol=flop1
mover=sphinxdisk.mover requester:moibenko
10:09:34 sphinx.fnal.gov 006849 moibenko I HMOV  READ_FROM_HSM start{'times': {'
t0': 915725367.144, 'in_queue': 0.0595669746399}, ......
.....

10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  Performing precautionary offlin
e/eject of device./sphinxdisk.fake
10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  Completed  precautionary offlin
e/eject of device./sphinxdisk.fake
10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  Requesting media changer load {
'first_access': 915469958.425, 'sum_rd_err': 0, 'last_access': 915488183.702, 'm
edia_type': 'diskfile', 'capacity_bytes': 1400000L, 'blocksize': 512, 'library':
 'sphinxdisk', 'file_family': 'sphinx', 'sum_wr_access': 0, 'sum_rd_access': 0, 
'remaining_bytes': 1291968L, 'wrapper': 'cpio', 'sum_wr_err': 0, 'external_label
': 'flop1'.....
......
10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  Media changer load status('ok',
 None)
10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  Requesting software mount flop1
 ./sphinxdisk.fake
10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  Software mount complete flop1 .
/sphinxdisk.fake
10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  WRAPPER.READ
........
10:09:35 sphinx.fnal.gov 006849 moibenko I HMOV  READ DONE{'unique_id': 'sphinx.
fnal.gov-915725374.048321-1071', 'retry_cnt': 0, 'mover': {'config_port': 7500, 
'media_changer': 'sphinxdisk.media_changer', 'port': 7508, 'device': './sphinxdi
</pre>
Fields in a log file are:
<ul>
<li>time
<li>node name
<li>user id
<li>user name
<li>severety indicator (I - information, E - Error)
<li>client abbreviation
<li>arbitrary message
</ul>
<hr>
<h3><a name="media_changer">
1.3.7 Media Changer
</h3>

The Media Changer mounts and dismounts the media into and from the drive according to a request from
the Mover. One Media Changer can serve multiple drives and libraries. When the drives are in the robot, the
Media Changer is the interface to the robotic software. 
<p>
The Media Changer issues multiple simultaneous commands by forking processes
that do the work.  A media changer parameter, MAXWORK, limits the maximum number of
simultaneous outstanding operations.   If the media changer receives
mount/dismount requests while there are  MAXWORK unfinished operations then
the new operations are ignored, the mover request will time out, and the
mover will reissue the mount/dismount request.
<p>
The reason for the MAXWORK parameter is because when the EMASS robot has an operation for ten minutes
it reports a timeout failure even though it eventually finishes the operation.  The MAXWORK parameter
can be set to 0 when it is necessary to perform work on a robot.
<p>
The media changer returns three status values:
<ul>
<li>The status returned by the underlying agent
<li>A translation of the underlying status with the values, SUCCESS, DRIVE_ERR, MEDIA_ERR, GNEREAL_ERR
<li>A text description returned by the underlying agent
</ul>
<p>
The media changer supports the following requests:
<ul>
<li>LOAD &ltvolume id&gt &ltdrive id&gt &ltmedia type&gt
<li>UNLOAD &ltvolume id&gt &ltdrive id&gt &ltmedia type&gt
<li>MAXWORK &ltmmax simultaneous operation&gt 
</ul>
<p>
The media changer mounting agents:
<ul>
<li>EMASS/Grau robot
<li>STK robot
<li>null media changer used by the disk movers and stand alone tape drives
<li>OCS operator assisted mounts - to be implemented
</ul>
<h3>
Tape Cleaning
</h3>
The media changer is not involved with tape cleaning.  The EMASS AMU and the
STK ACSLS tape library systems keep tape drive usage statistics and automatically
mount cleaning tapes.
<h3>
Tape statistics
</h3>
The media changer does not keep tape drive or cartridge statistics.  Summary statistics
are not very useful and the media does not run on the machine connected to the tape drive.
<p>
Enstore writes detail error statistics to its log when a file is closed.



<hr>
<h3><a name="inquisitor">
1.3.8 Inquisitor
</h3>
The Inquisitor obtains information from the Enstore system and creates the 
following reports using this information:<BR>
<UL>
<LI>an <a href="#inquisitor_ascii_status">ascii file</a> continuously recording the Enstore system status
<LI>an <a href="#inquisitor_html_status">html file</a> recording the latest status snapshot of the Enstore system
<LI>an <a href="#inquisitor_encp_status">html file</a> recording a snapshot of a history of <em>encp</em> commands 
<LI>a <a href="#inquisitor_plot_ita">plot of individual transfer activity</a>
<LI>a <a href="#inquisitor_plot_btd">plot of total bytes transferred/day</a>
</UL>

The reports are updated periodically based on 
<a href="#inquisitor_config">timeout values</a> in the Enstore
config file directing the Inquisitor to gather
each servers' information on a specific time frequency.  Each Enstore server may
have its own unique timeout value specified for it.  For example, the Inquisitor
may be instructed to gather information from the file_clerk every 60 seconds but
from the logserver every 135 seconds.  However the plots are not updated
automatically and may be updated by a user initiated command or by a cron job
for example.  The information for plotting is obtained from the log files.
<p>
The inquisitor will listen for command
line requests sent to it and will periodically check to see if it is time to
update information for any of the servers that it is monitoring.  If so, then the
server in question is contacted and the resulting information is formated for
output to the various reports. The possible information gathered from each of the
servers and which report it ends up in are listed below.  In addition to
gathering information from each Enstore server, the Inquisitor will collect
information from the log files on <em>encp</em> commands and report on the blocksizes
set in the Enstore config file.

<P>
<center><TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>SERVER</B></TD>
<TD NOSAVE><B>INFORMATION GATHERED</B></TD>
<TD NOSAVE><B>REPORTS EFFECTED</B></TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE>blocksizes</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>&nbsp;</TD>
<TD NOSAVE><em>encp</em> command history</TD>
<TD NOSAVE>continuous Ascii status file and<br><em>encp</em> html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>admin_clerk</TD>
<TD NOSAVE>alive status</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>config_server</TD>
<TD NOSAVE>alive status</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>file_clerk</TD>
<TD NOSAVE>alive status</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>inquisitor</TD>
<TD NOSAVE>alive status<br>refetch config file from config server</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>library manager(s)</TD>
<TD NOSAVE>alive status<br>suspect volume list<br>mover list<br>work queues</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>logserver</TD>
<TD NOSAVE>alive_status</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>media changer(s)</TD>
<TD NOSAVE>alive status</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>mover(s)</TD>
<TD NOSAVE>alive status<br>mover activity status</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>volume_clerk</TD>
<TD NOSAVE>alive status</TD>
<TD NOSAVE>continuous Ascii status file and<br>html snapshot file</TD>
</TR>
</TABLE></center>
<P>

Since the Inquisitor can request a new config file from the config_server 
periodically, it is possible to dynamically change the way information is
displayed and the type of information that is displayed without restarting the
Inquisitor.

<h4><a name="inquisitor_cmds">
1.3.8.1 Command Line Control of the Inquisitor 
</h4>
Inquisitor functionality may be controlled through a command line interface using
<B>ecmd</B>.  A summary of the supported commands is given below.  In addition
to the following commands, the Inquisitor command line interface supports the
general commands supported by all other servers; <i>alive</i>,
<i>config_host</i>, <i>config_port</i>, <i>help</i>, <i>server_verbose</i>,
<i>verbose</i>, <i>alive_rcv_timeout</i> <i>alive_retries</i>.
<P>
<center><TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>FUNCTION</B></TD>
<TD NOSAVE><B>COMMAND</B></TD>
<TD NOSAVE><B>OUTPUT</B></TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the maximum size of the ascii status file</TD>
<TD NOSAVE>ecmd inq --get_max_ascii_size</TD>
<TD NOSAVE>maximum ascii size</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the maximum number of <em>encp</em> status lines displayed</TD>
<TD NOSAVE>ecmd inq --get_max_encp_lines</TD>
<TD NOSAVE>maximum number of <em>encp</em> lines</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the html status file auto refresh rate</TD>
<TD NOSAVE>ecmd inq --get_refresh</TD>
<TD NOSAVE>refresh time</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the frequency for monitoring the volume clerk</TD>
<TD NOSAVE>ecmd inq --get_timeout volume_clerk</TD>
<TD NOSAVE>volume_clerk timeout value</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>get the frequency for looking for work</TD>
<TD NOSAVE>ecmd inq --get_timeout</TD>
<TD NOSAVE>Inquisitor wakeup time</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the maximum size of the ascii status file</TD>
<TD NOSAVE>ecmd inq --max_ascii_size=40000</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the maximum number of <em>encp</em> status lines displayed</TD>
<TD NOSAVE>ecmd inq --max_encp_lines=13</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots</TD>
<TD NOSAVE>ecmd inq --plot</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and use the log files located in the specified directory</TD>
<TD NOSAVE>ecmd inq --plot --logfile_dir=/tmp/logs</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and only plot information after the specified start_time</TD>
<TD NOSAVE>ecmd inq --plot --start_time=1998-12-25</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and only plot information before the specified stop_time</TD>
<TD NOSAVE>ecmd inq --plot --stop_time=1998-12-31</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>recreate the Inquisitor plots and only plot information between the specified times</TD>
<TD NOSAVE>ecmd inq --plot --start_time=1998-12-01 --stop_time=1998-12-31</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the html status file auto refresh rate</TD>
<TD NOSAVE>ecmd inq --refresh=60</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for monitoring the admin_clerk to the value in the config file</TD>
<TD NOSAVE>ecmd inq --reset_timeout admin_clerk</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for looking for work to the value in the config file</TD>
<TD NOSAVE>ecmd inq --reset_timeout</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for monitoring the file_clerk</TD>
<TD NOSAVE>ecmd inq --timeout=55 file_clerk</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>reset the frequency for looking for work</TD>
<TD NOSAVE>ecmd inq --timeout=10</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>close the current ascii status file and open a new one</TD>
<TD NOSAVE>ecmd inq --timestamp</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>monitor the log server now</TD>
<TD NOSAVE>ecmd inq --update logserver</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>monitor all the servers now</TD>
<TD NOSAVE>ecmd inq --update</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
</TABLE></center>

<h4><a name="inquisitor_config">
1.3.8.2 Inquisitor Config File Values
</h4>
The Inquisitor looks for the following values in the Inquisitor section of the
Enstore config file. The default value is used if the dictionary element is not
found. Dictionary elements with no default must be specified in the Enstore 
config file.  All frequencies are specified in seconds.<BR>
<P>
<center><TABLE BORDER COLS=3 WIDTH="100%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>DICTIONARY ELEMENT</B></TD>
<TD NOSAVE><B>DEFINITION</B></TD>
<TD NOSAVE><B>DEFAULT</B></TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>alive_rcv_timeout</TD>
<TD NOSAVE>seconds to wait for response to alive request</TD>
<TD NOSAVE>5</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>alive_retries</TD>
<TD NOSAVE>times to retry alive request</TD>
<TD NOSAVE>2</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>ascii_file</TD>
<TD NOSAVE>directory for ascii status file(s)</TD>
<TD NOSAVE>./</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>default_server_timeout</TD>
<TD NOSAVE>frequency to monitor servers not listed in <i>timeouts</i></TD>
<TD NOSAVE>60</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>host</TD>
<TD NOSAVE>node where inquisitor runs</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>html_file</TD>
<TD NOSAVE>directory for html status files</TD>
<TD NOSAVE>./</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>logname</TD>
<TD NOSAVE>ascii value used for id in messages to log server</TD>
<TD NOSAVE>INQS</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>max_ascii_size</TD>
<TD NOSAVE>maximum allowed size (bytes) of ascii status file</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>max_encp_lines</TD>
<TD NOSAVE>maximum number of <em>encp</em> lines to display</TD>
<TD NOSAVE>50</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>port</TD>
<TD NOSAVE>udp port for inquisitor communication</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>refresh</TD>
<TD NOSAVE>frequency for auto-refresh of html status page</TD>
<TD NOSAVE>120</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>timeout</TD>
<TD NOSAVE>frequency that inquisitor looks for work</TD>
<TD NOSAVE>5</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>timeouts</TD>
<TD NOSAVE>dictionary of frequencies for monitoring each server</TD>
<TD NOSAVE>&nbsp;</TD>
</TR>
</TABLE></center>
<P>
In addition to the information listed above, the inquisitor will look for the
<i>inq_timeout</i> dictionary element in each of the individual server sections.
If present, the value of this dictionary element will be used to specify the 
timeout value for monitoring this server.  This is the same as if the 
<i>timeouts</i>  dictionary element mentioned
above contained a dictionary element for the particular server.  For example, 
in order to monitor the file_clerk every 65 seconds, the Enstore config file 
must have one of the following in it:<BR>

<UL>
<LI>in the inquisitor section - a dictionary element, within the <i>timeouts</i>
dictionary element, for the file clerk set to 65
<LI>in the file_clerk section - the dictionary element <i>inq_timeout</i> set to
65
</UL>
The value in the individual server dictionary element will take precedence over
the value in the Inquisitor dictionary element.
<P>
In order to block monitoring of a particular server, set it's timeout value to 
-1.
<P>
An example Inquisitor dictionary element is given below:<BR>
<P>
<PRE>
configdict['inquisitor']   = { 'host':'rip6',          \
                               'port':7505,            \
                               'logname':'INQSRV',     \
                               'timeout':10,           \
                               'alive_rcv_timeout': 5, \
                               'alive_retries':1,      \
                               'ascii_file':'/rip6a/enstore/inquisitor/', \
                               'html_file':'/fnal/ups/prd/www_pages/enstore/', \
                               'default_server_timeout': 15, \
                               'timeouts' : { 'ait.library_manager': 15} }
</PRE>

<h4><a name="inquisitor_outputs">
1.3.8.3 Example Inquisitor Reports
</h4>
These examples reflect a running system on the <i>rip</i> cluster.
<h4><a name="inquisitor_ascii_status">
1.3.8.3.1 Example Ascii Status File
</h4>
This file records a continuous history of the status of the Enstore system as
monitored by the Inquisitor.  It contains the following information:<br>
<ul>
<li>block size information as recorded in the Enstore config file
<li>alive status for each server including node, port, and time
<li>library manager specific information
<ul>
<li>suspect volumes
<li>list of known movers, their ports, state, last time they were summoned and
number of attempts to summon them
<li>work queue including:
<ul>
<li>assigned mover
<li>node, node type, and port where mover is located
<li>work that mover is doing
<li>device label
<li>file family and file family width
<li>priorities of the work
<li>associated times
</ul>
<li>pending work queue including:
<ul>
<li>node, node type, and port where work originated
<li>work to be done
<li>file family and file family width
<li>priorities of the work
<li>associated times
</ul>
</ul>
<li>mover specific information
<ul>
<li>number of completed transfers
<li>current state of the mover
<li>number of bytes read and written on the last transfer (if idle)
<li>number of bytes read and written so far on the current transfer (if working)
</ul>
</ul>
<pre>
ENSTORE SYSTEM STATUS
DC12MAM.mover : alive on (rip2, 7535) at 1999-Jan-7 00:00:27

    Completed Transfers : 0,  Current State : idle 
    Last Transfer :  Read 0 bytes,  Wrote 0 bytes

DE13DLT.mover : alive on (rip1, 7526) at 1999-Jan-7 00:00:27

    Completed Transfers : 0,  Current State : idle 
    Last Transfer :  Read 0 bytes,  Wrote 0 bytes

DE14DLT.mover : alive on (rip1, 7527) at 1999-Jan-7 00:00:27

    Completed Transfers : 0,  Current State : idle 
    Last Transfer :  Read 0 bytes,  Wrote 0 bytes

DE15DLT.mover : alive on (rip1, 7528) at 1999-Jan-7 00:00:27

    Completed Transfers : 0,  Current State : idle 
    Last Transfer :  Read 0 bytes,  Wrote 0 bytes

DE16DLT.mover : alive on (rip1, 7529) at 1999-Jan-7 00:00:27

    Completed Transfers : 0,  Current State : idle 
    Last Transfer :  Read 0 bytes,  Wrote 0 bytes

DM05AIT.mover : alive on (rip2, 7530) at 1999-Jan-7 00:00:27

    Completed Transfers : 65,  Current State : busy writing 1073741824 bytes to the HSM
    Current Transfer :  Read 1073741824 bytes,  Wrote 1070792703 bytes

DM06AIT.mover : alive on (rip2, 7531) at 1999-Jan-7 00:00:27

    Completed Transfers : 59,  Current State : busy writing 1073741824 bytes to the HSM
    Current Transfer :  Read 0 bytes,  Wrote 0 bytes

DM07AIT.mover : alive on (rip2, 7532) at 1999-Jan-7 00:00:27

    Completed Transfers : 51,  Current State : busy writing 1073741824 bytes to the HSM
    Current Transfer :  Read 171179860 bytes,  Wrote 167182335 bytes

DM08AIT.mover : timed out on (rip2, 7533) at 1999-Jan-7 00:00:27
admin clerk     : alive on (rip6, 7503) at 1999-Jan-7 00:00:27
ait.library_manager : alive on (rip5, 7507) at 1999-Jan-7 00:00:27

    SUSPECT VOLUMES : NONE

    KNOWN MOVER           PORT    STATE         LAST SUMMONED        TRY COUNT
    DM05AIT.mover         7530    work_at_mo    1999-Jan-6 23:52:31     0  
    DM08AIT.mover         7533    work_at_mo    1999-Jan-6 21:26:51     0  
    DM07AIT.mover         7532    work_at_mo    1999-Jan-6 23:58:01     0  
    DM06AIT.mover         7531    work_at_mo    1999-Jan-6 23:59:38     0  

    Work for: DM08AIT.mover, from NODE: rip8.fnal.gov (Linux),  PORT: 7602
          WRITE to: /pnfs/grau/ait/jon1/100MB.trand__Jan06212546rip8.fnal.gov25511,  BYTES: 104857600
          DEVICE LABEL: CA2901,  FILE FAMILY: jon-ait-1,  FILE FAMILY WIDTH: 2
          PRIORITIES:  CURRENT 1,  BASE 1,  DELTA 0  and  AGETIME: 0
          JOB SUBMITTED: 1999-Jan-6 21:25:48
          FILE MODIFIED: 1999-Jan-6 21:25:49

          DM05AIT.mover, from NODE: rip8.fnal.gov (Linux),  PORT: 7600
          WRITE to: /pnfs/grau/ait/jon1/1GB.trand__Jan06235102rip8.fnal.gov25520,  BYTES: 1073741824
          DEVICE LABEL: CA2905,  FILE FAMILY: jon-ait-1,  FILE FAMILY WIDTH: 2
          PRIORITIES:  CURRENT 1,  BASE 1,  DELTA 0  and  AGETIME: 0
          JOB SUBMITTED: 1999-Jan-6 23:51:04
          FILE MODIFIED: 1999-Jan-6 23:51:04

          DM07AIT.mover, from NODE: rip8.fnal.gov (Linux),  PORT: 7601
          WRITE to: /pnfs/grau/ait/jon2/1GB.trand__Jan06235709rip8.fnal.gov25550,  BYTES: 1073741824
          DEVICE LABEL: CA2907,  FILE FAMILY: jon-ait-2,  FILE FAMILY WIDTH: 2
          PRIORITIES:  CURRENT 1,  BASE 1,  DELTA 0  and  AGETIME: 0
          JOB SUBMITTED: 1999-Jan-6 23:57:11
          FILE MODIFIED: 1999-Jan-6 23:57:11

          DM06AIT.mover, from NODE: rip4.fnal.gov (Linux),  PORT: 7600
          WRITE to: /pnfs/grau/ait/jon2/1GB.trand__Jan06235846rip4.fnal.gov17340,  BYTES: 1073741824
          DEVICE LABEL: CA2906,  FILE FAMILY: jon-ait-2,  FILE FAMILY WIDTH: 2
          PRIORITIES:  CURRENT 1,  BASE 1,  DELTA 0  and  AGETIME: 0
          JOB SUBMITTED: 1999-Jan-6 23:58:48
          FILE MODIFIED: 1999-Jan-6 23:58:48


    Pending work: from NODE: rip8.fnal.gov (Linux),  PORT: 7602
          WRITE to: /pnfs/grau/ait/jon1/1GB.trand__Jan06235128rip8.fnal.gov25511,  BYTES: 1073741824
          FILE FAMILY: jon-ait-1,  FILE FAMILY WIDTH: 2
          PRIORITIES:  CURRENT 1,  BASE 1,  DELTA 0  and  AGETIME: 0
          JOB SUBMITTED: 1999-Jan-6 23:51:29
          FILE MODIFIED: 1999-Jan-6 23:51:30

backup : NOT SUPPORTED IN INQUISITOR
blocksizes      : diskfile : 512,  redwood : 131072,  DECDLT : 131072,
                  floppy : 512,  cartridge : 512,  exabyte : 131072,
                  cassette : 512,  8MM : 131072
config server   : alive on (rip6.fnal.gov, 7500) at 1999-Jan-7 00:00:27
database : NOT SUPPORTED IN INQUISITOR
dc11.media_changer : alive on (rip1, 7521) at 1999-Jan-7 00:00:27
dc12.media_changer : alive on (rip1, 7522) at 1999-Jan-7 00:00:27
de13.media_changer : alive on (rip10, 7517) at 1999-Jan-7 00:00:27
de14.media_changer : alive on (rip10, 7518) at 1999-Jan-7 00:00:27
de15.media_changer : alive on (rip10, 7519) at 1999-Jan-7 00:00:27
de16.media_changer : alive on (rip10, 7520) at 1999-Jan-7 00:00:27
dlt.library_manager : alive on (rip5, 7509) at 1999-Jan-7 00:00:27

    SUSPECT VOLUMES : NONE

    KNOWN MOVER           PORT    STATE         LAST SUMMONED        TRY COUNT
    DE13DLT.mover         7526    idle_mover    1999-Jan-6 18:01:14     0  
    DE15DLT.mover         7528    idle_mover    1999-Jan-6 18:01:16     0  
    DE16DLT.mover         7529    idle_mover    1999-Jan-6 18:01:17     0  
    DE14DLT.mover         7527    idle_mover    1999-Jan-6 18:01:19     0  

    No work at movers
    No pending work

dm05.media_changer : alive on (rip1, 7513) at 1999-Jan-7 00:00:27
dm06.media_changer : alive on (rip1, 7514) at 1999-Jan-7 00:00:27
dm07.media_changer : alive on (rip1, 7515) at 1999-Jan-7 00:00:27
dm08.media_changer : alive on (rip1, 7516) at 1999-Jan-7 00:00:27
encp            : 23:58:16 on rip4.fnal.gov by bakken (Data Transfer Rate : 2.4 MB/S)
                     104857600 bytes copied to CA2906 at a user rate of 0.509 MB/S
                  23:56:39 on rip8.fnal.gov by bakken (Data Transfer Rate : 2.4 MB/S)
                     104857600 bytes copied to CA2907 at a user rate of 0.511 MB/S
                  23:54:27 on rip4.fnal.gov by bakken (Data Transfer Rate : 2.7 MB/S)
                     1073741824 bytes copied to CA2906 at a user rate of 1.93 MB/S
                  23:52:51 on rip8.fnal.gov by bakken (Data Transfer Rate : 2.7 MB/S)
                     1073741824 bytes copied to CA2907 at a user rate of 1.95 MB/S
                  23:50:58 on rip8.fnal.gov by bakken (Data Transfer Rate : 1.35 MB/S)
                     1048576 bytes copied to CA2905 at a user rate of 0.0183 MB/S
                  23:50:32 on rip8.fnal.gov by bakken (Data Transfer Rate : 2.41 MB/S)
                     104857600 bytes copied to CA2905 at a user rate of 0.233 MB/S
                  23:49:30 on rip8.fnal.gov by bakken (Data Transfer Rate : 2.7 MB/S)
                     1073741824 bytes copied to CA2905 at a user rate of 1.08 MB/S
                  23:45:04 on rip4.fnal.gov by bakken (Data Transfer Rate : 2.39 MB/S)
                     104857600 bytes copied to CA2906 at a user rate of 0.527 MB/S
                  23:43:32 on rip8.fnal.gov by bakken (Data Transfer Rate : 2.4 MB/S)
                     104857600 bytes copied to CA2907 at a user rate of 0.529 MB/S

file clerk      : alive on (rip6, 7501) at 1999-Jan-7 00:00:27
inquisitor      : alive on (rip6, 7505) at 1999-Jan-7 00:00:27
log server      : alive on (rip6, 7504) at 1999-Jan-7 00:00:27
mam.library_manager : alive on (rip5, 7508) at 1999-Jan-7 00:00:27

    SUSPECT VOLUMES : NONE

    KNOWN MOVER           PORT    STATE         LAST SUMMONED        TRY COUNT
    DC12MAM.mover         7535    idle_mover    1999-Jan-6 18:01:05     0  

    No work at movers
    No pending work

rip6.library_manager : alive on (rip5, 7506) at 1999-Jan-7 00:00:27

    SUSPECT VOLUMES : NONE

    KNOWN MOVER           PORT    STATE         LAST SUMMONED        TRY COUNT
    rip6.mover            7525    idle_mover    1999-Jan-6 18:00:50     0  

    No work at movers
    No pending work

rip6.media_changer : alive on (rip6, 7512) at 1999-Jan-7 00:00:27
rip6.mover : alive on (rip6, 7525) at 1999-Jan-7 00:00:27

    Completed Transfers : 0,  Current State : idle 
    Last Transfer :  Read 0 bytes,  Wrote 0 bytes

volume clerk    : alive on (rip6, 7502) at 1999-Jan-7 00:00:27
</pre>

<h4><a name="inquisitor_html_status">
1.3.8.3.2 Example Html Status Snapshot File
</h4>
The html snapshot file contains the last know status of the Enstore system. 
As such it will be a repeat of the last set of information in the Ascii status
file formatted for browsing minus the <i>encp</i> information.

<h4><a name="inquisitor_encp_status">
1.3.8.3.3 Example <em>encp</em> History Snapshot File
</h4>
Each <em>encp</em> history line contains the following information
<ul>
<li>end of transfer time
<li>node of <em>encp</em> process
<li>user tunning <em>encp</em>
<li>number of bytes transferred
<li>volume name
<li>data transfer rate (MB/s)
<li>user rate of transfer
</ul>
<p>
<title>Enstore Status</title>
<pre>

ENSTORE SYSTEM STATUS
</pre><P>
<CENTER><TABLE BORDER COLS=7 WIDTH="100%" NOSAVE>
<TH COLSPAN=7 VALIGN=CENTER>History of ENCP Commands</TH>
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>TIME</B></TD>
<TD NOSAVE><B>NODE</B></TD>
<TD NOSAVE><B>USER</B></TD>
<TD NOSAVE><B>BYTES</B></TD>
<TD NOSAVE><B>VOLUME</B></TD>
<TD NOSAVE><B>DATA TRANSFER RATE (MB/S)</B></TD>
<TD NOSAVE><B>USER RATE (MB/S)</B></TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>15:19:22</TD>
<TD NOSAVE>rip8.fnal.gov</TD>
<TD NOSAVE>moibenko</TD>
<TD NOSAVE>21036</TD>
<TD NOSAVE>rip6-01</TD>
<TD NOSAVE>2.47</TD>
<TD NOSAVE>0.04</TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>15:18:59</TD>
<TD NOSAVE>rip8.fnal.gov</TD>
<TD NOSAVE>moibenko</TD>
<TD NOSAVE>21036</TD>
<TD NOSAVE>rip6-01</TD>
<TD NOSAVE>0.664</TD>
<TD NOSAVE>0.0322</TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>12:57:19</TD>
<TD NOSAVE>rip4.fnal.gov</TD>
<TD NOSAVE>bakken</TD>
<TD NOSAVE>1048576</TD>
<TD NOSAVE>CA2904</TD>
<TD NOSAVE>0.698</TD>
<TD NOSAVE>0.00589</TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>12:57:04</TD>
<TD NOSAVE>rip8.fnal.gov</TD>
<TD NOSAVE>bakken</TD>
<TD NOSAVE>1048576</TD>
<TD NOSAVE>CA2903</TD>
<TD NOSAVE>0.703</TD>
<TD NOSAVE>0.00589</TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>12:53:57</TD>
<TD NOSAVE>rip4.fnal.gov</TD>
<TD NOSAVE>bakken</TD>
<TD NOSAVE>1073741824</TD>
<TD NOSAVE>CA2905</TD>
<TD NOSAVE>2.7</TD>
<TD NOSAVE>2.06</TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>12:53:41</TD>
<TD NOSAVE>rip8.fnal.gov</TD>
<TD NOSAVE>bakken</TD>
<TD NOSAVE>1073741824</TD>
<TD NOSAVE>CA2903</TD>
<TD NOSAVE>2.7</TD>
<TD NOSAVE>1.88</TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>12:52:31</TD>
<TD NOSAVE>rip8.fnal.gov</TD>
<TD NOSAVE>bakken</TD>
<TD NOSAVE>1048576</TD>
<TD NOSAVE>CA2904</TD>
<TD NOSAVE>0.711</TD>
<TD NOSAVE>0.0058</TD>
</TR>
<TR NOSAVE>
<TD NOSAVE>12:49:51</TD>
<TD NOSAVE>rip8.fnal.gov</TD>
<TD NOSAVE>bakken</TD>
<TD NOSAVE>104857600</TD>
<TD NOSAVE>CA2902</TD>
<TD NOSAVE>2.38</TD>
<TD NOSAVE>0.496</TD>
</TR>
</TABLE></CENTER>

<h4><a name="inquisitor_plot_ita">
1.3.8.3.4 Example Individual Transfer Activity Plot
</h4>
This plot shows the history of individual transfers (and their size) over a 
specified time interval.  This includes both reads and writes.
<p>
<center><img src="bpt.pts.gif"></center>
<p>
<div align=right><a href="bpt.pts.ps">(also available in Postscript)</a><div align=left>
<p>
<h4><a name="inquisitor_plot_btd">
1.3.8.3.5 Example Bytes Transferred/Day Plot
</h4>
This plot shows the number of bytes transferred per day over a specified time
interval.  This includes both reads and writes.
<p>
<center><img src="bytes.pts.gif"></center>
<p>
<div align=right><a href="bytes.pts.ps">(also available in Postscript)</a><div align=left>
<p>

<hr>
<h3><a name="alarm_server">
1.3.9 Alarm Server
</h3>
*** Plans are to create an Alarm Server which will monitor the information
gathered by the Inquisitor and react to error conditions.  This includes using
existing tools like Patrol to help recognise and recover from errors. Patrol was
developed at SLAC and enhanced and modified at DESY.  It is currently in use at
these institutions and at Fermilab.  We have begun investigating its use in
association with the Enstore system. ***
<p>
<center><img src="patrol-status.gif"></center>
<p>


<hr>
<h2><a name="server_protocols">
1.4 Server Protocols
</h2>
<p>
Communications between clients and servers is implemented in the python modules udp_client.py
and dispatching_worker.py which contain the classes UDPclient and DispatchingWorker respectively.
For example, a mover is a client of the media_changer; i.e., it sends mount and dismount requests
to the media_changer and waits for replies.  The client and the server may run on the same or
different machines and messages, that is requests and replies, are passed using the UDP network protocol.
UDP is not a guaranteed reliable protocol but the Enstore protocols, described later, implement reliability.

<p>
Generally, each server module has a corresponding client module that implements the client interface
to the server.   For the media_changer, the the mover imports media_change_client.py n which 
implements load and unload methods.
So,  mover.py imports media_changer_client which encapsulates the media_changer interface and media_changer_client
imports udp_client which encapsulates the UDP communications.   On the server side, media changer
imports dispatching_worker which encapsulates server UDP implementation.
So far, we have mentioned modules that are imported with the python "import" command.  Within the
modules, there are python "new" commands that instantiate the corresponding classes.

<p>
All clients are themselves clients of the configuration_server server; so, each time they send
a request to their server, they send a request to the configuration server to get the address
of their server.   In this way the configuration server is the only server that has a hard coded address.
When each process starts it is given the IP address and port number of its configuration server.
<p>
When a client is instantiated it determines a free UDP port on its machine on which it sends requests
to its server.
When the server reads a request it also gets the address (host, port) of the client which sent the request
and uses it to reply.
<p>
Client requests are called tickets and they are python dictionaries.  The items in the dictionary
are agreed upon between the client and the server.   For example the media changer ticket
must contain a volume id and a drive id.
<p>
One item required in the ticket dictionary is "work".  The "work" item in the dictionary
is used in dispatching_worker as a method name and a corresponding method in the server
is called to perform the work that the client requests.   For example, the media changer
ticket must contain a "work" item with a value "load"  or "unload" and the media changer
server has methods named load and unload.
<p>
When UDPClient sends a request it first prepends a client identification stamp and a request time stamp (which
serve as a unique identification) to the
ticket and stringifies the result.  Then it calculates a CRC of the message and appends a stringified
version of the CRC to the message.   Finally it sends the message to the server and waits for a response.
<p>
The response format is client timestamp, response message, and server time stamp.  If 
the client receives any response that 
does not start with the original client time stamp or if the wait for the response times out then
the request is resent.   More about this later.
<p>
The server implementation of the protocol in DispatchingWorker does a select on a list of read
file descriptors which includes the socket (host, port) as issued by the the configuration
server.   The select is repeated if it times out.
<p>
When input is detected on the socket, the server reads the request; checks the check sum; unpacks
and saves the client id, the client time stamp, and the ticket ; 
converts the ticket to a python dictionary; 
and calls the method specified by the "work" item in the ticket.   If any of these things fail
then the request is ignored presuming the client will resend the request.
<p>
The "work" is a text string but python is interpreted and allows runtime evaluation of method
names.  In the media changer the load method is in the media_changer.py module which has imported
and instantiated a DispatchingWorker class.   When the work method is finished it calls the DispatchingWorker
method reply_to_caller with a status result ticket.
<p>
reply_to_caller builds a stringified reply with the client time stamp, the status ticket, and
its own time stamp.  It sends the reply to the client; saves the clients address, the client message id,
and the complete reply in case of errors; and waits for more requests.
<p>
We save the complete reply in case of errors because we may get a request resent to the server which
the server has executed but whose response was not reliably returned.   Some requests,
for example, mounting a tape are not redoable; so, we save the reply and simply resend the reply.
When DispatchingWorker gets a request it first checks its request dictionary for a request that has the
same client id and time stamp and if it finds a match it resends the reply rather than executing the request.
<p>
The request dictionary contains all replies.
If it grows beyond a certain size (currently 1000 entries) then entries older
than 30 minutes are deleted.
<p>

he scheme described so far rquires that servers handle one request at a time and that clients
queue in the the servers udp input buffer waiting their turn.  This is satisfactory if requests
are guaranteed to finsih quickly; however, the media changers operation may take a long time to complete
while other operations might be done simultaneously.   To accomodate this, dispatching_worker was extended
to allow forking in servers.
<p>
The select in dispatching_worker now watches for input from the client socket and a list of pipe
fds on which the forked servers report their final status.  The parent server process then
reports this status back to the client.
<hr>
<h3><a name="trace">
1.5 Trace 
</h3>
Trace is a utility to trace execution of code through information saved in
the circular buffer, resided in a shared memory and available via special
commands. It was adapted from previous work where it has been used in the
real-time environment and is designed to have a minimal impact on the
performance of components of the system, as well as the overall performance.
Trace is widely used in all the <em>Enstore</em> modules.
<hr>
<h2><a name="db">
2 Databases in Enstore
</h2>
Enstores use databases to store persistent information.
Aside from the databases associated with PNFS, there are two databases, "file" and "volume" used by
file clerk and volume clerk respectively.
<p>
The database used in Enstore must provide the following:
<ul>
<li>
Support journaling of the database to record all changes and support full
     database recovery.
<li> 
Support transaction control to ensure the integrity of the information
in database.
<li>
Support database check-pointing in order to enable full database recovery.
<li>
Support performing daily backups of the database, log, and journal files.
<li>
Support recovery of corrupted databases using the journal or log files.
<li>
Support "python dictionary" interface.
</ul>
The directory that contains all database related files is called
"database directory" and is defined in configuration.

<h3><a name="current_db">
2.1 Current Underlying Database Implemented in Enstore
</h3>

The current Enstore implementation uses LIBTP (BSD DB v2.3) as the underlying
database product.
LIBTP is free for non-profit organizations like Fermilab, and has the
following features:

<ul>
<li>
one key dictionary-like database. It is designed to
store/retrieve binary large objects (BLOBs) of arbitrary length, by text key.
<li>
ability to store data items of unlimited size
<li>
support for various data storage structures: hash table, binary tree,
numbered records
<li>
allows duplicate keys (Enstore doesn't use them)
<li>
data scanning with cursors, multiple cursors may be opened at the same time
<li>
different levels of cursor stability
<li>
transactions
<li>
transaction logging
<li>
check-pointing
<li>
backup and recovery tools
<li>
custom locks
<li>
deadlock detection
</ul>


A LIBTP-Python shelve-like interface was developed. It provides access to:
<ul>
<li>All three data structures: hash table, binary tree, numbered records
<li>Cursors
<li>Transactions
<li>Locks
</ul>

LIBTP was chosen based on the following considerations:

<ul>
<li>
Nimbleness to allow us to set up test stands while developing and not being encumbered by
     database licensing issues.
<li>
It is similar to dbm-like databases used for the initial Enstore design.
This made it easy to develop a Python interface for it and any necessary
changes to the Enstore code were localized and relatively easy to make.
<li>
Database maintenance is relatively inexpensive. It requires only two
processes to run. One for check-pointing and the other for deadlock detection.
<li>
It is simple and fast enough.
<li>
It provides tools for database transaction logging, database backup and
recovery.
<li>
It is readily obtainable and free.
</ul>

<p> WE have examined Lib TP, and find that it meets the current mdest database
requiremtns of the project.  We have exploited "freeware" aspect
of it putting up many test stands.

We could replace LibTP a run II standard
database. However, we have no definite plans to do this, given our experience
with the tool, and the lack of any driving requirement to do this.

<p>
In addition to databases, file clerk and volume clerk also maintain
separate journal files.
These journal files can be used to recover databases when they can not
be recovered under normal circusmastance.

<h3><a name="backup_recovery">
2.2 Backup and Recovery Procedures
</h3>
<h3><a name="backup">
2.2.1 Backup
</h3>
Backup is a stand-alone procedure which can be performed manually at
any time or routinely using a cron job.
Currently the files that are backed up are database
files which contain the persistent data, log files which record the
transactions and journal files which are secondary transaction records
implemented to further help the recovery of databse if there is a need.
Enstore does live backup.
It copies those files to a
designated directory on a remote host.
The remote host and directory are defined in configuration server.
The backup procedure will perform the following actions:
<p>
<center><table>
<tr>
<td valign=top>Libtp database
<td valign=top><ul>
<li>identifies the log files that are involved in active transactions<br>
<li>creates the tar file of database files and all log files <br>
<li>deletes all log files that are not involved in active transactions
</ul>
<tr>
<td valign=top>Volume journal files
<td valign=top><ul>
<li>does journal file checkpointing (hold database access,
move current file to volume.jou.time_stamp, open empty
journal file, release database access)
<li>creates tar file of volume database file and journal files
<li>deletes old journal files
</ul>
<tr>
<td valign=top>File journal file
<td valign=top><ul>
<li>does journal file checkpointing
<li>creates tar file of file database file and journal files
<li>deletes old journal files
</ul>
<tr>
<td valign=top>Archives creation
<td valign=top><ul>
<li>creates new directory on remote host under designated
"root archival" director (name dbase.time_stamp)
<li>moves all the tar files to this area
</ul>
<tr>
<td valign=top>Archives cleanup
<td valign=top><ul><li>deletes all the archival directories created more then
N days ago (default is 10 days)</ul>
</table></center>
<h3><a name="recovery">
2.2.2 Recovery
</h3>
Recovery should be a job initiated manually in case of database corruption.
There are two ways to recover from a crash:
<p>
<center><table>
<tr>
<td valign=top>libtp utility
<td valign=top><ul>
<li>copy database tar file in designated directory, untar the file
<li>copy the latest log file from database directory.
<li>run db_recover utility
<li>delete all related files in database directory.
<li>move the resulting database files and the latest log file to
database directory.
</ul>
<tr>
<td valign=top>recovery from the journal files (not implemented yet)
<td valign=top><ul>
<li>copy database (database files and journal files) tar file
<li>in designated directory, untar the file
<li>copy the latest jouurnal file from the $ENSTORE_DB,
<li>run jou_recover utility
<li>delete all related files from database directory.
<li>move the resulting database files to database directory.
</ul>
</table></center>

<hr>
<h2><a name="admin_tools">
2.3 Administrative Tools
</h2>

Administrative tools will provide the following operations:
<ul>
<li>Display all volumes for a specified media
<li>List all the files and their location on a single or set of media
<li>List file/files on the media by creation date
<li>List all the media that belongs to a specified file family
<li>List files that belongs to a specified file family
<li>Display the date of the last mount for a specified volume
<li>List all media belonging to a file family sorted by the most recent media
mount date
<li>List all media belonging to a file family where the last access date is
before a specified date
<li>Export metadata of ejected media into a flat file
<li>Import metadata from a flat file when importing the media from outside the
Enstore system
<li>List all files/volumes that belong to user/group
<li>Mark the volume as readonly if all of the files on the media are older
than a specified date
<li> Delete specified files in the <em>pnfs</em> trashbin.
<li>Find and recycle volumes where all files have been deleted.
<li>Check for files known to the File Clerk but unknown to <em>pnfs</em>
</ul>

In addition, tools will be provided to implement the administrative functions
mentioned in the D0 Functional Specification section of this document.

<hr>
<h2><a name="protocol">
3 Communication Protocols
</h2>
The base protocol for Enstore is UDP for "brief" messages and TCP for data
transfers.

UDP message sizes are all less than size of maximum UDP packet size so the
protocol is very simple.

<p>
The base server protocol is the same for all servers.  State-fullness is
minimized, not eliminated.
<p>
Each transmission has a unique ID, timeout and maximum number of retries
associated with it.  The timeout allows for debugging.
For each reception, the "message" is checked against messages received to see
if the reception is a repeat. If the reception is a *repeat request*, send a
saved copy of the response; if the reception is *repeat response*, just
ignored it. This will take care of the case when a timeout/retry happens just
before a response is received.

<h3><a name="read_protocol">
3.1 Read Protocol
</h3>
The communications performed during a read operation are illustrated in the
diagram below and described more fully in the following text.
<p>
NOTE: The communications between the Mover and the Configuration Server happens
approximately every two minutes.  It has been added to the following drawing
to show that this communication is important, but it can occur anywhere in the
communications flow before the Mover contacts the Library Manager.
<p>
<center><img src=read.gif></center>
<p>
<div align=right><a href="read.ps">(also available in Postscript)</a><div align=left>
<p>
<ul>
<li>
The user (through <em>encp</em>) contacts <em>pnfs</em> asking for a bit file ID (bfid)
for the named file.
<li>
<em>pnfs</em> returns the bfid to <em>encp</em>.
<li>
<em>Encp</em> asks the Configuration Server with which File Clerk should it be
communicating.
<li>
The Configuration Server returns the location of the appropriate File Clerk.
<li>
<em>encp</em> asks the File Clerk the information about the file with given bit file ID.
<li>
The File Clerk asks the Volume Clerk information for the given Volume Label
<li>
The Volume Clerk returns the information to the File Clerk

<li>
The File Clerk returns file information containing the Volume Label and the Library Manager name
<li>
<em>encp</em> asks the Configuration Server the location of the Library Manager
dealing with this File Family
<li>
The Configuration Server returns the location of the Library manager
<li>
<em>encp</em> sends the read request to the Library Manager
<li>
The Library Manager asks the Volume Clerk if the volume for the
requested file has a read access.
<li>
The Volume Clerk confirms access permission
<li>
The Library Manager puts read request into internal request queue and
tells the <em>encp</em> that the request has been accepted
<li>
The Library Manager finds next potentially available mover in its list
of movers and sends summon message to it initiating mover dialog with
this Library manager
<li>
The Mover "wakes up" receiving summon message from the Library Manager
and asks the Library Manager if there is any work for it to do.
<li>
The Library Manager moves the file internally from request queue to
work queue. 
<li>
The Library Manager tells the Mover which file to read and which volume to
mount. 
<li>
The Mover tells <em>encp</em> from which host and port to read the data.
<li>
The Mover asks the Media Changer to mount a particular volume.
<li>
The Media Changer responds once the volume is mounted.
<li>
The Mover sends the data to <em>encp</em>.
<li>
The Mover tells <em>encp</em> when all the data has been transferred and sends the crc information.
<li>
The read has completed.
<li>
The Mover tells the Library Manager that he still has the volume mounted.
</ul>

<h3><a name="write_protocol">
3.2 Write Protocol
</h3>
The communications performed during a write operation are illustrated in the
diagram below and described more fully in the following text.
<p>
NOTE: The communications between the Mover and the Configuration Server happens
approximately every two minutes.  It has been added to the following drawing
to show that this communication is important, but it can occur anywhere in the
communications flow before the Mover contacts the Library Manager.
<p>
<center><img src=write.gif></center>
<p>
<div align=right><a href="write.ps">(also available in Postscript)</a><div align=left>
<p>
<ul>
<li>
The user (through <em>encp</em>) contacts <em>pnfs</em> with a request to create a file.
<li>
<em>pnfs</em> returns the file family and volume library information to <em>encp</em>.
<li>
<em>encp</em> asks the Configuration Server with which Library Manager should
it be communicating.
<li>
The Configuration Server returns the location of the appropriate Library
Manager.
<li>
<em>encp</em> sends the write request to the Library Manager, including file
family and number of bytes.
<li>
The Library Manager puts write request into internal request queueand tells
the <em>encp</em> that the request has been accepted.
<li>
The Library Manager finds next potentially available mover in its list
of movers and sends summon message to it initiating mover dialog with
this Library manager
<li>
The Mover "wakes up" receiving summon message from the Library Manager
and asks the Library Manager if there is any work for it to do.
<li>
The Library Manager asks the Volume Clerk for a volume for the file with the
specified size and file family. 
<li>
The Volume Clerk returns the volume to the Library Manager. 
<li>
The Library Manager moves the file internally from request queue to
work queue. 
<li>
The Library Manager tells the Mover which file to write and which volume to
mount. 
<li>
The Mover asks the Media Changer to mount a particular volume.
<li>
The Media Changer responds once the volume is mounted.
<li>
The Mover tells <em>encp</em> to which host and port to write the data.
<li>
The Mover tells the Volume Clerk that he is appending to this volume. 
<li>
The Volume Clerk acknowledges this. 
<li>
<em>Encp</em> sends the data to the Mover.
<li>
The Mover tells the Volume Clerk that the append operation is done and how much
space is left on the volume.
<li>
The Volume Clerk acknowledges this.
<li>
The Mover tells the File Clerk which file has been created.
<li>
The File Clerk responds with the bit file id.
<li>
The Mover tells <em>encp</em> that the file has been written and sends the bit
file ID and the crc.
<li>
<em>Encp</em> tells <em>pnfs</em> that the file has been created, and the bfid should
be stored.
<li>
The write has completed.
<li>
The Mover tells the Library Manager that he still has the volume mounted.
<li>
The Library Manager tells the Mover that there is no work to be done.
<li>
The Mover tells the Media Changer to dismount the volume.
</ul>

<hr>
<h2><a name="error">
4 Error Control
</h2>

<h3><a name="eass">
4.1 Assumptions about Errors
</h3>
Enstore conforms to the (oral) statements made about Run II operating conditions:
<UL>
	<LI>No single error when reading a tape is fatal to upper level
		software.

	<LI>When writing, errors should be handled by retries
		on different media.

	<LI>Mover nodes may crash, with minimal disruption of the system.

	<LI>The system should generate alarms and receive immediate service
	   when its throughput falls below predefined levels.

	<LI>Routine error conditions should be cleared in normal
	   business hours.

	<LI>It shall be possible to redirect writes to another library
	   in case of library failure.

	<LI>The system shall be capable of being monitored by PATROL.

</UL>
</P>

<h3><a name="eover">
4.2 Error Overview
</h3>

<P> Enstore is a distributed system. For a transfer to succeed, many of the
Enstore processes must be up and running.  Therefore, the servers are
robust, and run on reliable computers. Nevertheless, it is good to consider
the the intrinsic ability for the system to recover when a process or
system running a process crashes.  This is summarized in the table below:


<center><TABLE BORDER=1 CELLPADDING=2 CELLSPACING=0>

<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Process</B></FONT></FONT></TH>
<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Where is State?</B></FONT></FONT></TH>
<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Effect of Crash</B></FONT></FONT></TH>


<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2><em>Encp</em> </FONT> </TD>
<TD> <P><FONT SIZE=2>In the user's <em>encp</em> transfer command </FONT> </TD>
<TD> <P><FONT SIZE=2>Transfer is canceled/aborted </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Configuration Server </FONT> </TD>
<TD> <P><FONT SIZE=2>Static configuration file </FONT> </TD>
<TD> <P><FONT SIZE=2>Wait for restart of server </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>File Clerk </FONT> </TD>
<TD> <P><FONT SIZE=2>Persistent database table </FONT> </TD>
<TD> <P><FONT SIZE=2>Wait for restart of server </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Volume Clerk </FONT> </TD>
<TD> <P><FONT SIZE=2>Persistent database table </FONT> </TD>
<TD> <P><FONT SIZE=2>Wait for restart of server </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Library Manager </FONT> </TD>
<TD> <P><FONT SIZE=2>In-memory lists of what work is queued, and what work
is at what mover </FONT> </TD>
<TD> <P><FONT SIZE=2>Recovery of state is not yet implemented. Recovery of state is possible through <em>encp</em> retries. </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Mover </FONT> </TD>
<TD> <P><FONT SIZE=2>If busy, the current transfer + the current volume </FONT> </TD>
<TD> <P><FONT SIZE=2><em>Encp</em> retries writes, exits with errors on read. </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2><em>pnfs</em> NFS Servers </FONT> </TD>
<TD> <P><FONT SIZE=2>DBM database "file metadata" </FONT> </TD>
<TD> <P><FONT SIZE=2>NFS retry mechanisms </FONT> </TD>


<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Media Changer </FONT> </TD>
<TD> <P><FONT SIZE=2>In memory lists of work given to library micro </FONT> </TD>
<TD> <P><FONT SIZE=2>State refreshed by Enstore UDP retry protocol mechanism </FONT> </TD>


<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Log Server </FONT> </TD>
<TD> <P><FONT SIZE=2>None </FONT> </TD>
<TD> <P><FONT SIZE=2>Logs are not written </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Inquisitor </FONT> </TD>
<TD> <P><FONT SIZE=2>None </FONT> </TD>
<TD> <P><FONT SIZE=2>Displays are not refreshed until restart </FONT> </TD>



</TABLE></center>


<h3><a name="edet">
4.3 Detailed Error Discussion
</h3>

<P>Much of the system state is stored within the user's <em>encp</em> client.
This allows the <em>encp</em> client to retry on a large number of different
errors. This retry is given a very high priority when it is received
by the Library Manager so the user doesn't have to wait again for
their job. It is the Library Manager's responsibility to ensure the
error is not just repeated; for example, on a volume read error, the
volume should not go to the same drive on a retry. The Library
Manager gets the retry, volume and drive information from the ticket.</P>

<P>Many interesting errors are related to cases where the volume
cannot be written or read, or when it is suspected that volume is
jammed, etc. More experience is needed with the actual hardware
before the correct error control behavior is established. In the
interim, Enstore will make the following working assumptions: 
</P>
<UL>
	<LI>If there is trouble during a load or unload operation, the
	volume is assumed to be physically jammed. No operations on the
	drive or the volume until an administrator looks at the problem. 
	<LI>If several drives have fatal write errors on a volume, the
	volume will be marked read only. 
	<LI>If several drives have fatal read errors on a volume, the volume
	will be marked no access. 
	<LI><P>If a drive has several fatal errors on different volumes, the
	drive will be marked offline.</P>
</UL>
<DL>
	<DT><BR>
	</DT>
	<DT>The following table describes the error conditions that Enstore
	handles. 
	</DT>
	<DT><BR>
	</DT>
	<DT><BR><BR>
	</DT>
</DL>
<center><TABLE BORDER=1 CELLPADDING=2 CELLSPACING=0>
	<TR VALIGN=TOP>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Error Code</B></FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Description</FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Administrator</FONT></FONT></P>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Responsibility</FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Mover</FONT></FONT></P>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Responsibility</FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Library Manager</FONT></FONT></P>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Responsibility</FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2><em>encp</em></FONT></FONT></P>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Responsibility</FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Retry</FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Volume </FONT></FONT>
			</P>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>State</FONT></FONT></TH>
		<TH>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>Drive </FONT></FONT>
			</P>
			<P><FONT COLOR="#0000ff"><FONT SIZE=2>State</FONT></FONT></TH>
	</TR>
	<TR>
		<TH COLSPAN=9 VALIGN=TOP>
			<P><BR><BR>
			</P>
			<P><BR>
		</TH>
	</TR>
	<TR>
		<TH COLSPAN=9 VALIGN=TOP>
			<P><FONT COLOR="#ff3333"><FONT SIZE=2>Volume Write Errors</FONT></FONT></TH>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_NOTAPE</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Request volume was not found in the library. Volume Clerk's data base is inconsistent with library micro's database. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check volume in morning </FONT>
		</TD> 
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_TAPEBUSY</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Requested volume is in another drive. Enstore bug,
			or some other system has mounted volume or library micro put
			volume elsewhere. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_DRIVEBUSY</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>A volume is already in drive. Enstore bug or
			misconfiguration. Note: mover waits for automatic cleaning tape to
			be ejected. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and configuration in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_BADMOUNT</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mount failure or load operation failed. Must
			assume jammed volume. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_BADSPACE</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>EOD cookie does not produce EOD. Wrong volume,
			Enstore bug or drive space error. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_ERROR</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Error writing data block or file mark. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive in morning </FONT>
			</P>
			<P><BR><BR>
			</P>
			<P><FONT SIZE=2>Check volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>If several errors occur with different volumes,
			offline the drive.</FONT></P>
			<P><FONT SIZE=2>If several errors occurs with different drives,
			mark volume as read only </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
			</P>
			<P><BR><BR>
			</P>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
			</P>
			<P><BR><BR>
			</P>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><BR><BR>
			</P>
			<P><BR><BR>
			</P>
			<P><FONT SIZE=2>Read Only</FONT></P>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_EOT</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Hit EOT while writing data block or file mark. </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume as full and read only </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Read only </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_UNLOAD</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Error unloading volume from drive. Must assume
			jammed volume. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Not involved </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_UNMOUNT</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Error unmounting volume. Volume is hanging in the
			drive. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Not involved </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_NOBLANKS</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No more blank volumes. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Administrator should be paged.</FONT></P>
			<P><FONT SIZE=2>DAQ should switch to alternate library. </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>WRITE_MOVER_CRASH</B></FONT></TD>
		<TD>
			<P><FONT SIZE=2>If mover is connected to an <em>encp</em>, <em>encp</em> will notice
			its sockets being torn down prematurely. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume as no access, retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR>
		<TD COLSPAN=9 VALIGN=TOP>
			<P><BR><BR>
			</P>
			<P><BR>
		</TD>
	</TR>
	<TR>
		<TH COLSPAN=9>
			<P><FONT COLOR="#ff3333"><FONT SIZE=2><B>Volume Read Errors</B></FONT></FONT></TH>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_NOTAPE</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Request volume was not found in the library. Volume Clerk's data base is inconsistent with library micro's database. </FONT>

		</TD>
		<TD>
			<P><FONT SIZE=2>Check volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_TAPE_BUSY</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Requested volume is in another drive. Enstore bug,
			or some other system has mounted volume or library micro put
			volume elsewhere. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_DRIVEBUSY</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>A volume is already in drive. Enstore bug or
			misconfiguration. Note: mover waits for automatic cleaning tape to
			be ejected. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and configuration in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_BADMOUNT</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mount failure or load operation failed. Must
			assume jammed volume. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_BADLOCATE</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Failed space or initial CRC's don't match. Either
			file location cookie is corrupted, wrong volume in the drive or
			drive cannot space properly. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_ERROR</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Error reading data block. Run of the mill read
			error. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive in morning </FONT>
			</P>
			<P><BR><BR>
			</P>
			<P><FONT SIZE=2>Check volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>If several errors occur with different volumes,
			offline the drive.</FONT></P>
			<P><FONT SIZE=2>If several errors occurs with different drives,
			mark volume as read only </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Retry </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Yes</FONT></P>
			<P><BR><BR>
			</P>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><BR><BR>
			</P>
			<P><BR><BR>
			</P>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_COMP_CRC</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>CRC mismatch Drive and the volume are suspicious.
			Corrupt file location cookie, drive space error, wrong volume in
			the drive, etc. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume as no access</FONT></P>
			<P><FONT SIZE=2>Offline drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_EOT</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Hit EOT when reading. Corrupt file location
			cookie, drive space error, or wrong volume in the drive. Should
			have hit an EOF. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume as no access</FONT></P>
			<P><FONT SIZE=2>Offline drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_EOD</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Hit EOD when reading. Corrupt file location
			cookie, drive space error, or wrong volume in the drive. Should
			have hit an EOF. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume as no access</FONT></P>
			<P><FONT SIZE=2>Offline drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_UNLOAD</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Error unloading volume from drive. Must assume
			jammed volume. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Not involved </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_UNMOUNT</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Error when unmounting volume. Volume is hanging in
			the drive. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive and volume in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access</FONT></P>
			<P><FONT SIZE=2>Offline the drive </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Not involved </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>READ_MOVER_CRASH</B></FONT></TD>
		<TD>
			<P><FONT SIZE=2>If a Mover is connected to an <em>encp</em>, <em>encp</em> will
			notice its sockets being torn down prematurely. The volume is tied
			up at a mover. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check volume and drive in the morning </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Mark volume no access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>No Access </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
	<TR>
		<TD COLSPAN=9 VALIGN=TOP>
			<P><BR><BR>
			</P>
			<P><BR>
		</TD>
	</TR>
	<TR>
		<TH COLSPAN=9>
			<P><FONT COLOR="#ff3333"><FONT SIZE=2><B>Other Errors</B></FONT></FONT></TH>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>ENCP_GONE</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>User has gone away while request is queued. </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Unilateral unbind </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>TCP_HUNG</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>It appears that the data TCP link is hung.</FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check with user in morning </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Compute an anticipated transfer time for every
			socket operation and abort the transfer if the actual transfer
			takes more than three times the expected value. </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>No </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>LM_CRASH</B> </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Library Manager crashes, and loses its queue of
			pending work. The <em>encp's</em> will never be called back, and will wait
			forever. </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Ping Library Manager, every N mins (30) to see if
			its request has gotten lost. </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
	</TR>
	<TR VALIGN=TOP>
		<TD>
			<P><FONT SIZE=2><B>MOVER_CRASH</B></FONT></TD>
		<TD>
			<P><FONT SIZE=2>Mover is idle. The system degrades. </FONT>
		</TD>
		<TD>
			<P><FONT SIZE=2>Check drive in morning </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Remove from list when mover fails to respond to
			summon </FONT>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><BR>
		</TD>
		<TD>
			<P><FONT SIZE=2>Offline </FONT>
		</TD>
	</TR>
</TABLE></center>
<P><BR><BR>
</P>

<ul>
  <li> "Freeze the volume in the drive" means:
       <ul>
	 <li>Not unloading the volume from the drive
	 <li>Freezing the volume
	 <li>Offlining the drive
       </ul>
  <li> "Freezing the volume" means:
       <ul>
	 <li>mark the volume as "system noaccess"
	 <li>log that this happened and let an administrator look at the
	      problem in the morning.
	 <li><em>Encp</em> shall not retry.
       </ul>
  <li> "Offlining the drive" means:
       <ul>
	 <li>Preserving as much state as possible.
	 <li>Writing a complete description in an error log.
	 <li>Leaving the problem until business hours unless the capacity of
	      the system falls below a threshold.
       </ul>
</ul>

<hr>
<h2><a name="impexp">
5 Volume Import and Export
</h2>

It is certainly  possible to import and export information by copying disk resident data
from Enstore using the <em>encp</em> command. 
However, given the need to move large amounts of data, and the wide spread
use compatible tape drives and
media, it is usually  more efficient to interchange tape volumes: that is, to write
tapes outside of Enstore and import them into the system, and to write tapes
inside of Enstore and remove them from the system. In this way, for example,
Enstore can be used as a kind of tape copy facility (one that always copies from tape to disk to tape).

<p> What follows are draft design notions, importing and exporting are 
not yet fully implemented.

<h3><a name="volexp">
5.1 Volume Export
</h3>

<p> Exportable volumes are built in Enstore using the <em>encp</em> command, with the
command line switch --ephemeral, which specifies a temporary, "ephemeral" file family. An
ephemeral file family is a unique file family name created just for this <em>encp</em>
command, with a file family width of exactly one. Under these conditions, files will
be placed on tape volume in the order specified by the user. Once the data is written to the
tape, the file family name is changed to the tape_label_name.ephemeral.

<p>An experimenter wishing to build an exportable volumes would follow these steps:

<ul>

<li> Select a file structuring method which is supported in Enstore, which
your users can read, and complies with tape interchange standards of your
experiment.  (For D0 this is the CPIO format.)

<li> Decide what kind of media you want for the exported volume. (For D0
this will be the media selected by the SMWG.)

<li> Identify and make locally disk resident all the files that you want to put on a
volume. Consider the capacity of the tape, and whether you can tolerate files
overflowing onto another volume.

<li> Specify those files on a single <em>encp</em> command line, in the order you would
like to have them placed on tape. Use the <em>encp</em> the --ephemeral file family
switch on the command line.
[After the initial <em>encp</em> copy, the file family for the tape is known. If the
user chooses, she may append additional files to the tape.  However,
the no additional tapes will be added to the file family once it is
full. It is recommended that all files be copied with one <em>encp</em> command.]

<li> Optionally generate metadata for the volume, if your user will want to
know what files are on the tape.

<li> Move the volume from the robot to a shelf library, using an Enstore
administration tool.

<li> Remove the volume from the Enstore system, using an Enstore
administration tool.

</ul>

An experiment can make tapes in the Enstore system at Fermilab, and
give the volumes to an experimenter, who can read the tapes anywhere.
Experimenters can optionally generate a metadata file which is provides a
map of the exported tape. However,
some tape formats, such as CPIO,  are sufficiently self-describing so that the tape may
be dumped to disk with standard utilities.
<p>
As an example, an experimenter can stage an entire CPIO exported volume using
gnu CPIO at her home institution. (Since there are many files on an
Enstore tape, special care should be taken to select a nonrewind tape
device.
On a UNIX system, CPIO tapes can be read with no special infrastructure
other than 
gnu CPIO. For example, here is a simple script to read an exported tape at
a home institution:

<pre>
#!/bin/sh

# en_dump_tape  A shell script to dump an  Enstore CPIO format tape.
# This is pseudo code, not functional yet

$tape=$1
test ! -f $tape || exit 1  ## Try to make sure we have not selected a device file
while /bin/true ; do  
   mt -t $tape fsf 1      || exit 1
   (dd if=$tape  | cpio -o  )  || exit 1
done

</pre>

<h3><a name="volimp">
5.2 Volume Import
</h3>

Volume import is a good implementation for repeated and sizeable
transfers of data. Volume import is not as easy as export and, therefore, it is not a good choice for small, occasional
transfers.

Planning is important.
Tapes in a robot require special labels so that can be automatically
scanned and placed in the robot. The labels must be unique within a robot and
within an Enstore system. You will need to procure tape volumes with labels
meeting these requirements. Imported tapes are read-only in Enstore. (Details
of these requirements are TBD, after the serial media working group
decision). 

<p> Although it is not mandatory, an Enstore tool will very likely have to be produced to get good
results for experimenters at home institutions wishing to make importable tapes. 

<p>If the tapes are to be accessed many times, the experiment must take some
time to think about the layout of data on the tapes and how the files ought
to fit into the Enstore name space.

<ul>
 <li> Files that are accessed together should likely be put on the same tape. 
 <li> Most likely, the same file should not be placed on more than one
      tape. Duplicate data files (very important data) is handled differently.
 <li> Files must definitely not span tapes. 
 <li> If a likely order of future access is known, files should be put on 
	tape in that order.
 <li> Think of how you would like the files to appear in the Enstore name
        space.
 <li> Identify a file structuring method that is compatible with Enstore and
        your experiment's data interchange standards.
 <li> Files should be written with a blocksize yielding good performance.
	(precise recommendation is TBD, after SMWG decision).
 <li> Files should be written with the recommended partitioning.
	(precise recommendation is TBD, after SMWG decision).

</ul>

<p> Since the objective is to import a large amount of data, it is required to
generate metadata for each tape; otherwise the tape will have to be scanned to
determine the metadata and this defeats the purpose if importing volumes!


<p> Metadata for each tape is:

<ul>
<li> The external bar-coded label of the tape.
<li> The kind of tape media.
<li> The file structuring method used to generate the tape.
<li> The blocksize used in writing the tape.
</ul>

<p> Metadata for each file is:

<ul>
<li> An Adler 32 CRC of the first 65536 bytes of the file. If not
     available, a value of "None" is acceptable and Enstore skips the check
     (not recommended).
<li> An Adler 32 CRC of all the bytes in the file. If not
     available, a value of "None" is acceptable and Enstore skips the check
     (not recommended).
<li> The number of blocks (or less desirably, the number of file marks) preceding the beginning of the wrapper.
<li> The number of the partition holding the file (if this feature is used).
<li> A name for the file.
<li> A path for the file in the <em>pnfs</em> namespace.
</ul>

<p> The procedure the user would follow is:

<ul>
 <li>Identify the files you want to put on a tape.
 <li>Put an optically bar-coded tape in the tape drive. Sheets of bar codes
      could be sent to the remote site or actual pre-labled tapes could be
      used. The important point is that the robot will be able to recognize
      the tapes.
 <li>Using a tool we provide:
 <ul>
   <li>Place the files on the tape.
   <li>Generate the  metadata for that file.
 </ul>
 <li> Remove the tape from the drive.
</ul>


The procedure for importing a volume is:

<ul>
  <li> Deliver the volume to the data center.

  <li> Use an Enstore administration utility to make a record for the volume
  in the volume table, using the volume metadata, and physical
  location as the shelf library.

  <li> Use an Enstore administration utility to place the metadata for
  each file into the file table and <em>pnfs</em> namespace.

</ul>




<hr>
<h2><a name="test_system">
6 Test System
</h2>

The Enstore hardware test system was designed to be able to test
data movement at rates comparable to requirements for Run II data
logging, to evaluate gigabit networking technologies, and to 
determine scaling for the larger support hardware required to 
support all of Run II.  The ability to sustain data rates comparable
to Run II data logging requirements allows the test system to double
as the RIP (Reconstruction Input Pipeline) test platform.

The RIP/Enstore hardware test system consists of the following:
<ul>
  <li> 10 x86 PC's with:
       <ul>
	 <li> dual 400 MHz Pentium II processors
	 <li> 128 MB SDRAM memory
	 <li> integrated fast ethernet
	 <li> integrated ultra wide SCSI (1 bus)
	 <li> integrated ultra SCSI (1 bus)
	 <li> integrated video
	 <li> 2 4-GB Seagate Barracuda SCSI disks
	 <li> gigabit ethernet interface (Packet Engines)
	 <li> two nodes with differential wide SCSI interface
	 <li> four nodes with wide LVD SCSI interface
	 <li> two nodes with Fibre Channel (for SCSI) interface
       </ul>
  <li> Disk chassis with 4 ultra wide SCSI buses, each with 2 18-GB disks
       
  <li> Disk chassis with dual FC-AL loop, FC controller, 4 9-GB FC disks.
       
  <li> Foundry fast ethernet switch with gigabit ethernet uplinks (2)
       
  <li> Foundry gigabit ethernet switch
       
  <li> Packet Engines gigabit ethernet full duplex repeater
</ul>

Two nodes are connected to tape drives on the EMASS robot.  One node,
via a single wide differential SCSI bus, is connected to four Sony AIT
tape drives.  The other is connected via a single bus to four Quantum
DLT 7000 drives, and two Exabyte Mammoth drives.
<p>
The RIP cluster is located physically adjacent to the SAM cluster,
which has similar PC's.  For testing throughput to/from clients, the
SAM cluster can be connected to the RIP cluster via 10, 100, and 1000
Mbps network uplinks (though each SAM node has only 10/100 Mbps
capability).
<p>
<center><img src="test-network.gif"></center>
<p>
<div align=right><a href="test-network.ps">(also available in Postscript)</a><div align=left>
<p>

Enstore has been installed on the Test System and is fully operational.
The administration of the system is flexible and can be changed by
modifying a single configuration file.
<p>
Currently, the configuration is as follows:
<center><TABLE BORDER COLS=2 WIDTH="50%" NOSAVE >
<TR VALIGN=CENTER NOSAVE>
<TD NOSAVE><B>NODE</B></TD>
<TD NOSAVE><B>SERVERS</B></TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip1</TD>
<TD NOSAVE>4-AIT Media Changers<br>2-AIT Media Changers<br>4-DLT Movers</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip2</TD>
<TD NOSAVE>4-AIT Movers</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip3</TD>
<TD NOSAVE>cluster console</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip4</TD>
<TD NOSAVE>General Use</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip5</TD>
<TD NOSAVE>Disk Library Manager<br>AIT Library Manager<br>DLT Library
Manager<br>Mammoth Library Manager<br>Redwood-50 Library
Manager<br>Redwood-20 Library Manager</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip6</TD>
<TD NOSAVE>Configuration Server<br>File Clerk<br>Volume Clerk<br>Admin
Clerk<br>Log Server<br>Inquisitor<br>Alarm Server<br>Disk Mover<br>Disk
Media Changer</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip7</TD>
<TD NOSAVE>General Use</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip8</TD>
<TD NOSAVE>General Use<br>Serves Home areas</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip9</TD>
<TD NOSAVE>2-STK Movers</TD>
</TR>
<TR VALIGN=LEFT NOSAVE>
<TD NOSAVE>rip10</TD>
<TD NOSAVE>4-DLT Media Changers<br>STK Media Changer</TD>
</TR>
</TABLE></center>
<P>

In the past, we have also tested exabytes on AIX machines. We have not
continued with this effort, but rather have concentrated on drives and cpus
that will most likely be used in conjunction with the EMASS robot.

<center><img src="test-configuration.gif"></center>
<p>
<div align=right><a href="test-configuration.ps">(also available in Postscript)</a><div align=left>
<p>

<h2><a name="ts_results">
6.1 Test System Results
</h2>

<h3>Test System Configuration:</h3>

<ul>
<li> EMASS Robot:
<center>     <table cols=2 width="50%">
     <tr>
     <td> 2 AITS
     <td> (15 of 84 tapes allocated)
     <tr>
     <td> 1 Mammoth
     <td> (15 of 42 tapes allocated)
     <tr>
     <td> 2 DLTS
     <td> (15 of 84 tapes allocated)
     </table></center>
<li> STK Robot:
<center>     <table cols=2 width="50%">
     <tr>
     <td> 2 Redwoods
     <td> (5 of 200 tapes allocated)
     </table></center>
<li> Disk test movers
</ul>

<h3>Preliminary Rates Measurements:</h3>
<center><table cols=5 width="80%">
<tr>
<td> Device
<td> Writing
<td> Reading
<td> Network
<td> "Mem->tape"
<tr>
<td> &nbsp;
<td> (MB/S)
<td> (MB/S)
<td> (Mbits/S)
<td> (MB/S)
<tr>
<td> AIT
<td> 2.7
<td> 2.7
<td> 94
<td> 2.7
<tr>
<td> Mam
<td> 2.8
<td> 2.7
<td> 94
<td> 2.8
<tr>
<td> DLT
<td> 4.9
<td> 4.8
<td> 94
<td> 4.9
<tr>
<td> STK
<td> 8.8
<td> 7.5
<td> 83
<td> 9.8
</table></center>

<dl>
<dt> Some STK details:</dt>
  <dd>
       <ul>
	 <li> 1 GB file transferred
	 <li> Mount time ~ 41 seconds
	 <li> Seek time ~ 0 seconfs (beginning of tape)
	 <li> Enstore queue wait time ~ 1 second
	 <li> Transfer time ~ 116 seconds (8.8 MB/S)
	 <li> EOF time ~ 0 seconds
	 <li> Get stats time ~ 34 seconds
	 <li> Effective User rate: 5.7 MB/s (appending)
       </ul
	      </dd>
</dl>

<h3>Preliminary CPU Utilization for AIT transfers:</h3>
<center><table cols=3 width="40%">
<tr>
<td> &nbsp;
<td> Mover
<td> User's <em>Encp</em>
<tr>
<td> no crc
<td> 5-10%
<td> 5-10%
<tr>
<td> crc
<td> 10-20%
<td> 10-20%
</table></center>


<hr>
<h2><a name="interface">
7 Interfaces and Integration
</h2>

Below is an (incomplete) summary of interfaces that are intrinsic to
the Enstore software. They are specified and coded as
part of the software project.

<center><TABLE BORDER=1 CELLPADDING=2 CELLSPACING=0>

<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Function</B></FONT></FONT></TH>
<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Person</B></FONT></FONT></TH>
<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Software</B></FONT></FONT></TH>


<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Initiate a specific transfer betwen tape and disk </FONT> </TD>
<TD> <P><FONT SIZE=2>Experimenter </FONT> </TD>
<TD> <P><FONT SIZE=2><em>encp</em> </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Organize names </FONT> </TD>
<TD> <P><FONT SIZE=2>Experimenter </FONT> </TD>
<TD> <P><FONT SIZE=2><em>pnfs</em> </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Choose library to write to </FONT> </TD>
<TD> <P><FONT SIZE=2>Experimenter </FONT> </TD>
<TD> <P><FONT SIZE=2><em>pnfs</em> </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Create file families, administer width </FONT> </TD>
<TD> <P><FONT SIZE=2>Experimenter </FONT> </TD>
<TD> <P><FONT SIZE=2><em>pnfs</em> </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Current status on web </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administrator </FONT> </TD>
<TD> <P><FONT SIZE=2>Inquisitor </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Summary status on Web </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administrator </FONT> </TD>
<TD> <P><FONT SIZE=2>Inquisitor </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Routine periodic monitoring </FONT> </TD>
<TD> <P><FONT SIZE=2>TBD </FONT> </TD>
<TD> <P><FONT SIZE=2>Patrol  (or TBD?) + Alarm module </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Move volumes between shelf and library </FONT> </TD>
<TD> <P><FONT SIZE=2>Experimenter </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administration Utility </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Move volumes between shelf and out-of-system (includes new volumes) </FONT> </TD>
<TD> <P><FONT SIZE=2>Experimenter </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administration Utility </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Drain system </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administrator  </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administration Utility </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Shutdown system </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administrator </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administration Utility </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>(Re)Start system </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administrator </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore Administration Utility </FONT> </TD>

</TABLE></center>

<p> Interfacing to an experiment means placing Enstore in a larger
system context. 
 From the point of view of Enstore with network attached tapes, there is an
Enstore system which interfaces to the rest of D0 on at the NIC-card cable
connector. In addition to the interfaces to the user and administrators (which
are intrinsic to Enstore software), there are other miscellaneous interface
issues associated with a real instance of the software, since the whole system
must conform to the Experiments, Division's and Laboratories system
constraints:

<center><TABLE BORDER=1 CELLPADDING=2 CELLSPACING=0>

<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Type</FONT></FONT></TH>
<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Constraint</FONT></FONT></TH>
<TH> <P><FONT COLOR="#0000ff"><FONT SIZE=2><B>Imposed by</FONT></FONT></TH>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Network </FONT> </TD>
<TD> <P><FONT SIZE=2>Conforming Physical Media </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Network </FONT> </TD>
<TD> <P><FONT SIZE=2>Protocol Extensions </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Network </FONT> </TD>
<TD> <P><FONT SIZE=2>16K minimum UDP Datagram size </FONT> </TD>
<TD> <P><FONT SIZE=2>Enstore </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Network </FONT> </TD>
<TD> <P><FONT SIZE=2>Traffic pattern to machines where one NIC card is not sufficient, </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Siting </FONT> </TD>
<TD> <P><FONT SIZE=2>Location </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Operations </FONT> </TD>
<TD> <P><FONT SIZE=2>Run II Operations Software Framework (Patrol or TBD) </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Operations </FONT> </TD>
<TD> <P><FONT SIZE=2>Failure planning (i.e. broken tape library, Power) </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Operations </FONT> </TD>
<TD> <P><FONT SIZE=2>Upgrade-ability </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Operations + Security </FONT> </TD>
<TD> <P><FONT SIZE=2>Standard Administration </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

<TR VALIGN=TOP>
<TD> <P><FONT SIZE=2>Security </FONT> </TD>
<TD> <P><FONT SIZE=2>Authentication etc are TBD </FONT> </TD>
<TD> <P><FONT SIZE=2>System </FONT> </TD>

</TABLE></center>


<p>All production hardware is to come from the D0 Budget, requisitioned by
D0. This includes the Enstore system, tape drives and other peripherals.
D0 have a baseline system design. From the point of view of the storage
management project, The main features the D0 system are:    

<ul>
 <li>IP Based connectivity.

 <li>A few SMP boxes back-ending their DA at the D0 assembly building.

 <li>An analysis facility in FCC consisting of a large SMP box and
 smaller SMP boxes.

 <li>Many hundred farm nodes, most likely an ensemble of small PC's or
 economical RISC work stations provided by the lowest bidder.  Whether there
 are "I/O nodes" associated with the farm is  TBD.

 <li>An Enstore system with about 8-10 mover nodes, and sufficient other
      nodes to run the rest of the system.
</ul>

<p> Given the discussion above, the main interface issues with D0 are:

<ul>
 <li> Specification  and design of a data lan.
 <li> The number and type of NIC cards on each computer.
 <li> Tuning the features in Enstore for the significant multiple NIC card
      machines.
</ul>

<p> To keep transfers efficient, It is important to have a network design
which avoids congestion.  

<p> It is important to characterize the rate achievable on a NIC card, and the
amount of CPU required to drive NIC cards.  Typically, there is less CPU/BYTE
when writing to the network, then when read from it. One vendor reports, for
CPU's available in late 1998:

<pre>
	10-11 MB/S on a 100 mbps ethernet NIC
	30-35 MB/S on a Gbit ethernet card (standard MTU)
	70-80 MB/S on a Gbit ethernet card (jumbo MTU)
	
	30-35 MB/S/CPU   standard ethernet frames
	78    MB/S/CPU   "jumbo" (~9000 byte) frames.
					(n.b. GB ethernet here...)
</pre>

<p>On an analysis server, Allocation of streams to NIC cards is most easily
accomplished statistically. For machines with very many potential streams,
this is best effected by fewer, fatter pipes.  Ideally, there is little packet
loss and traffic is regulated by the TCP window.  If Jumbo frames were
acceptable not generally, but only between the Enstore system and the D0
analysis machine, statistical load balancing over four GB NICs consuming two
CPUS could be could easily sustain the (imagined) 150 MB/S peak tape rate
for D0, with very little congestion.

<p>The problem becomes more difficult as the throughput of a NIC card
decreases. The basic unit of transfer is a stream carrying the full tape
rate.  (i.e 5 MB/Sfor AIT-2 + some allowance for expansion).  It is in fact,
a little questionable whether two such tape streams should be multiplexed onto
a single 100 MBPS NIC card -- Congestion, slow start and other rate-inhibiting
mechanisms may be invoked.


<p>Other system design and integration issues<p>

<ul>
<li> A specific site for the Enstore installation has not been identified.
<li> The Enstore project has begun to learn about PATROL, but is unaware of
     its formal selection for Run II.
<li> The OSS dept has work items relating to installing and administering the
     operating systems for the D0 Enstore system.
<li> Run II authentication systems are TBD.
<li> The Enstore mover machines have not yet been identified.
</ul>



<hr>
<h2><a name="d0req">
8 D0 Requirements
</h2>

<h3><a name="d0reqa">
8.1 Summary of D0 Functional Specifications
</h3>

Below is a list of functional specifications on Enstore generated by D0.
For the most part, the product complies in software features. Execptions
are noted. The operational procedures (Section 6)are mostly to be constructed.

<H3>
The Functionality we expect of a Storage Management Layer</H3>
There are 6 major functional areas.&nbsp; They are described in more detail
and broken down further below.
<UL>
<LI>
Cataloging and
Database Functions for Files and Tape Volumes</LI>

<LI>
Specification
and Control of&nbsp; Tape Volume Storage Locations</LI>

<LI>
Control of various
parameters which govern the functional behavior and performance of the
system</LI>

<LI>
Management of
the robot resources (including error recovery and tracking)</LI>

<LI>
Movement of files
between users machine/local disk and tape in robot.</LI>

<LI>
Operational procedures
to run and manage the robot, the data stored in the system, the tape drives,&nbsp;
and the "databases"</LI>
</UL>

<H3>
1) Cataloging and Database Functions</H3>


<H3>
1.1) Maintenance of the primary "database" of file
to tape volume information</H3>

<UL>
<LI>
reliable and backed up
"database" of each&nbsp; volume and file. the volume location of each file,&nbsp;
and the position within each volume for each file</LI>

<p> <b><it> PNFS backup issues will be treated in DESY visit this February </it></b></P>

<LI>
assurance that all movement
of files/deletion of files is correctly reflected in the "database"</LI>

<LI>
notation of volume/file
status (e.g. if unreadable or errors)</LI>

<LI>
tracking of the tape volume format</LI>
</UL>



<H3>
1.2) Provide access to File namespace
and Volume Information</H3>

<UL>
<LI>
tools for users to easily
and intuitively view all files in the system along with other commonly
needed information about the file - such as owner, 'grouping', date written,
etc.</LI>

<LI>
tools for users to easily
and intuitively view all volumes in the system</LI>

<LI>
tools for viewing file/volume
related information such as all files on a volume</LI>

<LI>
tools for exporting all,&nbsp;
or recently changed parts,&nbsp; of the file and volume database to the
data access layer (for performance reasons) or to remote institutions</LI>
</UL>


<H3>
2) Specification and control of Tape storage locations</H3>

<UL>
<LI>
ability
to handle several distinct robot storage locations</LI>

<LI>
ability
to treat a single physical robot as multiple logical storage locations</LI>

<LI>
ability
to handle various physical&nbsp; 'shelves' as possible storage locations</LI>

<LI>
ability
to migrate tape volumes between storage locations</LI>

<LI>
ability
to import volumes into storage locations (given sufficient meta-data in
an acceptable format)</LI>

<LI>
ability
to export volumes (with their associate meta-data)</LI>

<LI>
possibly
implementation of quota system for particular user/group within a storage
location</LI>
</UL>

<H3>
3) Control of parameters which govern the functional
behavior of the system</H3>


<H3>
3.1) Control of parameters which govern allocation
and use of tape drives</H3>

<UL>
<LI>
possibly specicification of preference
or affinity between certain access modes, users&nbsp; or groups, and certain
subsets or classes of physical tape drives</LI>
<p> <b><it> Cannot do this right now, requires hook in Library Manager </it></b></P>
</UL>


<H3>
3.2) Control of parameters which govern how files
are written to tape</H3>

<UL>
<LI>
specification of "groupings" or File
Families for files</LI>

<LI>
specification of "width" for a grouping
- ie number of tapes allowed to be written in //</LI>

<LI>
possibly specification of a list of
files to be treated logically as one 'work unit'</LI>

<LI>
specification of&nbsp; "append to tapes"
policy</LI>

<LI>
specification of file wrappering format</LI>

<LI>
associaton of tape volumes to a particular
file family and tape library</LI>
</UL>

<H3>
<B>3.3) Control of parameters which govern how files
are read from tape</B></H3>

<UL>
<LI>
specification of error/retry behavior</LI>
<p> <b><it> Cannot do this dynamically, have static retry behavior, and will makr this comply to D0 needs. </it></b></P>
</UL>
<H3>
<B>3.4) Control of parameters which govern</B> <B>access
to files and volumes</B></H3>

<UL>
<LI>
access control based on user/group
for each file and each file family</LI>
</UL>

<H3>
<B>3.5) Control of parameters which govern network
routing between storage system movers and client machines</B></H3>

<UL>
<LI>
ability to choose optimal path to load
balance in the case of multiple network interfaces on a single machine</LI>
</UL>

<H3>
<B>3.6) Ability to set defaults for many/most of
the above parameters</B></H3>

<UL>
<LI>
storing of default values to be used
for all transfers/work done for</LI>

<UL>
<LI>
a particular user/group</LI>

<LI>
a particular file family</LI>

<LI>
a particular storage location</LI>

<LI>
? possibly others</LI>
</UL>
</UL>


<H3>
4) Management of the robot resources (including error
recovery and tracking)</H3>

<UL>
<LI>
Maintenance of a queue of work to do
in case of excess demand on the robot or on the tape drives</LI>

<LI>
Ability to specify policies governing
the ordering and manipulation of that queue of work, and therefore the
delay seen by the user, including (but not limited to)</LI>

<UL>
<LI>
specification of a priority for all
work requested</LI>

<LI>
specification of a priority increment
and delta time in order to implement a priority boost/aging algorithm (or
equivalent mechanisms)</LI>

<LI>
specification of&nbsp; policy for dismounting
of tapes after work completed</LI>

<LI>
other possible parameters to be decided
based on tests/tuning of system</LI>
</UL>

<LI>
Cleanup of work queue in case of errors
and cancelled requesting processes</LI>

<LI>
Allocation of tape drives to units
of work in the queue</LI>

<LI>
Retry of errored file reads/writes
up to specified maximum</LI>

<LI>
Repeat attempts at failed work using
alternate tape drive resources</LI>

<LI>
Notation and tracking of all work done,
all errors encountered, all retries performed</LI>
<p> <b><it> Does does not track reties in a user-visible way, except that the delay is apparent. </it></b></P>
</UL>


<H3>
<B>5) Movement of Files&nbsp;
between users machine/local disk and tape in robot.</B></H3>

<UL>
<LI>
ability
to transfer files from any network-connected machine to/from tape drive
in robot</LI>

<LI>
ability
to transfer files reliably and with error detection, and correction by
retry</LI>

<LI>
ability
to transfer files at > N% of raw tape bandwidth. N is probably about 50.</LI>

<LI>
nothing
done to exclude the possibility of adding an intermediate disk cache layer
to rate adjust movement of data from tape drive to end user data sink -
should that become necessary</LI>

<LI>
nothing
done to exclude the possibility of cooperation with the data access layer
as a distributed disk cache of recently requested data</LI>
</UL>

<H3>
6) Operational procedures to run and manage the robot,
tape drives and "databases"</H3>


<H3>
6.1) Robot and Tape Drive Hardware</H3>


<UL>
<LI>
Well defined and safe procedures for
dealing with repair and maintenance of the robot itself</LI>

<LI>
Procedures for monitoring the status
of&nbsp; tape drives and for replacing faulty drives with drives which
have been checked and tested through another well-defined process.</LI>
</UL>

<H3>
6.2) Operator procedures for import/export of batches
of tapes</H3>

<UL>
<LI>
Interface between Storage Management
system and Operator work/console system</LI>

<LI>
Definition of policies for executing
batch imports/exports of tape volumes</LI>
</UL>

<H3>
6.3) Quality assurance procedures to assure integrity
of the data and metadata</H3>

<UL>
<LI>
Routine backup of "database"</LI>

<LI>
Maintenance of a water-tight redo log
of all transaction to be used in case of errors</LI>

<LI>
Recovery procedures in place, tested
and executed in case of failures</LI>

<LI>
Ability to recover files and data on
tape in case of complete and catastrophic loss of all "databases"</LI>

<LI>
Routine checks on readability of sample
of tapes - maintenance of statistics</LI>
</UL>


<h3><a name="d0reqb">
8.2 Sam/Enstore Interface Notes.
</h3>


Below are note on the SAM/Enstore interface provided by D0.
For the most part, Enstore complies in software features. Exceptions
are noted.

<H3>
SAM/Enstore Interface</H3>
The SAM data access layer will use the command/executable provided by Enstore
to issue file commands.
<P>The basic format of this command is one of the following:
<P><em>encp</em>&nbsp; &lt;input file>&nbsp; &lt;destination directory
in pnfs space>
<BR><em>encp</em>&nbsp; &lt;file in pnfs file space>&nbsp; &lt;output
file>
<P>The exact syntax of the above may be changing somewhat, but is immaterial.
<P>The following enhancements have been requested and (we think) agreed
to by Enstore.
<BR>&nbsp;
<BR>&nbsp;

<TABLE BORDER COLS=3 WIDTH="100%"  NOSAVE >
<TR  NOSAVE>
<TD NOSAVE><b>Request to Enstore</b></TD>
<TD><b>Implementation proposed</b></TD>
<TD><b>Rationale</b></TD>
</TR>



<TR NOSAVE>
<TD NOSAVE>Allow wild cards in input or output file spec. As each file
arrives some notification should be provided.
<br><p><em>The notational issues in this items have not been addressed.
Enstore provides cp-like list features. The user can launch many encps.
Input wildcarding is allowed and furnished by the user's shell glob capabilities. Output
wildcarding doesn't make any sense -- Enstore allows the user to specify a
input list of files and an output directory; in this case the input names
are used in creating the output files in the directory (<it>i.e.</it>,
similar to the UNIX cp command.</em>
</TD>

<TD>Enstore will implement notification by writing a message to stdout.
</TD>
<TD>Permits a number of files to be supplied or dispatched serially with
one encp.&nbsp;</TD>
</TR>


<TR>
<TD>Allow list of comma delimited files in input or output file spec
<br><p><em>The notational issues in this items have not been addressed.
Enstore provides cp-like list features and the delimiter in this case is a
blank. Notification after each transfer is provided with the --d0sam switch.</em>
</TD>

<TD>Notification of each file arrival (or dispatch) as for wild cards.</TD>

<TD>Permits a number of files to be supplied or dispatched serially with
one encp.&nbsp;</TD>
</TR>

<TR>
<TD>At the end of each file transaction provide information about the physical
location of the file, its position on the tape, error/retries, which tape
drive it was written on.&nbsp;</TD>

<TD>This was originally discussed as being written to stdout along with
informational messages about the state of the copy job.&nbsp; Latest thoughts
appear to be to write all metadata related to the physical location of
the file and how it got there into a separate, but parallel pnfs file system,
into a file of the same name (we think?)</TD>

<TD>It is very convenient when doing queries in order to gather information
on files to optimize access patterns and when making reports,&nbsp; to
have all of the physical information on the files in the SAM Oracle file
and event catalog. Multiple pnfs query calls would be awkward and unsymmetric
with respect to files managed by SAM, but not stored in the Enstore Robot
space.</TD>
</TR>

<TR>
<TD>Allow additional parameters on the enstore 'copy' command to control
the positioning of the job in the enstore job queue.&nbsp; Initial priority,
Aging Delta Time and Priority Increment would be sufficient.&nbsp;</TD>

<TD>Exact implementation of the desired effect left to Enstore. Whether
at a certain priority a job becomes pre-emptive of a job already in progress
left for later stages of the project, after some experience with resource
allocation.&nbsp;</TD>

<TD>Need some degree of control over the ordering and priority of jobs
already submitted to the enstore queue, in order to balance the flows of
data and minimize job latency where necessary, but without rigid allocation
of resources to particular access modes or projects</TD>
</TR>

<TR>
<TD>At the end of each file transaction provide information about the job
which copied the file - dwell time in queue, final priority, robot arm
wait time, file seek time, file transfer time and MBs, etc.&nbsp;</TD>

<TD>This is now going to be available in the parallel pnfs file metatdata
file system</TD>

<TD>This information is needed by the Global Resource Manager in order
to feed into the algorithm which adjusts the rate of flow of jobs by access
mode.</TD>
</TR>

<TR>
<TD>When an enstore job fails because of a tape error or failure of the
receiving encp (or network or whatever) the job queue of enstore should
be cleaned up appropriately.&nbsp;</TD>

<TD>Could live without this in 1st implementation, but would be nice to
determine what is appropriate behavior in each of the possible failure
modes. We are expecting automatic retries when tape cannot be read or written
in a particular drive and the tape only marked as unreadable if tried in
n drives.</TD>

<TD>SAM does not wish to handle tape errors, tape statistics or retries
- merely to note relevant information on state of media and record drive
used in the File and Event Catalog</TD>
</TR>

<TR>
<TD>If the STK robot and a couple of drives cannot be hooked up with an
enstore test system by October 1, then Enstore needs to emulate the delays
of a robot for Tape mount, File seek time, and File transfer time, in order
to test the Global Resource Manager.&nbsp;</TD>

<TD>Part of this is already implemented as a 'simple' model.&nbsp; Is this
adequate - it is not installed yet, SAM have not tried it.&nbsp;</TD>

<TD>Essential to simulate queing for scarce resources - the tape drive,
and the network bandwidth.&nbsp;</TD>
</TR>
</TABLE>

<BR>&nbsp;
<P><A HREF="enstore-return-format.txt">Latest specification of data returned
on call to encp</A>
</BODY>
</HTML>



</body>
</html>
