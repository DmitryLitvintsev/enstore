<html> <head>

<title> D0EnSrv2 </title>

</head>

<body BGCOLOR=#FFFFFF>

<font size=7><b><i>
<center> D0EnSrv2</center>
</i></b></font>
<br><p>

<br><p>
<center>$Revision$</center>
<center>$Date$GMT</center>
<br><p>

<br><p>

D0EnSrv2 is the node that runs the "base" Enstore servers, namely, the
Configuration Server, the Logger, the Alarm Server, and the
Inquisitor.  It is also the central D0En patrol monitoring node as
well as the primary web server.
<p>
<p>
Configuration Server
<ul>

<li> <a
href="http://www-hppc.fnal.gov/enstore/design.html#config_server">
Detailed Information </a> on the Enstore Configuration Server is
available from the <a
href="http://www-hppc.fnal.gov/enstore/design.html"> Enstore Technical
Document.</a> Command details are covered there.

<p>
<li> Complete <a
href="http://www-d0en.fnal.gov/enstore/config_enstore_system.html">
Configuration Information </a> is available on the web. You can also
see it directly by
    <ul>

       <li> Logging in as enstore to one of the console servers

       <li> Type "setup enstore"

       <li> Type " enstore config --show
    </ul>

<p>
<li> New configurations (be careful!) can be loaded by
    <ul>

       <li> Logging in as enstore to one of the console servers

       <li> Type "setup enstore"

       <li> type "python &lt new_config_file &gt" and verify there are
       no errors.

       <li> Type " enstore config --load --config-file= &lt
       new_config_file &gt"
    </ul>

</ul>



<br>
<br>
<br>
Logger
<ul>
<li> <a
href="http://www-hppc.fnal.gov/enstore/design.html#log_server">
Detailed Information </a> on the Enstore Logger is available from the
<a href="http://www-hppc.fnal.gov/enstore/design.html"> Enstore
Technical Document.</a> Command details are covered there.

<p>
<li> The <a href="http://www-d0en.fnal.gov/enstore/enstore_logs.html

</ul>

<br>
<br>
<br>
Alarm Server
<ul>
<p>
<li>
</ul>

<br>
<br>
<br>
Inquisitor
<ul>
</ul>



<br>
<br>
<br>
Patrol
<ul>
<p>
<li>
</ul>


<br>
<br>
<br>
Web Server
<ul>
<p>
<li>
</ul>


<br>
<br>
<br>
Cron jobs executing on d0ensrv1:
<ul>

<li> user root - plog.  If this fails, either you are out of disk
space on /diska for the pnfs logs or else something is not configured
correctly. (This cronjob runs in the pnfs framework, not the enstore
one).

<li> user root - pnfsFastBackup. If this fails either you are out of
disk space on /diska for the backups, or the backup node, d0ensrv3, is
down or out of disk space, or the network connection to the backup
node is down. (This cronjob runs in the pnfs framework, not the
enstore one).

<li> user root - delfile. If this fails, there is probably with the
file clerk.

<li> user root - rsync. If this fails, there are problems with the
raid disks, /diska or /diskb.

<li> user enstore backup.  If this fails either you are out of disk
space on /diskb for the backups, or the backup node, d0ensrv3, is down
or out of disk space, or the network connection to the backup node is
down.

<li> user enstore - checkfile. This doesn't work right now.

<li> user enstore - checkvolume. This doesn't work right now.

<li> user root - tarit. If this fails, the most likely reason is
because either rip8 (the backup node for the ide system disk) or the
network connection to rip8 is down.

</ul>



<br>
<br>
<br>
Raid Software
<ul>

<li> Mirroring has been disabled in the standard linux kernel.  To
re-enable it, a patch from
ftp://ftp.kernel.org/pub/linux/daemons/raid/alpha/raid0145-19990824-2.2.11.patch
was applied to the 2.2.13 linux kernel.  This went smoothly.

<li> Up to date raid tools, not available via the standard
distribution, also had to be installed to make mirroring work
properly. These raidtools were gotten from the same place as the raid
patch,
ftp://ftp.kernel.org/pub/linux/daemons/raid/alpha/raidtools-19990824-0.90.tar.gz

<li> The raid devices are declared in /etc/raidtab (not the older
/etc/mdtab). There are 2 raid disks on d0ensrv1 and each one is
composed of 2 9 GB scsi disks to form 1 9 GB scsi disk.  There are no
spare disks in the configuration.

<li> You can check the <a
href="http://www-d0en.fnal.gov/enstore/d0ensrv1_raid_status.txt"> Raid Status
Page </a> to look for errors.  This page is updated every few
minutes. You can also log into d0ensrv1 and type "raidinfo" to get
current values.  One needs to pay particular attention to the 1st few
lines that show the disk's current state.

</ul>



<br>
<br>
<br>
Bootup
<ul>

<li> Raidstart is invoked.

<li> Each raid disk has e2fsck run on it to correct any errors

<li> The pnfs servers are started.

<li> The Enstore servers are started.

<li> FTT makes /dev/rmt scsi devices.

<li> The BMC watchdog is armed and deadman is started at real-time
priority.


</ul>

<br>
<br>
<br>
Some Details:
<ul>

<li> Each serial line is configured at 19200 baud rate.

<li> The linux kernel, via lilo, has been configured to send its root
console output to ttyS0. The kernel sends its output to ttyS0 by
adding "append="console=ttyS0,19200 console=tty0 ..." to the kernel
stanza.

<li> The barracuda 9 GB hard drives needs to have several options
enabled to operate efficiently.  Do this by adding a line to each
stanza of lilo.conf: append=".... console=tty0
ncr53c8xx=tags:4,sync:10,ultra:y"

<li> The bios needs to be configured to use vt100 terminals, xon/xoff
flow control, and ???.  Also lilo needs to have the following line
once, ahead of all stanzas, "serial=0,19200n8".

<li> Gettys need to be running on in order to open a tty session. The
/etc/inittab has to have "c:12345:respawn:/sbin/mingetty ttyS0" line.

<li> A backup of the system ide disk is made once per day to
rip8:/fnal/enstore/servers/srv1.  This allows one to have an
up-to-date copy in order to replace a failed node. Procedures (tarit
and loadit in /root) are in place to do this; One also needs the
d0enrtbt floppy to create a replacement system disk.

<li> Although a system disk backup is made daily, the user's home
directory files are not copied. No one should be saving files on these
nodes without copies in appropriately backed up machines.

</ul>

</body></html>
